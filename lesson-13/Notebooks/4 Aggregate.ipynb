{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a82b2d70-528e-4d70-82d8-e09e3c6a9ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.5.0`\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971c7adb-21bc-45c6-9739-19d3542686c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4caaef5-6fb9-4333-8b72-ed1115989f12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "24/09/09 20:39:35 INFO SparkContext: Running Spark version 3.5.0\n",
      "24/09/09 20:39:35 INFO SparkContext: OS info Linux, 6.8.0-40-generic, amd64\n",
      "24/09/09 20:39:35 INFO SparkContext: Java version 11.0.24\n",
      "24/09/09 20:39:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/09 20:39:36 INFO ResourceUtils: ==============================================================\n",
      "24/09/09 20:39:36 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/09/09 20:39:36 INFO ResourceUtils: ==============================================================\n",
      "24/09/09 20:39:36 INFO SparkContext: Submitted application: Aggregate\n",
      "24/09/09 20:39:36 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/09/09 20:39:36 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/09/09 20:39:36 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/09/09 20:39:36 INFO SecurityManager: Changing view acls to: vadim\n",
      "24/09/09 20:39:36 INFO SecurityManager: Changing modify acls to: vadim\n",
      "24/09/09 20:39:36 INFO SecurityManager: Changing view acls groups to: \n",
      "24/09/09 20:39:36 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/09/09 20:39:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: vadim; groups with view permissions: EMPTY; users with modify permissions: vadim; groups with modify permissions: EMPTY\n",
      "24/09/09 20:39:36 INFO Utils: Successfully started service 'sparkDriver' on port 45529.\n",
      "24/09/09 20:39:36 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/09/09 20:39:36 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/09/09 20:39:36 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/09/09 20:39:36 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/09/09 20:39:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/09/09 20:39:36 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-dc567bbb-8c97-4faa-8e15-7833edbd4cb9\n",
      "24/09/09 20:39:36 INFO MemoryStore: MemoryStore started with capacity 4.5 GiB\n",
      "24/09/09 20:39:36 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/09/09 20:39:37 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/09/09 20:39:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/09/09 20:39:37 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "24/09/09 20:39:37 INFO Executor: Starting executor ID driver on host ubuntu\n",
      "24/09/09 20:39:37 INFO Executor: OS info Linux, 6.8.0-40-generic, amd64\n",
      "24/09/09 20:39:37 INFO Executor: Java version 11.0.24\n",
      "24/09/09 20:39:37 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "24/09/09 20:39:37 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@45b80189 for default.\n",
      "24/09/09 20:39:37 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44823.\n",
      "24/09/09 20:39:37 INFO NettyBlockTransferService: Server created on ubuntu:44823\n",
      "24/09/09 20:39:37 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/09/09 20:39:37 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ubuntu, 44823, None)\n",
      "24/09/09 20:39:37 INFO BlockManagerMasterEndpoint: Registering block manager ubuntu:44823 with 4.5 GiB RAM, BlockManagerId(driver, ubuntu, 44823, None)\n",
      "24/09/09 20:39:37 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ubuntu, 44823, None)\n",
      "24/09/09 20:39:37 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ubuntu, 44823, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@590e0b06\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "                .builder()\n",
    "                .master(\"local[*]\")\n",
    "                .appName(\"Aggregate\")\n",
    "                .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43b3cca5-0d39-4c53-b21d-764e17dae944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/09 20:39:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/09/09 20:39:39 INFO SharedState: Warehouse path is 'file:/home/vadim/workspace/Spark/UDF%20UDAF/spark-warehouse'.\n",
      "24/09/09 20:39:40 INFO CodeGenerator: Code generated in 294.502561 ms\n",
      "24/09/09 20:39:42 INFO CodeGenerator: Code generated in 14.621223 ms\n",
      "24/09/09 20:39:42 INFO CodeGenerator: Code generated in 17.643521 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----+--------+-------+----------+\n",
      "|   firstName|lastName|state|quantity|revenue| timestamp|\n",
      "+------------+--------+-----+--------+-------+----------+\n",
      "|Jean-Georges|  Perrin|   NC|       1|    300|1551903533|\n",
      "|Jean-Georges|  Perrin|   NC|       2|    120|1551903567|\n",
      "|Jean-Georges|  Perrin|   CA|       4|     75|1551903599|\n",
      "|      Holden|   Karau|   CA|       6|     37|1551904299|\n",
      "|       Ginni| Rometty|   NY|       7|     91|1551916792|\n",
      "|      Holden|   Karau|   CA|       4|    153|1552876129|\n",
      "+------------+--------+-----+--------+-------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32mDataFrame\u001b[39m = [firstName: string, lastName: string ... 4 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq(\n",
    "    (\"Jean-Georges\", \"Perrin\", \"NC\", 1, 300, 1551903533),\n",
    "    (\"Jean-Georges\", \"Perrin\", \"NC\", 2, 120, 1551903567),\n",
    "    (\"Jean-Georges\", \"Perrin\", \"CA\" ,4, 75, 1551903599),\n",
    "    (\"Holden\", \"Karau\", \"CA\" , 6, 37, 1551904299),\n",
    "    (\"Ginni\", \"Rometty\", \"NY\", 7, 91, 1551916792),\n",
    "    (\"Holden\", \"Karau\", \"CA\", 4, 153, 1552876129)\n",
    ").toDF(\"firstName\", \"lastName\", \"state\", \"quantity\", \"revenue\", \"timestamp\")\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9983de2-1de5-40ad-8dc6-cae3b1bf37b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/09 20:39:43 INFO CodeGenerator: Code generated in 114.926076 ms\n",
      "24/09/09 20:39:43 INFO CodeGenerator: Code generated in 20.06419 ms\n",
      "24/09/09 20:39:43 INFO DAGScheduler: Registering RDD 2 (show at cell5.sc:8) as input to shuffle 0\n",
      "24/09/09 20:39:43 INFO DAGScheduler: Got map stage job 0 (show at cell5.sc:8) with 6 output partitions\n",
      "24/09/09 20:39:43 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (show at cell5.sc:8)\n",
      "24/09/09 20:39:43 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/09 20:39:43 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/09 20:39:43 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at show at cell5.sc:8), which has no missing parents\n",
      "24/09/09 20:39:43 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 43.6 KiB, free 4.5 GiB)\n",
      "24/09/09 20:39:44 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 18.6 KiB, free 4.5 GiB)\n",
      "24/09/09 20:39:44 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ubuntu:44823 (size: 18.6 KiB, free: 4.5 GiB)\n",
      "24/09/09 20:39:44 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/09 20:39:44 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at show at cell5.sc:8) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n",
      "24/09/09 20:39:44 INFO TaskSchedulerImpl: Adding task set 0.0 with 6 tasks resource profile 0\n",
      "24/09/09 20:39:44 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7840 bytes) \n",
      "24/09/09 20:39:44 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7840 bytes) \n",
      "24/09/09 20:39:44 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7840 bytes) \n",
      "24/09/09 20:39:44 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7832 bytes) \n",
      "24/09/09 20:39:44 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7832 bytes) \n",
      "24/09/09 20:39:44 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7832 bytes) \n",
      "24/09/09 20:39:44 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "24/09/09 20:39:44 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "24/09/09 20:39:44 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
      "24/09/09 20:39:44 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
      "24/09/09 20:39:44 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)\n",
      "24/09/09 20:39:44 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)\n",
      "24/09/09 20:39:44 INFO CodeGenerator: Code generated in 45.731242 ms\n",
      "24/09/09 20:39:44 INFO CodeGenerator: Code generated in 14.767399 ms\n",
      "24/09/09 20:39:44 INFO CodeGenerator: Code generated in 10.566411 ms\n",
      "24/09/09 20:39:44 INFO CodeGenerator: Code generated in 9.004953 ms\n",
      "24/09/09 20:39:44 INFO CodeGenerator: Code generated in 21.851303 ms\n",
      "24/09/09 20:39:44 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 2442 bytes result sent to driver\n",
      "24/09/09 20:39:44 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 2442 bytes result sent to driver\n",
      "24/09/09 20:39:44 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2442 bytes result sent to driver\n",
      "24/09/09 20:39:44 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2442 bytes result sent to driver\n",
      "24/09/09 20:39:44 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2442 bytes result sent to driver\n",
      "24/09/09 20:39:44 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 2442 bytes result sent to driver\n",
      "24/09/09 20:39:44 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 459 ms on ubuntu (executor driver) (1/6)\n",
      "24/09/09 20:39:44 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 455 ms on ubuntu (executor driver) (2/6)\n",
      "24/09/09 20:39:44 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 463 ms on ubuntu (executor driver) (3/6)\n",
      "24/09/09 20:39:44 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 485 ms on ubuntu (executor driver) (4/6)\n",
      "24/09/09 20:39:44 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 466 ms on ubuntu (executor driver) (5/6)\n",
      "24/09/09 20:39:44 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 464 ms on ubuntu (executor driver) (6/6)\n",
      "24/09/09 20:39:44 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/09/09 20:39:44 INFO DAGScheduler: ShuffleMapStage 0 (show at cell5.sc:8) finished in 0,737 s\n",
      "24/09/09 20:39:44 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/09 20:39:44 INFO DAGScheduler: running: Set()\n",
      "24/09/09 20:39:44 INFO DAGScheduler: waiting: Set()\n",
      "24/09/09 20:39:44 INFO DAGScheduler: failed: Set()\n",
      "24/09/09 20:39:44 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/09/09 20:39:44 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/09/09 20:39:44 INFO CodeGenerator: Code generated in 29.318191 ms\n",
      "24/09/09 20:39:44 INFO SparkContext: Starting job: show at cell5.sc:8\n",
      "24/09/09 20:39:44 INFO DAGScheduler: Got job 1 (show at cell5.sc:8) with 1 output partitions\n",
      "24/09/09 20:39:44 INFO DAGScheduler: Final stage: ResultStage 2 (show at cell5.sc:8)\n",
      "24/09/09 20:39:44 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
      "24/09/09 20:39:44 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/09 20:39:44 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at show at cell5.sc:8), which has no missing parents\n",
      "24/09/09 20:39:44 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 49.7 KiB, free 4.5 GiB)\n",
      "24/09/09 20:39:44 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.6 KiB, free 4.5 GiB)\n",
      "24/09/09 20:39:44 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ubuntu:44823 (size: 20.6 KiB, free: 4.5 GiB)\n",
      "24/09/09 20:39:44 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/09 20:39:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at show at cell5.sc:8) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/09 20:39:44 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "24/09/09 20:39:44 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 6) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/09 20:39:44 INFO Executor: Running task 0.0 in stage 2.0 (TID 6)\n",
      "24/09/09 20:39:44 INFO ShuffleBlockFetcherIterator: Getting 6 (774.0 B) non-empty blocks including 6 (774.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/09 20:39:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms\n",
      "24/09/09 20:39:44 INFO CodeGenerator: Code generated in 28.496045 ms\n",
      "24/09/09 20:39:45 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ubuntu:44823 in memory (size: 18.6 KiB, free: 4.5 GiB)\n",
      "24/09/09 20:39:45 INFO Executor: Finished task 0.0 in stage 2.0 (TID 6). 5228 bytes result sent to driver\n",
      "24/09/09 20:39:45 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 6) in 195 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/09 20:39:45 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/09/09 20:39:45 INFO DAGScheduler: ResultStage 2 (show at cell5.sc:8) finished in 0,221 s\n",
      "24/09/09 20:39:45 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/09 20:39:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/09/09 20:39:45 INFO DAGScheduler: Job 1 finished: show at cell5.sc:8, took 0,256684 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----+-------+-------+-------+\n",
      "|   firstName|lastName|state|sum_qty|sum_rev|avg_rev|\n",
      "+------------+--------+-----+-------+-------+-------+\n",
      "|Jean-Georges|  Perrin|   NC|      3|    420|  210.0|\n",
      "|Jean-Georges|  Perrin|   CA|      4|     75|   75.0|\n",
      "|      Holden|   Karau|   CA|     10|    190|   95.0|\n",
      "|       Ginni| Rometty|   NY|      7|     91|   91.0|\n",
      "+------------+--------+-----+-------+-------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36maggDf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [firstName: string, lastName: string ... 4 more fields]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val aggDf = data\n",
    "            .groupBy($\"firstName\", $\"lastName\", $\"state\")\n",
    "            .agg(\n",
    "                sum(\"quantity\").as(\"sum_qty\"),\n",
    "                sum(\"revenue\").as(\"sum_rev\"),\n",
    "                avg(\"revenue\").as(\"avg_rev\"))\n",
    "\n",
    "aggDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d99a8f-470e-48ed-beac-3adcc428758b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
