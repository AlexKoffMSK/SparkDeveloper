{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9157853e-0a6e-4cbe-bd75-dc7140b2a070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.5.0`\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19c06e9d-a8ce-4b5b-a221-1197d6d1aa21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.expressions._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c28eed4-44ce-4ee0-9cf0-4a67c80b94ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "24/09/09 20:54:05 INFO SparkContext: Running Spark version 3.5.0\n",
      "24/09/09 20:54:05 INFO SparkContext: OS info Linux, 6.8.0-40-generic, amd64\n",
      "24/09/09 20:54:05 INFO SparkContext: Java version 11.0.24\n",
      "24/09/09 20:54:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/09 20:54:06 INFO ResourceUtils: ==============================================================\n",
      "24/09/09 20:54:06 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/09/09 20:54:06 INFO ResourceUtils: ==============================================================\n",
      "24/09/09 20:54:06 INFO SparkContext: Submitted application: UDAF\n",
      "24/09/09 20:54:06 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/09/09 20:54:06 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/09/09 20:54:06 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/09/09 20:54:06 INFO SecurityManager: Changing view acls to: vadim\n",
      "24/09/09 20:54:06 INFO SecurityManager: Changing modify acls to: vadim\n",
      "24/09/09 20:54:06 INFO SecurityManager: Changing view acls groups to: \n",
      "24/09/09 20:54:06 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/09/09 20:54:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: vadim; groups with view permissions: EMPTY; users with modify permissions: vadim; groups with modify permissions: EMPTY\n",
      "24/09/09 20:54:06 INFO Utils: Successfully started service 'sparkDriver' on port 40505.\n",
      "24/09/09 20:54:06 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/09/09 20:54:06 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/09/09 20:54:06 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/09/09 20:54:06 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/09/09 20:54:06 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/09/09 20:54:06 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c745b464-433e-46f6-9870-024b2c939b5a\n",
      "24/09/09 20:54:06 INFO MemoryStore: MemoryStore started with capacity 4.5 GiB\n",
      "24/09/09 20:54:06 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/09/09 20:54:06 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/09/09 20:54:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/09/09 20:54:06 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/09/09 20:54:06 INFO Utils: Successfully started service 'SparkUI' on port 4042.\n",
      "24/09/09 20:54:07 INFO Executor: Starting executor ID driver on host ubuntu\n",
      "24/09/09 20:54:07 INFO Executor: OS info Linux, 6.8.0-40-generic, amd64\n",
      "24/09/09 20:54:07 INFO Executor: Java version 11.0.24\n",
      "24/09/09 20:54:07 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "24/09/09 20:54:07 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@f342020 for default.\n",
      "24/09/09 20:54:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40629.\n",
      "24/09/09 20:54:07 INFO NettyBlockTransferService: Server created on ubuntu:40629\n",
      "24/09/09 20:54:07 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/09/09 20:54:07 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ubuntu, 40629, None)\n",
      "24/09/09 20:54:07 INFO BlockManagerMasterEndpoint: Registering block manager ubuntu:40629 with 4.5 GiB RAM, BlockManagerId(driver, ubuntu, 40629, None)\n",
      "24/09/09 20:54:07 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ubuntu, 40629, None)\n",
      "24/09/09 20:54:07 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ubuntu, 40629, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@1bb190cc\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "                .builder()\n",
    "                .master(\"local[*]\")\n",
    "                .appName(\"UDAF\")\n",
    "                .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9907b685",
   "metadata": {},
   "source": [
    "## Простой пример: Среднее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ecc6411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/09 20:54:09 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/09/09 20:54:09 INFO SharedState: Warehouse path is 'file:/home/vadim/workspace/Spark/UDF%20UDAF/spark-warehouse'.\n",
      "24/09/09 20:54:10 INFO CodeGenerator: Code generated in 279.395204 ms\n",
      "24/09/09 20:54:11 INFO CodeGenerator: Code generated in 10.862698 ms\n",
      "24/09/09 20:54:11 INFO CodeGenerator: Code generated in 16.298351 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+\n",
      "|   name|depId|salary|\n",
      "+-------+-----+------+\n",
      "|Michael|    1|  3000|\n",
      "|Michael|    2|  6000|\n",
      "|   Andy|    1|  4500|\n",
      "|   Andy|    2|  2500|\n",
      "| Justin|    3|  3500|\n",
      "|  Berta|    3|  4000|\n",
      "+-------+-----+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [name: string, depId: int ... 1 more field]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data1 = Seq(\n",
    "                (\"Michael\", 1, 3000),\n",
    "                (\"Michael\", 2, 6000),\n",
    "                (\"Andy\", 1, 4500),\n",
    "                (\"Andy\", 2, 2500),\n",
    "                (\"Justin\", 3, 3500),\n",
    "                (\"Berta\", 3, 4000))\n",
    "            .toDF(\"name\", \"depId\", \"salary\")\n",
    "\n",
    "data1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45addd36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mAverage\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mMyAverage\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "\n",
    "case class Average(var sum: Long, var count: Long)\n",
    "\n",
    "class MyAverage extends Aggregator[Long, Average, Double] {\n",
    "  // Начальное значение. Должно соответствовать свойству: любое b + zero = b\n",
    "  def zero: Average = Average(0L, 0L)\n",
    "  // Объединение двух значений в новое значение.\n",
    "  // Для повышения производительности функция может изменять `buffer` и \n",
    "  // возвращать его вместо создания нового объекта.\n",
    "  def reduce(buffer: Average, data: Long): Average = {\n",
    "    buffer.sum += data\n",
    "    buffer.count += 1\n",
    "    buffer\n",
    "  }\n",
    "  // Объединение двух промежуточных значения\n",
    "  def merge(b1: Average, b2: Average): Average = {\n",
    "    b1.sum += b2.sum\n",
    "    b1.count += b2.count\n",
    "    b1\n",
    "  }\n",
    "  // Преобразование выходных данных\n",
    "  def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count\n",
    "  // Кодировщик для типа промежуточного значения\n",
    "  def bufferEncoder: Encoder[Average] = Encoders.product\n",
    "  // Кодировщик для типа выходного значения\n",
    "  def outputEncoder: Encoder[Double] = Encoders.scalaDouble\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66c7c96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mmyAverageUDAF\u001b[39m: \u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedAggregator\u001b[39m(\n",
       "  ammonite.$sess.cell5$Helper$MyAverage@4865b929,\n",
       "  \u001b[33mExpressionEncoder\u001b[39m(\n",
       "    \u001b[33mBoundReference\u001b[39m(\u001b[32m0\u001b[39m, LongType, \u001b[32mfalse\u001b[39m),\n",
       "    \u001b[33mAssertNotNull\u001b[39m(\n",
       "      \u001b[33mUpCast\u001b[39m(\n",
       "        \u001b[33mGetColumnByOrdinal\u001b[39m(\u001b[32m0\u001b[39m, LongType),\n",
       "        LongType,\n",
       "        \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"long\\\"\"\u001b[39m)\n",
       "      ),\n",
       "      \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"long\\\"\"\u001b[39m)\n",
       "    ),\n",
       "    Long\n",
       "  ),\n",
       "  \u001b[32mNone\u001b[39m,\n",
       "  \u001b[32mtrue\u001b[39m,\n",
       "  \u001b[32mtrue\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val myAverageUDAF = udaf(new MyAverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c8718ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/09 20:54:14 INFO CodeGenerator: Code generated in 9.322327 ms\n",
      "24/09/09 20:54:14 INFO DAGScheduler: Registering RDD 3 (show at cell7.sc:3) as input to shuffle 0\n",
      "24/09/09 20:54:14 INFO DAGScheduler: Got map stage job 0 (show at cell7.sc:3) with 6 output partitions\n",
      "24/09/09 20:54:14 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (show at cell7.sc:3)\n",
      "24/09/09 20:54:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/09 20:54:14 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/09 20:54:14 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at show at cell7.sc:3), which has no missing parents\n",
      "24/09/09 20:54:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 28.8 KiB, free 4.5 GiB)\n",
      "24/09/09 20:54:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/09 20:54:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ubuntu:40629 (size: 13.0 KiB, free: 4.5 GiB)\n",
      "24/09/09 20:54:15 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/09 20:54:15 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at show at cell7.sc:3) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n",
      "24/09/09 20:54:15 INFO TaskSchedulerImpl: Adding task set 0.0 with 6 tasks resource profile 0\n",
      "24/09/09 20:54:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7784 bytes) \n",
      "24/09/09 20:54:15 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7784 bytes) \n",
      "24/09/09 20:54:15 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7784 bytes) \n",
      "24/09/09 20:54:15 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7784 bytes) \n",
      "24/09/09 20:54:15 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7784 bytes) \n",
      "24/09/09 20:54:15 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7784 bytes) \n",
      "24/09/09 20:54:15 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)\n",
      "24/09/09 20:54:15 INFO Executor: Running task 4.0 in stage 0.0 (TID 4)\n",
      "24/09/09 20:54:15 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "24/09/09 20:54:15 INFO Executor: Running task 5.0 in stage 0.0 (TID 5)\n",
      "24/09/09 20:54:15 INFO Executor: Running task 2.0 in stage 0.0 (TID 2)\n",
      "24/09/09 20:54:15 INFO Executor: Running task 3.0 in stage 0.0 (TID 3)\n",
      "24/09/09 20:54:15 INFO CodeGenerator: Code generated in 17.473573 ms\n",
      "24/09/09 20:54:15 INFO CodeGenerator: Code generated in 11.008 ms\n",
      "24/09/09 20:54:15 INFO CodeGenerator: Code generated in 10.060113 ms\n",
      "24/09/09 20:54:15 INFO CodeGenerator: Code generated in 8.904898 ms\n",
      "24/09/09 20:54:15 INFO CodeGenerator: Code generated in 9.986805 ms\n",
      "24/09/09 20:54:15 INFO CodeGenerator: Code generated in 12.030605 ms\n",
      "24/09/09 20:54:15 INFO CodeGenerator: Code generated in 14.686558 ms\n",
      "24/09/09 20:54:15 INFO Executor: Finished task 5.0 in stage 0.0 (TID 5). 2172 bytes result sent to driver\n",
      "24/09/09 20:54:15 INFO Executor: Finished task 4.0 in stage 0.0 (TID 4). 2172 bytes result sent to driver\n",
      "24/09/09 20:54:15 INFO Executor: Finished task 2.0 in stage 0.0 (TID 2). 2172 bytes result sent to driver\n",
      "24/09/09 20:54:15 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2172 bytes result sent to driver\n",
      "24/09/09 20:54:15 INFO Executor: Finished task 3.0 in stage 0.0 (TID 3). 2172 bytes result sent to driver\n",
      "24/09/09 20:54:15 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 2172 bytes result sent to driver\n",
      "24/09/09 20:54:15 INFO TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 502 ms on ubuntu (executor driver) (1/6)\n",
      "24/09/09 20:54:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 527 ms on ubuntu (executor driver) (2/6)\n",
      "24/09/09 20:54:15 INFO TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 509 ms on ubuntu (executor driver) (3/6)\n",
      "24/09/09 20:54:15 INFO TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 509 ms on ubuntu (executor driver) (4/6)\n",
      "24/09/09 20:54:15 INFO TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 512 ms on ubuntu (executor driver) (5/6)\n",
      "24/09/09 20:54:15 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 519 ms on ubuntu (executor driver) (6/6)\n",
      "24/09/09 20:54:15 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/09/09 20:54:15 INFO DAGScheduler: ShuffleMapStage 0 (show at cell7.sc:3) finished in 0,937 s\n",
      "24/09/09 20:54:15 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/09 20:54:15 INFO DAGScheduler: running: Set()\n",
      "24/09/09 20:54:15 INFO DAGScheduler: waiting: Set()\n",
      "24/09/09 20:54:15 INFO DAGScheduler: failed: Set()\n",
      "24/09/09 20:54:15 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/09/09 20:54:15 INFO SparkContext: Starting job: show at cell7.sc:3\n",
      "24/09/09 20:54:15 INFO DAGScheduler: Got job 1 (show at cell7.sc:3) with 1 output partitions\n",
      "24/09/09 20:54:15 INFO DAGScheduler: Final stage: ResultStage 2 (show at cell7.sc:3)\n",
      "24/09/09 20:54:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\n",
      "24/09/09 20:54:15 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/09 20:54:15 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[6] at show at cell7.sc:3), which has no missing parents\n",
      "24/09/09 20:54:15 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 34.7 KiB, free 4.5 GiB)\n",
      "24/09/09 20:54:15 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 15.7 KiB, free 4.5 GiB)\n",
      "24/09/09 20:54:15 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ubuntu:40629 (size: 15.7 KiB, free: 4.5 GiB)\n",
      "24/09/09 20:54:15 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/09 20:54:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at show at cell7.sc:3) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/09 20:54:15 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "24/09/09 20:54:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 6) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/09 20:54:15 INFO Executor: Running task 0.0 in stage 2.0 (TID 6)\n",
      "24/09/09 20:54:16 INFO ShuffleBlockFetcherIterator: Getting 6 (480.0 B) non-empty blocks including 6 (480.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/09 20:54:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms\n",
      "24/09/09 20:54:16 INFO CodeGenerator: Code generated in 11.212693 ms\n",
      "24/09/09 20:54:16 INFO CodeGenerator: Code generated in 14.940307 ms\n",
      "24/09/09 20:54:16 INFO CodeGenerator: Code generated in 7.415442 ms\n",
      "24/09/09 20:54:16 INFO Executor: Finished task 0.0 in stage 2.0 (TID 6). 4535 bytes result sent to driver\n",
      "24/09/09 20:54:16 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 6) in 198 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/09 20:54:16 INFO DAGScheduler: ResultStage 2 (show at cell7.sc:3) finished in 0,230 s\n",
      "24/09/09 20:54:16 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/09/09 20:54:16 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/09 20:54:16 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/09/09 20:54:16 INFO DAGScheduler: Job 1 finished: show at cell7.sc:3, took 0,271748 s\n",
      "24/09/09 20:54:16 INFO CodeGenerator: Code generated in 27.997274 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|depId|average_salary|\n",
      "+-----+--------------+\n",
      "|    1|        3750.0|\n",
      "|    2|        4250.0|\n",
      "|    3|        3750.0|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1\n",
    "    .groupBy(\"depId\")\n",
    "    .agg(myAverageUDAF($\"salary\").as(\"average_salary\"))\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ff419ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/09 20:54:16 INFO CodeGenerator: Code generated in 10.431046 ms\n",
      "24/09/09 20:54:16 INFO DAGScheduler: Registering RDD 10 (show at cell8.sc:3) as input to shuffle 1\n",
      "24/09/09 20:54:16 INFO DAGScheduler: Got map stage job 2 (show at cell8.sc:3) with 6 output partitions\n",
      "24/09/09 20:54:16 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (show at cell8.sc:3)\n",
      "24/09/09 20:54:16 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/09 20:54:16 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/09 20:54:16 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[10] at show at cell8.sc:3), which has no missing parents\n",
      "24/09/09 20:54:16 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 28.8 KiB, free 4.5 GiB)\n",
      "24/09/09 20:54:16 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/09 20:54:16 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ubuntu:40629 (size: 13.0 KiB, free: 4.5 GiB)\n",
      "24/09/09 20:54:16 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/09 20:54:16 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[10] at show at cell8.sc:3) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n",
      "24/09/09 20:54:16 INFO TaskSchedulerImpl: Adding task set 3.0 with 6 tasks resource profile 0\n",
      "24/09/09 20:54:16 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 7) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7792 bytes) \n",
      "24/09/09 20:54:16 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 8) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7792 bytes) \n",
      "24/09/09 20:54:16 INFO TaskSetManager: Starting task 2.0 in stage 3.0 (TID 9) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7792 bytes) \n",
      "24/09/09 20:54:16 INFO TaskSetManager: Starting task 3.0 in stage 3.0 (TID 10) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7792 bytes) \n",
      "24/09/09 20:54:16 INFO TaskSetManager: Starting task 4.0 in stage 3.0 (TID 11) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7792 bytes) \n",
      "24/09/09 20:54:16 INFO TaskSetManager: Starting task 5.0 in stage 3.0 (TID 12) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7792 bytes) \n",
      "24/09/09 20:54:16 INFO Executor: Running task 1.0 in stage 3.0 (TID 8)\n",
      "24/09/09 20:54:16 INFO Executor: Running task 2.0 in stage 3.0 (TID 9)\n",
      "24/09/09 20:54:16 INFO Executor: Running task 5.0 in stage 3.0 (TID 12)\n",
      "24/09/09 20:54:16 INFO Executor: Running task 3.0 in stage 3.0 (TID 10)\n",
      "24/09/09 20:54:16 INFO Executor: Running task 4.0 in stage 3.0 (TID 11)\n",
      "24/09/09 20:54:16 INFO Executor: Running task 0.0 in stage 3.0 (TID 7)\n",
      "24/09/09 20:54:16 INFO CodeGenerator: Code generated in 11.778927 ms\n",
      "24/09/09 20:54:16 INFO CodeGenerator: Code generated in 16.407488 ms\n",
      "24/09/09 20:54:16 INFO CodeGenerator: Code generated in 15.49958 ms\n",
      "24/09/09 20:54:17 INFO Executor: Finished task 3.0 in stage 3.0 (TID 10). 2172 bytes result sent to driver\n",
      "24/09/09 20:54:17 INFO TaskSetManager: Finished task 3.0 in stage 3.0 (TID 10) in 142 ms on ubuntu (executor driver) (1/6)\n",
      "24/09/09 20:54:17 INFO Executor: Finished task 2.0 in stage 3.0 (TID 9). 2172 bytes result sent to driver\n",
      "24/09/09 20:54:17 INFO Executor: Finished task 4.0 in stage 3.0 (TID 11). 2172 bytes result sent to driver\n",
      "24/09/09 20:54:17 INFO TaskSetManager: Finished task 4.0 in stage 3.0 (TID 11) in 144 ms on ubuntu (executor driver) (2/6)\n",
      "24/09/09 20:54:17 INFO TaskSetManager: Finished task 2.0 in stage 3.0 (TID 9) in 151 ms on ubuntu (executor driver) (3/6)\n",
      "24/09/09 20:54:17 INFO Executor: Finished task 0.0 in stage 3.0 (TID 7). 2172 bytes result sent to driver\n",
      "24/09/09 20:54:17 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 7) in 158 ms on ubuntu (executor driver) (4/6)\n",
      "24/09/09 20:54:17 INFO Executor: Finished task 1.0 in stage 3.0 (TID 8). 2172 bytes result sent to driver\n",
      "24/09/09 20:54:17 INFO Executor: Finished task 5.0 in stage 3.0 (TID 12). 2172 bytes result sent to driver\n",
      "24/09/09 20:54:17 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 8) in 163 ms on ubuntu (executor driver) (5/6)\n",
      "24/09/09 20:54:17 INFO TaskSetManager: Finished task 5.0 in stage 3.0 (TID 12) in 165 ms on ubuntu (executor driver) (6/6)\n",
      "24/09/09 20:54:17 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/09/09 20:54:17 INFO DAGScheduler: ShuffleMapStage 3 (show at cell8.sc:3) finished in 0,226 s\n",
      "24/09/09 20:54:17 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/09 20:54:17 INFO DAGScheduler: running: Set()\n",
      "24/09/09 20:54:17 INFO DAGScheduler: waiting: Set()\n",
      "24/09/09 20:54:17 INFO DAGScheduler: failed: Set()\n",
      "24/09/09 20:54:17 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ubuntu:40629 in memory (size: 13.0 KiB, free: 4.5 GiB)\n",
      "24/09/09 20:54:17 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/09/09 20:54:17 INFO BlockManagerInfo: Removed broadcast_1_piece0 on ubuntu:40629 in memory (size: 15.7 KiB, free: 4.5 GiB)\n",
      "24/09/09 20:54:17 INFO SparkContext: Starting job: show at cell8.sc:3\n",
      "24/09/09 20:54:17 INFO DAGScheduler: Got job 3 (show at cell8.sc:3) with 1 output partitions\n",
      "24/09/09 20:54:17 INFO DAGScheduler: Final stage: ResultStage 5 (show at cell8.sc:3)\n",
      "24/09/09 20:54:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\n",
      "24/09/09 20:54:17 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/09 20:54:17 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[13] at show at cell8.sc:3), which has no missing parents\n",
      "24/09/09 20:54:17 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 34.7 KiB, free 4.5 GiB)\n",
      "24/09/09 20:54:17 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 15.7 KiB, free 4.5 GiB)\n",
      "24/09/09 20:54:17 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ubuntu:40629 (size: 15.7 KiB, free: 4.5 GiB)\n",
      "24/09/09 20:54:17 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/09 20:54:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[13] at show at cell8.sc:3) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/09 20:54:17 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "24/09/09 20:54:17 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 13) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/09 20:54:17 INFO Executor: Running task 0.0 in stage 5.0 (TID 13)\n",
      "24/09/09 20:54:17 INFO ShuffleBlockFetcherIterator: Getting 6 (564.0 B) non-empty blocks including 6 (564.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/09 20:54:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "24/09/09 20:54:17 INFO CodeGenerator: Code generated in 10.8224 ms\n",
      "24/09/09 20:54:17 INFO Executor: Finished task 0.0 in stage 5.0 (TID 13). 4528 bytes result sent to driver\n",
      "24/09/09 20:54:17 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 13) in 77 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/09 20:54:17 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/09/09 20:54:17 INFO DAGScheduler: ResultStage 5 (show at cell8.sc:3) finished in 0,102 s\n",
      "24/09/09 20:54:17 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/09 20:54:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "24/09/09 20:54:17 INFO DAGScheduler: Job 3 finished: show at cell8.sc:3, took 0,126461 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|   name|average_salary|\n",
      "+-------+--------------+\n",
      "|Michael|        4500.0|\n",
      "|   Andy|        3500.0|\n",
      "| Justin|        3500.0|\n",
      "|  Berta|        4000.0|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1\n",
    "    .groupBy($\"name\")\n",
    "    .agg(myAverageUDAF($\"salary\").as(\"average_salary\"))\n",
    "    .show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeaaf7a",
   "metadata": {},
   "source": [
    "## Продвинутый пример: сумма с ограничением"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0114f8",
   "metadata": {},
   "source": [
    "Каждый клиент получает один балл за каждую заказанную единицу товара, но неболее трёх баллов в одном заказе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c579a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/09 20:54:18 INFO CodeGenerator: Code generated in 22.381653 ms\n",
      "24/09/09 20:54:18 INFO CodeGenerator: Code generated in 6.1743 ms\n",
      "24/09/09 20:54:18 INFO CodeGenerator: Code generated in 11.791842 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----+--------+-------+----------+\n",
      "|   firstName|lastName|state|quantity|revenue| timestamp|\n",
      "+------------+--------+-----+--------+-------+----------+\n",
      "|Jean-Georges|  Perrin|   NC|       1|    300|1551903533|\n",
      "|Jean-Georges|  Perrin|   NC|       2|    120|1551903567|\n",
      "|Jean-Georges|  Perrin|   CA|       4|     75|1551903599|\n",
      "|      Holden|   Karau|   CA|       6|     37|1551904299|\n",
      "|       Ginni| Rometty|   NY|       7|     91|1551916792|\n",
      "|      Holden|   Karau|   CA|       4|    153|1552876129|\n",
      "+------------+--------+-----+--------+-------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata\u001b[39m: \u001b[32mDataFrame\u001b[39m = [firstName: string, lastName: string ... 4 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq(\n",
    "    (\"Jean-Georges\", \"Perrin\", \"NC\", 1, 300, 1551903533),\n",
    "    (\"Jean-Georges\", \"Perrin\", \"NC\", 2, 120, 1551903567),\n",
    "    (\"Jean-Georges\", \"Perrin\", \"CA\" ,4, 75, 1551903599),\n",
    "    (\"Holden\", \"Karau\", \"CA\" , 6, 37, 1551904299),\n",
    "    (\"Ginni\", \"Rometty\", \"NY\", 7, 91, 1551916792),\n",
    "    (\"Holden\", \"Karau\", \"CA\", 4, 153, 1552876129)\n",
    ").toDF(\"firstName\", \"lastName\", \"state\", \"quantity\", \"revenue\", \"timestamp\")\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5e7a296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mBuffer\u001b[39m\n",
       "defined \u001b[32mclass\u001b[39m \u001b[36mPointAttribution\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "\n",
    "case class Buffer(var value: Int) \n",
    "\n",
    "class PointAttribution extends Aggregator[Int, Buffer, Int] {\n",
    "  val MAX_POINT_PER_ORDER = 3\n",
    "  // Начальное значение. Должно соответствовать свойству: любое b + zero = b\n",
    "  def zero: Buffer = Buffer(0)\n",
    "  // Объединение двух значений в новое значение.\n",
    "  // Для повышения производительности функция может изменять `buffer` и \n",
    "  // возвращать его вместо создания нового объекта.\n",
    "  def reduce(buffer: Buffer, data: Int): Buffer = {\n",
    "    val outputValue = if (data < MAX_POINT_PER_ORDER) data else MAX_POINT_PER_ORDER\n",
    "    buffer.value += outputValue\n",
    "    buffer\n",
    "  }\n",
    "  // Объединение двух промежуточных значения\n",
    "  def merge(b1: Buffer, b2: Buffer): Buffer = {\n",
    "    b1.value += b2.value\n",
    "    b1\n",
    "  }\n",
    "  // Преобразование выходных данных\n",
    "  def finish(reduction: Buffer): Int = reduction.value\n",
    "  // Кодировщик для типа промежуточного значения\n",
    "  def bufferEncoder: Encoder[Buffer] = Encoders.product\n",
    "  // Кодировщик для типа выходного значения\n",
    "  def outputEncoder: Encoder[Int] = Encoders.scalaInt\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35b44e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpointAttribution\u001b[39m: \u001b[32mPointAttribution\u001b[39m = ammonite.$sess.cell10$Helper$PointAttribution@3ebd6943\n",
       "\u001b[36mpointAttributionUDAF\u001b[39m: \u001b[32mUserDefinedFunction\u001b[39m = \u001b[33mUserDefinedAggregator\u001b[39m(\n",
       "  ammonite.$sess.cell10$Helper$PointAttribution@3ebd6943,\n",
       "  \u001b[33mExpressionEncoder\u001b[39m(\n",
       "    \u001b[33mBoundReference\u001b[39m(\u001b[32m0\u001b[39m, IntegerType, \u001b[32mfalse\u001b[39m),\n",
       "    \u001b[33mAssertNotNull\u001b[39m(\n",
       "      \u001b[33mUpCast\u001b[39m(\n",
       "        \u001b[33mGetColumnByOrdinal\u001b[39m(\u001b[32m0\u001b[39m, IntegerType),\n",
       "        IntegerType,\n",
       "        \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"int\\\"\"\u001b[39m)\n",
       "      ),\n",
       "      \u001b[33mList\u001b[39m(\u001b[32m\"- root class: \\\"int\\\"\"\u001b[39m)\n",
       "    ),\n",
       "    Int\n",
       "  ),\n",
       "  \u001b[32mNone\u001b[39m,\n",
       "  \u001b[32mtrue\u001b[39m,\n",
       "  \u001b[32mtrue\u001b[39m\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pointAttribution = new PointAttribution\n",
    "val pointAttributionUDAF = udaf(pointAttribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5113579e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/09 20:54:19 INFO CodeGenerator: Code generated in 11.818888 ms\n",
      "24/09/09 20:54:19 INFO DAGScheduler: Registering RDD 17 (show at cell12.sc:5) as input to shuffle 2\n",
      "24/09/09 20:54:19 INFO DAGScheduler: Got map stage job 4 (show at cell12.sc:5) with 6 output partitions\n",
      "24/09/09 20:54:19 INFO DAGScheduler: Final stage: ShuffleMapStage 6 (show at cell12.sc:5)\n",
      "24/09/09 20:54:19 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/09 20:54:19 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/09 20:54:19 INFO DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[17] at show at cell12.sc:5), which has no missing parents\n",
      "24/09/09 20:54:19 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 28.1 KiB, free 4.5 GiB)\n",
      "24/09/09 20:54:19 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 12.8 KiB, free 4.5 GiB)\n",
      "24/09/09 20:54:19 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ubuntu:40629 (size: 12.8 KiB, free: 4.5 GiB)\n",
      "24/09/09 20:54:19 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/09 20:54:19 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[17] at show at cell12.sc:5) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n",
      "24/09/09 20:54:19 INFO TaskSchedulerImpl: Adding task set 6.0 with 6 tasks resource profile 0\n",
      "24/09/09 20:54:19 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 14) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7832 bytes) \n",
      "24/09/09 20:54:19 INFO TaskSetManager: Starting task 1.0 in stage 6.0 (TID 15) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7832 bytes) \n",
      "24/09/09 20:54:19 INFO TaskSetManager: Starting task 2.0 in stage 6.0 (TID 16) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7832 bytes) \n",
      "24/09/09 20:54:19 INFO TaskSetManager: Starting task 3.0 in stage 6.0 (TID 17) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7824 bytes) \n",
      "24/09/09 20:54:19 INFO TaskSetManager: Starting task 4.0 in stage 6.0 (TID 18) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7824 bytes) \n",
      "24/09/09 20:54:19 INFO TaskSetManager: Starting task 5.0 in stage 6.0 (TID 19) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7824 bytes) \n",
      "24/09/09 20:54:19 INFO Executor: Running task 2.0 in stage 6.0 (TID 16)\n",
      "24/09/09 20:54:19 INFO Executor: Running task 5.0 in stage 6.0 (TID 19)\n",
      "24/09/09 20:54:19 INFO Executor: Running task 0.0 in stage 6.0 (TID 14)\n",
      "24/09/09 20:54:19 INFO Executor: Running task 3.0 in stage 6.0 (TID 17)\n",
      "24/09/09 20:54:19 INFO Executor: Running task 1.0 in stage 6.0 (TID 15)\n",
      "24/09/09 20:54:19 INFO Executor: Running task 4.0 in stage 6.0 (TID 18)\n",
      "24/09/09 20:54:19 INFO CodeGenerator: Code generated in 21.859997 ms\n",
      "24/09/09 20:54:19 INFO CodeGenerator: Code generated in 11.247389 ms\n",
      "24/09/09 20:54:19 INFO CodeGenerator: Code generated in 7.107171 ms\n",
      "24/09/09 20:54:19 INFO CodeGenerator: Code generated in 7.985283 ms\n",
      "24/09/09 20:54:19 INFO CodeGenerator: Code generated in 7.740115 ms\n",
      "24/09/09 20:54:19 INFO CodeGenerator: Code generated in 5.834318 ms\n",
      "24/09/09 20:54:19 INFO CodeGenerator: Code generated in 5.746467 ms\n",
      "24/09/09 20:54:19 INFO CodeGenerator: Code generated in 15.580413 ms\n",
      "24/09/09 20:54:19 INFO CodeGenerator: Code generated in 7.505261 ms\n",
      "24/09/09 20:54:20 INFO Executor: Finished task 0.0 in stage 6.0 (TID 14). 2129 bytes result sent to driver\n",
      "24/09/09 20:54:20 INFO Executor: Finished task 3.0 in stage 6.0 (TID 17). 2129 bytes result sent to driver\n",
      "24/09/09 20:54:20 INFO Executor: Finished task 4.0 in stage 6.0 (TID 18). 2129 bytes result sent to driver\n",
      "24/09/09 20:54:20 INFO Executor: Finished task 2.0 in stage 6.0 (TID 16). 2129 bytes result sent to driver\n",
      "24/09/09 20:54:20 INFO Executor: Finished task 5.0 in stage 6.0 (TID 19). 2129 bytes result sent to driver\n",
      "24/09/09 20:54:20 INFO Executor: Finished task 1.0 in stage 6.0 (TID 15). 2172 bytes result sent to driver\n",
      "24/09/09 20:54:20 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 14) in 195 ms on ubuntu (executor driver) (1/6)\n",
      "24/09/09 20:54:20 INFO TaskSetManager: Finished task 3.0 in stage 6.0 (TID 17) in 196 ms on ubuntu (executor driver) (2/6)\n",
      "24/09/09 20:54:20 INFO TaskSetManager: Finished task 4.0 in stage 6.0 (TID 18) in 195 ms on ubuntu (executor driver) (3/6)\n",
      "24/09/09 20:54:20 INFO TaskSetManager: Finished task 2.0 in stage 6.0 (TID 16) in 201 ms on ubuntu (executor driver) (4/6)\n",
      "24/09/09 20:54:20 INFO TaskSetManager: Finished task 5.0 in stage 6.0 (TID 19) in 194 ms on ubuntu (executor driver) (5/6)\n",
      "24/09/09 20:54:20 INFO TaskSetManager: Finished task 1.0 in stage 6.0 (TID 15) in 211 ms on ubuntu (executor driver) (6/6)\n",
      "24/09/09 20:54:20 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "24/09/09 20:54:20 INFO DAGScheduler: ShuffleMapStage 6 (show at cell12.sc:5) finished in 0,262 s\n",
      "24/09/09 20:54:20 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/09 20:54:20 INFO DAGScheduler: running: Set()\n",
      "24/09/09 20:54:20 INFO DAGScheduler: waiting: Set()\n",
      "24/09/09 20:54:20 INFO DAGScheduler: failed: Set()\n",
      "24/09/09 20:54:20 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/09/09 20:54:20 INFO SparkContext: Starting job: show at cell12.sc:5\n",
      "24/09/09 20:54:20 INFO DAGScheduler: Got job 5 (show at cell12.sc:5) with 1 output partitions\n",
      "24/09/09 20:54:20 INFO DAGScheduler: Final stage: ResultStage 8 (show at cell12.sc:5)\n",
      "24/09/09 20:54:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\n",
      "24/09/09 20:54:20 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/09 20:54:20 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[20] at show at cell12.sc:5), which has no missing parents\n",
      "24/09/09 20:54:20 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 35.2 KiB, free 4.5 GiB)\n",
      "24/09/09 20:54:20 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 15.9 KiB, free 4.5 GiB)\n",
      "24/09/09 20:54:20 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on ubuntu:40629 (size: 15.9 KiB, free: 4.5 GiB)\n",
      "24/09/09 20:54:20 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/09 20:54:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[20] at show at cell12.sc:5) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/09 20:54:20 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "24/09/09 20:54:20 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 20) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/09 20:54:20 INFO Executor: Running task 0.0 in stage 8.0 (TID 20)\n",
      "24/09/09 20:54:20 INFO ShuffleBlockFetcherIterator: Getting 6 (774.0 B) non-empty blocks including 6 (774.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/09 20:54:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "24/09/09 20:54:20 INFO CodeGenerator: Code generated in 11.771723 ms\n",
      "24/09/09 20:54:20 INFO CodeGenerator: Code generated in 7.610519 ms\n",
      "24/09/09 20:54:20 INFO CodeGenerator: Code generated in 15.726637 ms\n",
      "24/09/09 20:54:20 INFO CodeGenerator: Code generated in 10.664866 ms\n",
      "24/09/09 20:54:20 INFO Executor: Finished task 0.0 in stage 8.0 (TID 20). 4641 bytes result sent to driver\n",
      "24/09/09 20:54:20 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 20) in 94 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/09 20:54:20 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "24/09/09 20:54:20 INFO DAGScheduler: ResultStage 8 (show at cell12.sc:5) finished in 0,123 s\n",
      "24/09/09 20:54:20 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/09 20:54:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "24/09/09 20:54:20 INFO DAGScheduler: Job 5 finished: show at cell12.sc:5, took 0,150679 s\n",
      "24/09/09 20:54:20 INFO CodeGenerator: Code generated in 15.910729 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----+-------------+-----+\n",
      "|   firstName|lastName|state|sum(quantity)|point|\n",
      "+------------+--------+-----+-------------+-----+\n",
      "|Jean-Georges|  Perrin|   NC|            3|    3|\n",
      "|Jean-Georges|  Perrin|   CA|            4|    3|\n",
      "|      Holden|   Karau|   CA|           10|    6|\n",
      "|       Ginni| Rometty|   NY|            7|    3|\n",
      "+------------+--------+-----+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mt1\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m104861519797981L\u001b[39m\n",
       "\u001b[36mduration1\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.671533804\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t1 = System.nanoTime\n",
    "\n",
    "data\n",
    "      .groupBy($\"firstName\", $\"lastName\", $\"state\")\n",
    "      .agg(sum(\"quantity\"), pointAttributionUDAF($\"quantity\").as(\"point\"))\n",
    "      .show()\n",
    "\n",
    "val duration1 = (System.nanoTime - t1) / 1e9d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b089a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/09 20:54:20 INFO BlockManagerInfo: Removed broadcast_3_piece0 on ubuntu:40629 in memory (size: 15.7 KiB, free: 4.5 GiB)\n",
      "24/09/09 20:54:20 INFO BlockManagerInfo: Removed broadcast_5_piece0 on ubuntu:40629 in memory (size: 15.9 KiB, free: 4.5 GiB)\n",
      "24/09/09 20:54:20 INFO BlockManagerInfo: Removed broadcast_4_piece0 on ubuntu:40629 in memory (size: 12.8 KiB, free: 4.5 GiB)\n",
      "24/09/09 20:54:20 INFO BlockManagerInfo: Removed broadcast_2_piece0 on ubuntu:40629 in memory (size: 13.0 KiB, free: 4.5 GiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration1 = 0.671533804\n"
     ]
    }
   ],
   "source": [
    "println(s\"duration1 = $duration1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a70e16e",
   "metadata": {},
   "source": [
    "Вариант решения этой задачи без UDAF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c00a544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/09 20:54:21 INFO CodeGenerator: Code generated in 53.132465 ms\n",
      "24/09/09 20:54:21 INFO CodeGenerator: Code generated in 10.394832 ms\n",
      "24/09/09 20:54:21 INFO DAGScheduler: Registering RDD 23 (show at cell14.sc:7) as input to shuffle 3\n",
      "24/09/09 20:54:21 INFO DAGScheduler: Got map stage job 6 (show at cell14.sc:7) with 6 output partitions\n",
      "24/09/09 20:54:21 INFO DAGScheduler: Final stage: ShuffleMapStage 9 (show at cell14.sc:7)\n",
      "24/09/09 20:54:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/09 20:54:21 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/09 20:54:21 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[23] at show at cell14.sc:7), which has no missing parents\n",
      "24/09/09 20:54:21 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 38.8 KiB, free 4.5 GiB)\n",
      "24/09/09 20:54:21 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 17.0 KiB, free 4.5 GiB)\n",
      "24/09/09 20:54:21 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on ubuntu:40629 (size: 17.0 KiB, free: 4.5 GiB)\n",
      "24/09/09 20:54:21 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/09 20:54:21 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[23] at show at cell14.sc:7) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))\n",
      "24/09/09 20:54:21 INFO TaskSchedulerImpl: Adding task set 9.0 with 6 tasks resource profile 0\n",
      "24/09/09 20:54:21 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 21) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7840 bytes) \n",
      "24/09/09 20:54:21 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 22) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7840 bytes) \n",
      "24/09/09 20:54:21 INFO TaskSetManager: Starting task 2.0 in stage 9.0 (TID 23) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7840 bytes) \n",
      "24/09/09 20:54:21 INFO TaskSetManager: Starting task 3.0 in stage 9.0 (TID 24) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7832 bytes) \n",
      "24/09/09 20:54:21 INFO TaskSetManager: Starting task 4.0 in stage 9.0 (TID 25) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7832 bytes) \n",
      "24/09/09 20:54:21 INFO TaskSetManager: Starting task 5.0 in stage 9.0 (TID 26) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7832 bytes) \n",
      "24/09/09 20:54:21 INFO Executor: Running task 0.0 in stage 9.0 (TID 21)\n",
      "24/09/09 20:54:21 INFO Executor: Running task 1.0 in stage 9.0 (TID 22)\n",
      "24/09/09 20:54:21 INFO Executor: Running task 2.0 in stage 9.0 (TID 23)\n",
      "24/09/09 20:54:21 INFO Executor: Running task 5.0 in stage 9.0 (TID 26)\n",
      "24/09/09 20:54:21 INFO Executor: Running task 4.0 in stage 9.0 (TID 25)\n",
      "24/09/09 20:54:21 INFO Executor: Running task 3.0 in stage 9.0 (TID 24)\n",
      "24/09/09 20:54:21 INFO CodeGenerator: Code generated in 42.367173 ms\n",
      "24/09/09 20:54:21 INFO CodeGenerator: Code generated in 6.395588 ms\n",
      "24/09/09 20:54:21 INFO CodeGenerator: Code generated in 54.834125 ms\n",
      "24/09/09 20:54:21 INFO Executor: Finished task 2.0 in stage 9.0 (TID 23). 2442 bytes result sent to driver\n",
      "24/09/09 20:54:21 INFO Executor: Finished task 0.0 in stage 9.0 (TID 21). 2442 bytes result sent to driver\n",
      "24/09/09 20:54:21 INFO TaskSetManager: Finished task 2.0 in stage 9.0 (TID 23) in 237 ms on ubuntu (executor driver) (1/6)\n",
      "24/09/09 20:54:21 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 21) in 247 ms on ubuntu (executor driver) (2/6)\n",
      "24/09/09 20:54:21 INFO Executor: Finished task 5.0 in stage 9.0 (TID 26). 2442 bytes result sent to driver\n",
      "24/09/09 20:54:21 INFO Executor: Finished task 3.0 in stage 9.0 (TID 24). 2485 bytes result sent to driver\n",
      "24/09/09 20:54:21 INFO Executor: Finished task 4.0 in stage 9.0 (TID 25). 2485 bytes result sent to driver\n",
      "24/09/09 20:54:21 INFO Executor: Finished task 1.0 in stage 9.0 (TID 22). 2442 bytes result sent to driver\n",
      "24/09/09 20:54:21 INFO TaskSetManager: Finished task 5.0 in stage 9.0 (TID 26) in 246 ms on ubuntu (executor driver) (3/6)\n",
      "24/09/09 20:54:21 INFO TaskSetManager: Finished task 3.0 in stage 9.0 (TID 24) in 252 ms on ubuntu (executor driver) (4/6)\n",
      "24/09/09 20:54:21 INFO TaskSetManager: Finished task 4.0 in stage 9.0 (TID 25) in 253 ms on ubuntu (executor driver) (5/6)\n",
      "24/09/09 20:54:21 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 22) in 258 ms on ubuntu (executor driver) (6/6)\n",
      "24/09/09 20:54:21 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "24/09/09 20:54:21 INFO DAGScheduler: ShuffleMapStage 9 (show at cell14.sc:7) finished in 0,296 s\n",
      "24/09/09 20:54:21 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/09 20:54:21 INFO DAGScheduler: running: Set()\n",
      "24/09/09 20:54:21 INFO DAGScheduler: waiting: Set()\n",
      "24/09/09 20:54:21 INFO DAGScheduler: failed: Set()\n",
      "24/09/09 20:54:21 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/09/09 20:54:21 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/09/09 20:54:21 INFO CodeGenerator: Code generated in 47.918248 ms\n",
      "24/09/09 20:54:21 INFO SparkContext: Starting job: show at cell14.sc:7\n",
      "24/09/09 20:54:21 INFO DAGScheduler: Got job 7 (show at cell14.sc:7) with 1 output partitions\n",
      "24/09/09 20:54:21 INFO DAGScheduler: Final stage: ResultStage 11 (show at cell14.sc:7)\n",
      "24/09/09 20:54:21 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)\n",
      "24/09/09 20:54:21 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/09 20:54:21 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[26] at show at cell14.sc:7), which has no missing parents\n",
      "24/09/09 20:54:21 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 43.5 KiB, free 4.5 GiB)\n",
      "24/09/09 20:54:21 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 18.5 KiB, free 4.5 GiB)\n",
      "24/09/09 20:54:21 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on ubuntu:40629 (size: 18.5 KiB, free: 4.5 GiB)\n",
      "24/09/09 20:54:21 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/09 20:54:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[26] at show at cell14.sc:7) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/09 20:54:21 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
      "24/09/09 20:54:21 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 27) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/09 20:54:21 INFO Executor: Running task 0.0 in stage 11.0 (TID 27)\n",
      "24/09/09 20:54:21 INFO ShuffleBlockFetcherIterator: Getting 6 (702.0 B) non-empty blocks including 6 (702.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/09 20:54:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "24/09/09 20:54:22 INFO CodeGenerator: Code generated in 29.443848 ms\n",
      "24/09/09 20:54:22 INFO Executor: Finished task 0.0 in stage 11.0 (TID 27). 5079 bytes result sent to driver\n",
      "24/09/09 20:54:22 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 27) in 79 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/09 20:54:22 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "24/09/09 20:54:22 INFO DAGScheduler: ResultStage 11 (show at cell14.sc:7) finished in 0,120 s\n",
      "24/09/09 20:54:22 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/09 20:54:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
      "24/09/09 20:54:22 INFO DAGScheduler: Job 7 finished: show at cell14.sc:7, took 0,132805 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+-----+-------------+-----+\n",
      "|   firstName|lastName|state|sum(quantity)|point|\n",
      "+------------+--------+-----+-------------+-----+\n",
      "|Jean-Georges|  Perrin|   NC|            3|    3|\n",
      "|Jean-Georges|  Perrin|   CA|            4|    3|\n",
      "|      Holden|   Karau|   CA|           10|    6|\n",
      "|       Ginni| Rometty|   NY|            7|    3|\n",
      "+------------+--------+-----+-------------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mmax\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m3\u001b[39m\n",
       "\u001b[36mt2\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m104863062705093L\u001b[39m\n",
       "\u001b[36mduration2\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.867179575\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val max = pointAttribution.MAX_POINT_PER_ORDER\n",
    "\n",
    "val t2 = System.nanoTime\n",
    "data\n",
    "        .withColumn(\"point\", when($\"quantity\".$greater(max), max).otherwise($\"quantity\"))\n",
    "        .groupBy($\"firstName\", $\"lastName\", $\"state\")\n",
    "        .agg(sum(\"quantity\"), sum(\"point\").as(\"point\"))\n",
    "        .show()\n",
    "        \n",
    "val duration2 = (System.nanoTime - t2) / 1e9d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdc68c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration1 = 0.671533804\n",
      "duration2 = 0.867179575\n",
      "duration1 - duration2 = -0.19564577100000002\n"
     ]
    }
   ],
   "source": [
    "println(s\"duration1 = $duration1\\nduration2 = $duration2\\nduration1 - duration2 = ${duration1 - duration2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999d7443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
