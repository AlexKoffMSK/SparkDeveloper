{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2515a1c-c4f9-4890-aa6d-641bf419cea6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.5.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc99a5fd-2141-4ffe-afb1-4b0a595a75af",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ab7dc3-9173-4d9e-9a91-97256abb3a30",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49835da8-c4d0-461a-a2c8-3c0959118bb4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7df9de1d-1f63-41b1-999b-69614c723fae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "24/09/06 14:50:35 INFO SparkContext: Running Spark version 3.5.0\n",
      "24/09/06 14:50:35 INFO SparkContext: OS info Linux, 6.8.0-40-generic, amd64\n",
      "24/09/06 14:50:35 INFO SparkContext: Java version 11.0.24\n",
      "24/09/06 14:50:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/06 14:50:35 INFO ResourceUtils: ==============================================================\n",
      "24/09/06 14:50:35 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/09/06 14:50:35 INFO ResourceUtils: ==============================================================\n",
      "24/09/06 14:50:35 INFO SparkContext: Submitted application: Dataframe API\n",
      "24/09/06 14:50:35 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/09/06 14:50:35 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/09/06 14:50:35 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/09/06 14:50:35 INFO SecurityManager: Changing view acls to: vadim\n",
      "24/09/06 14:50:35 INFO SecurityManager: Changing modify acls to: vadim\n",
      "24/09/06 14:50:35 INFO SecurityManager: Changing view acls groups to: \n",
      "24/09/06 14:50:35 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/09/06 14:50:35 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: vadim; groups with view permissions: EMPTY; users with modify permissions: vadim; groups with modify permissions: EMPTY\n",
      "24/09/06 14:50:36 INFO Utils: Successfully started service 'sparkDriver' on port 39831.\n",
      "24/09/06 14:50:36 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/09/06 14:50:36 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/09/06 14:50:36 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/09/06 14:50:36 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/09/06 14:50:36 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/09/06 14:50:36 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-121302a0-12ba-4e7e-840c-c4697036c653\n",
      "24/09/06 14:50:36 INFO MemoryStore: MemoryStore started with capacity 4.5 GiB\n",
      "24/09/06 14:50:36 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/09/06 14:50:36 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/09/06 14:50:36 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/09/06 14:50:36 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "24/09/06 14:50:36 INFO Executor: Starting executor ID driver on host ubuntu\n",
      "24/09/06 14:50:36 INFO Executor: OS info Linux, 6.8.0-40-generic, amd64\n",
      "24/09/06 14:50:36 INFO Executor: Java version 11.0.24\n",
      "24/09/06 14:50:36 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "24/09/06 14:50:36 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@5144ad68 for default.\n",
      "24/09/06 14:50:36 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37607.\n",
      "24/09/06 14:50:36 INFO NettyBlockTransferService: Server created on ubuntu:37607\n",
      "24/09/06 14:50:36 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/09/06 14:50:36 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ubuntu, 37607, None)\n",
      "24/09/06 14:50:36 INFO BlockManagerMasterEndpoint: Registering block manager ubuntu:37607 with 4.5 GiB RAM, BlockManagerId(driver, ubuntu, 37607, None)\n",
      "24/09/06 14:50:36 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ubuntu, 37607, None)\n",
      "24/09/06 14:50:36 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ubuntu, 37607, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@6fc7d508\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "                .builder()\n",
    "                .master(\"local[*]\")\n",
    "                .appName(\"Dataframe API\")\n",
    "                .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59c101c3-900c-4a51-8f5f-bfc0ce535ef9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.version == 3.5.0\n"
     ]
    }
   ],
   "source": [
    "println(s\"spark.version == ${spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0928fb8-44ac-4f97-ab64-94faf88d9e6d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1 Создание DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74a8a2d",
   "metadata": {},
   "source": [
    "### createDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd693919",
   "metadata": {},
   "source": [
    "#### From List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc8e4c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata1\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\u001b[32m\"1\"\u001b[39m, \u001b[32m\"Spark\"\u001b[39m),\n",
       "  (\u001b[32m\"2\"\u001b[39m, \u001b[32m\"Scala\"\u001b[39m),\n",
       "  (\u001b[32m\"3\"\u001b[39m, \u001b[32m\"Java\"\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data1 = Seq((\"1\", \"Spark\"), (\"2\", \"Scala\"), (\"3\", \"Java\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "607ab09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/09/06 14:50:39 INFO SharedState: Warehouse path is 'file:/home/vadim/workspace/Spark/DataFrame/spark-warehouse'.\n",
      "24/09/06 14:50:40 INFO CodeGenerator: Code generated in 147.225362 ms\n",
      "24/09/06 14:50:40 INFO CodeGenerator: Code generated in 16.427192 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| _1|   _2|\n",
      "+---+-----+\n",
      "|  1|Spark|\n",
      "|  2|Scala|\n",
      "|  3| Java|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_1: string, _2: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = spark.createDataFrame(data1)\n",
    "df1.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffde9fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|StudentID|Course|\n",
      "+---------+------+\n",
      "|        1| Spark|\n",
      "|        2| Scala|\n",
      "|        3|  Java|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf1WithNames\u001b[39m: \u001b[32mDataFrame\u001b[39m = [StudentID: string, Course: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1WithNames = df1.withColumnsRenamed(Map(\"_1\" -> \"StudentID\", \"_2\" -> \"Course\"))\n",
    "df1WithNames.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b603ce90",
   "metadata": {},
   "source": [
    "##### withSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b8e03cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mschema1\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"StudentID\"\u001b[39m, StringType, \u001b[32mfalse\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Course\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {})\n",
       ")\n",
       "\u001b[36mschema2\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"StudentID\"\u001b[39m, StringType, \u001b[32mfalse\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Course\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {})\n",
       ")\n",
       "\u001b[36mschema3\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"StudentID\"\u001b[39m, StringType, \u001b[32mfalse\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Course\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {})\n",
       ")\n",
       "\u001b[36mres9_3\u001b[39m: \u001b[32mBoolean\u001b[39m = \u001b[32mtrue\u001b[39m\n",
       "\u001b[36mres9_4\u001b[39m: \u001b[32mBoolean\u001b[39m = \u001b[32mtrue\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema1 = StructType(Array(StructField(\"StudentID\", StringType, false),\n",
    "                              StructField(\"Course\", StringType, true)))\n",
    "\n",
    "val schema2 = new StructType()\n",
    "                    .add(StructField(\"StudentID\", StringType, false))\n",
    "                    .add(StructField(\"Course\", StringType, true))\n",
    "\n",
    "val schema3 = StructType(\n",
    "                StructField(\"StudentID\", StringType, false) :: \n",
    "                StructField(\"Course\", StringType, true) :: \n",
    "                Nil)\n",
    "\n",
    "schema1.equals(schema2)\n",
    "schema2.equals(schema3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c308052f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|StudentID|Course|\n",
      "+---------+------+\n",
      "|        1| Spark|\n",
      "|        2| Scala|\n",
      "|        3|  Java|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mcollection.JavaConverters._\u001b[39m\n",
       "\u001b[36mdata2\u001b[39m: \u001b[32mjava\u001b[39m.\u001b[32mutil\u001b[39m.\u001b[32mList\u001b[39m[\u001b[32mRow\u001b[39m] = \u001b[33mSeqWrapper\u001b[39m(\u001b[33mList\u001b[39m([1,Spark], [2,Scala], [3,Java]))\n",
       "\u001b[36mdf2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [StudentID: string, Course: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collection.JavaConverters._\n",
    "\n",
    "val data2 = data1.map(s => Row(s._1, s._2)).asJava\n",
    "val df2 = spark.createDataFrame(data2, schema2)\n",
    "df2.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd336b",
   "metadata": {},
   "source": [
    "#### fromRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb8852af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:43 INFO CodeGenerator: Code generated in 21.785238 ms\n",
      "24/09/06 14:50:43 INFO SparkContext: Starting job: show at cell11.sc:3\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Got job 0 (show at cell11.sc:3) with 1 output partitions\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Final stage: ResultStage 0 (show at cell11.sc:3)\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at show at cell11.sc:3), which has no missing parents\n",
      "24/09/06 14:50:43 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:43 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:43 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ubuntu:37607 (size: 5.8 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:43 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at show at cell11.sc:3) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:43 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:43 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/06 14:50:43 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "24/09/06 14:50:43 INFO CodeGenerator: Code generated in 14.237791 ms\n",
      "24/09/06 14:50:43 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1452 bytes result sent to driver\n",
      "24/09/06 14:50:43 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 135 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:43 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:43 INFO DAGScheduler: ResultStage 0 (show at cell11.sc:3) finished in 0,317 s\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Job 0 finished: show at cell11.sc:3, took 0,361708 s\n",
      "24/09/06 14:50:43 INFO SparkContext: Starting job: show at cell11.sc:3\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Got job 1 (show at cell11.sc:3) with 4 output partitions\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Final stage: ResultStage 1 (show at cell11.sc:3)\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at show at cell11.sc:3), which has no missing parents\n",
      "24/09/06 14:50:43 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:43 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:43 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ubuntu:37607 (size: 5.8 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:43 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at show at cell11.sc:3) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\n",
      "24/09/06 14:50:43 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks resource profile 0\n",
      "24/09/06 14:50:43 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/06 14:50:43 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/06 14:50:43 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/06 14:50:43 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/06 14:50:43 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "24/09/06 14:50:43 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
      "24/09/06 14:50:43 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)\n",
      "24/09/06 14:50:43 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)\n",
      "24/09/06 14:50:43 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1452 bytes result sent to driver\n",
      "24/09/06 14:50:43 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 1409 bytes result sent to driver\n",
      "24/09/06 14:50:43 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1447 bytes result sent to driver\n",
      "24/09/06 14:50:43 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 1409 bytes result sent to driver\n",
      "24/09/06 14:50:43 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 25 ms on ubuntu (executor driver) (1/4)\n",
      "24/09/06 14:50:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 29 ms on ubuntu (executor driver) (2/4)\n",
      "24/09/06 14:50:43 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 27 ms on ubuntu (executor driver) (3/4)\n",
      "24/09/06 14:50:43 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 30 ms on ubuntu (executor driver) (4/4)\n",
      "24/09/06 14:50:43 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:43 INFO DAGScheduler: ResultStage 1 (show at cell11.sc:3) finished in 0,047 s\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Job 1 finished: show at cell11.sc:3, took 0,055823 s\n",
      "24/09/06 14:50:43 INFO SparkContext: Starting job: show at cell11.sc:3\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Got job 2 (show at cell11.sc:3) with 3 output partitions\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Final stage: ResultStage 2 (show at cell11.sc:3)\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[3] at show at cell11.sc:3), which has no missing parents\n",
      "24/09/06 14:50:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ubuntu:37607 (size: 5.8 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:43 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 2 (MapPartitionsRDD[3] at show at cell11.sc:3) (first 15 tasks are for partitions Vector(5, 6, 7))\n",
      "24/09/06 14:50:43 INFO TaskSchedulerImpl: Adding task set 2.0 with 3 tasks resource profile 0\n",
      "24/09/06 14:50:43 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 5) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/06 14:50:43 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 6) (ubuntu, executor driver, partition 6, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/06 14:50:43 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 7) (ubuntu, executor driver, partition 7, PROCESS_LOCAL, 7667 bytes) \n",
      "24/09/06 14:50:43 INFO Executor: Running task 1.0 in stage 2.0 (TID 6)\n",
      "24/09/06 14:50:43 INFO Executor: Running task 0.0 in stage 2.0 (TID 5)\n",
      "24/09/06 14:50:43 INFO Executor: Running task 2.0 in stage 2.0 (TID 7)\n",
      "24/09/06 14:50:43 INFO Executor: Finished task 1.0 in stage 2.0 (TID 6). 1409 bytes result sent to driver\n",
      "24/09/06 14:50:43 INFO Executor: Finished task 2.0 in stage 2.0 (TID 7). 1447 bytes result sent to driver\n",
      "24/09/06 14:50:43 INFO Executor: Finished task 0.0 in stage 2.0 (TID 5). 1447 bytes result sent to driver\n",
      "24/09/06 14:50:43 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 6) in 14 ms on ubuntu (executor driver) (1/3)\n",
      "24/09/06 14:50:43 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 5) in 19 ms on ubuntu (executor driver) (2/3)\n",
      "24/09/06 14:50:43 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 7) in 18 ms on ubuntu (executor driver) (3/3)\n",
      "24/09/06 14:50:43 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:43 INFO DAGScheduler: ResultStage 2 (show at cell11.sc:3) finished in 0,031 s\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/09/06 14:50:43 INFO DAGScheduler: Job 2 finished: show at cell11.sc:3, took 0,038563 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| _1|   _2|\n",
      "+---+-----+\n",
      "|  1|Spark|\n",
      "|  2|Scala|\n",
      "|  3| Java|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd1\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = ParallelCollectionRDD[0] at parallelize at cell11.sc:1\n",
       "\u001b[36mfromRDD1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_1: string, _2: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = spark.sparkContext.parallelize(data1)\n",
    "val fromRDD1 = spark.createDataFrame(rdd1)\n",
    "fromRDD1.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e85415",
   "metadata": {},
   "source": [
    "##### withSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4d1e502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:44 INFO CodeGenerator: Code generated in 7.967094 ms\n",
      "24/09/06 14:50:44 INFO SparkContext: Starting job: show at cell12.sc:3\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Got job 3 (show at cell12.sc:3) with 1 output partitions\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Final stage: ResultStage 3 (show at cell12.sc:3)\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[7] at show at cell12.sc:3), which has no missing parents\n",
      "24/09/06 14:50:44 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:44 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:44 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ubuntu:37607 (size: 9.8 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:44 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at show at cell12.sc:3) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:44 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:44 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 8) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/06 14:50:44 INFO Executor: Running task 0.0 in stage 3.0 (TID 8)\n",
      "24/09/06 14:50:44 INFO CodeGenerator: Code generated in 9.01884 ms\n",
      "24/09/06 14:50:44 INFO Executor: Finished task 0.0 in stage 3.0 (TID 8). 1409 bytes result sent to driver\n",
      "24/09/06 14:50:44 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 8) in 40 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:44 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:44 INFO DAGScheduler: ResultStage 3 (show at cell12.sc:3) finished in 0,095 s\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Job 3 finished: show at cell12.sc:3, took 0,108704 s\n",
      "24/09/06 14:50:44 INFO SparkContext: Starting job: show at cell12.sc:3\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Got job 4 (show at cell12.sc:3) with 4 output partitions\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Final stage: ResultStage 4 (show at cell12.sc:3)\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[7] at show at cell12.sc:3), which has no missing parents\n",
      "24/09/06 14:50:44 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 21.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:44 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:44 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ubuntu:37607 (size: 9.8 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:44 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 4 (MapPartitionsRDD[7] at show at cell12.sc:3) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\n",
      "24/09/06 14:50:44 INFO TaskSchedulerImpl: Adding task set 4.0 with 4 tasks resource profile 0\n",
      "24/09/06 14:50:44 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 9) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/06 14:50:44 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 10) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/06 14:50:44 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 11) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/06 14:50:44 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 12) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/06 14:50:44 INFO Executor: Running task 1.0 in stage 4.0 (TID 10)\n",
      "24/09/06 14:50:44 INFO Executor: Running task 2.0 in stage 4.0 (TID 11)\n",
      "24/09/06 14:50:44 INFO Executor: Running task 3.0 in stage 4.0 (TID 12)\n",
      "24/09/06 14:50:44 INFO Executor: Running task 0.0 in stage 4.0 (TID 9)\n",
      "24/09/06 14:50:44 INFO Executor: Finished task 0.0 in stage 4.0 (TID 9). 1409 bytes result sent to driver\n",
      "24/09/06 14:50:44 INFO Executor: Finished task 3.0 in stage 4.0 (TID 12). 1409 bytes result sent to driver\n",
      "24/09/06 14:50:44 INFO Executor: Finished task 2.0 in stage 4.0 (TID 11). 1409 bytes result sent to driver\n",
      "24/09/06 14:50:44 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 9) in 30 ms on ubuntu (executor driver) (1/4)\n",
      "24/09/06 14:50:44 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 12) in 31 ms on ubuntu (executor driver) (2/4)\n",
      "24/09/06 14:50:44 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 11) in 34 ms on ubuntu (executor driver) (3/4)\n",
      "24/09/06 14:50:44 INFO CodeGenerator: Code generated in 40.746718 ms\n",
      "24/09/06 14:50:44 INFO Executor: Finished task 1.0 in stage 4.0 (TID 10). 1447 bytes result sent to driver\n",
      "24/09/06 14:50:44 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 10) in 88 ms on ubuntu (executor driver) (4/4)\n",
      "24/09/06 14:50:44 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:44 INFO DAGScheduler: ResultStage 4 (show at cell12.sc:3) finished in 0,109 s\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Job 4 finished: show at cell12.sc:3, took 0,117342 s\n",
      "24/09/06 14:50:44 INFO SparkContext: Starting job: show at cell12.sc:3\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Got job 5 (show at cell12.sc:3) with 3 output partitions\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Final stage: ResultStage 5 (show at cell12.sc:3)\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[7] at show at cell12.sc:3), which has no missing parents\n",
      "24/09/06 14:50:44 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 21.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:44 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:44 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on ubuntu:37607 (size: 9.8 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:44 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 5 (MapPartitionsRDD[7] at show at cell12.sc:3) (first 15 tasks are for partitions Vector(5, 6, 7))\n",
      "24/09/06 14:50:44 INFO TaskSchedulerImpl: Adding task set 5.0 with 3 tasks resource profile 0\n",
      "24/09/06 14:50:44 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 13) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/06 14:50:44 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 14) (ubuntu, executor driver, partition 6, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/06 14:50:44 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 15) (ubuntu, executor driver, partition 7, PROCESS_LOCAL, 7667 bytes) \n",
      "24/09/06 14:50:44 INFO Executor: Running task 0.0 in stage 5.0 (TID 13)\n",
      "24/09/06 14:50:44 INFO Executor: Running task 1.0 in stage 5.0 (TID 14)\n",
      "24/09/06 14:50:44 INFO Executor: Running task 2.0 in stage 5.0 (TID 15)\n",
      "24/09/06 14:50:44 INFO Executor: Finished task 1.0 in stage 5.0 (TID 14). 1409 bytes result sent to driver\n",
      "24/09/06 14:50:44 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 14) in 17 ms on ubuntu (executor driver) (1/3)\n",
      "24/09/06 14:50:44 INFO Executor: Finished task 0.0 in stage 5.0 (TID 13). 1447 bytes result sent to driver\n",
      "24/09/06 14:50:44 INFO Executor: Finished task 2.0 in stage 5.0 (TID 15). 1447 bytes result sent to driver\n",
      "24/09/06 14:50:44 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 13) in 31 ms on ubuntu (executor driver) (2/3)\n",
      "24/09/06 14:50:44 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 15) in 33 ms on ubuntu (executor driver) (3/3)\n",
      "24/09/06 14:50:44 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:44 INFO DAGScheduler: ResultStage 5 (show at cell12.sc:3) finished in 0,058 s\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "24/09/06 14:50:44 INFO DAGScheduler: Job 5 finished: show at cell12.sc:3, took 0,068179 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|StudentID|Course|\n",
      "+---------+------+\n",
      "|        1| Spark|\n",
      "|        2| Scala|\n",
      "|        3|  Java|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd2\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mRow\u001b[39m] = MapPartitionsRDD[4] at map at cell12.sc:1\n",
       "\u001b[36mfromRDD2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [StudentID: string, Course: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd2 = rdd1.map(s => Row(s._1, s._2))\n",
    "val fromRDD2 = spark.createDataFrame(rdd2, schema2)\n",
    "fromRDD2.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f7656",
   "metadata": {},
   "source": [
    "### toDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d021abd",
   "metadata": {},
   "source": [
    "#### From List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "010978b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:45 INFO CodeGenerator: Code generated in 16.612429 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| _1|   _2|\n",
      "+---+-----+\n",
      "|  1|Spark|\n",
      "|  2|Scala|\n",
      "|  3| Java|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf11\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_1: string, _2: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df11 = data1.toDF()\n",
    "df11.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7da3bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|StudentID|Course|\n",
      "+---------+------+\n",
      "|        1| Spark|\n",
      "|        2| Scala|\n",
      "|        3|  Java|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.toDF(\"StudentID\", \"Course\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b56c3330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|StudentID|Course|\n",
      "+---------+------+\n",
      "|        1| Spark|\n",
      "|        2| Scala|\n",
      "|        3|  Java|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mcolumns\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m\"StudentID\"\u001b[39m, \u001b[32m\"Course\"\u001b[39m)\n",
       "\u001b[36mdf12\u001b[39m: \u001b[32mDataFrame\u001b[39m = [StudentID: string, Course: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val columns = List(\"StudentID\", \"Course\")\n",
    "val df12 = data1.toDF(columns: _*)\n",
    "df12.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f508c0da",
   "metadata": {},
   "source": [
    "#### fromRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14ca80f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:46 INFO SparkContext: Starting job: show at cell16.sc:2\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Got job 6 (show at cell16.sc:2) with 1 output partitions\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Final stage: ResultStage 6 (show at cell16.sc:2)\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[10] at show at cell16.sc:2), which has no missing parents\n",
      "24/09/06 14:50:46 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:46 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:46 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on ubuntu:37607 (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:46 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[10] at show at cell16.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:46 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:46 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 16) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/06 14:50:46 INFO Executor: Running task 0.0 in stage 6.0 (TID 16)\n",
      "24/09/06 14:50:46 INFO Executor: Finished task 0.0 in stage 6.0 (TID 16). 1452 bytes result sent to driver\n",
      "24/09/06 14:50:46 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 16) in 12 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:46 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:46 INFO DAGScheduler: ResultStage 6 (show at cell16.sc:2) finished in 0,029 s\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Job 6 finished: show at cell16.sc:2, took 0,035314 s\n",
      "24/09/06 14:50:46 INFO SparkContext: Starting job: show at cell16.sc:2\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Got job 7 (show at cell16.sc:2) with 4 output partitions\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Final stage: ResultStage 7 (show at cell16.sc:2)\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[10] at show at cell16.sc:2), which has no missing parents\n",
      "24/09/06 14:50:46 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:46 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:46 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on ubuntu:37607 (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:46 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 7 (MapPartitionsRDD[10] at show at cell16.sc:2) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\n",
      "24/09/06 14:50:46 INFO TaskSchedulerImpl: Adding task set 7.0 with 4 tasks resource profile 0\n",
      "24/09/06 14:50:46 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 17) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/06 14:50:46 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 18) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/06 14:50:46 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 19) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/06 14:50:46 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 20) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/06 14:50:46 INFO Executor: Running task 0.0 in stage 7.0 (TID 17)\n",
      "24/09/06 14:50:46 INFO Executor: Running task 1.0 in stage 7.0 (TID 18)\n",
      "24/09/06 14:50:46 INFO Executor: Running task 2.0 in stage 7.0 (TID 19)\n",
      "24/09/06 14:50:46 INFO Executor: Running task 3.0 in stage 7.0 (TID 20)\n",
      "24/09/06 14:50:46 INFO BlockManagerInfo: Removed broadcast_6_piece0 on ubuntu:37607 in memory (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:46 INFO Executor: Finished task 0.0 in stage 7.0 (TID 17). 1409 bytes result sent to driver\n",
      "24/09/06 14:50:46 INFO Executor: Finished task 2.0 in stage 7.0 (TID 19). 1409 bytes result sent to driver\n",
      "24/09/06 14:50:46 INFO Executor: Finished task 3.0 in stage 7.0 (TID 20). 1452 bytes result sent to driver\n",
      "24/09/06 14:50:46 INFO Executor: Finished task 1.0 in stage 7.0 (TID 18). 1447 bytes result sent to driver\n",
      "24/09/06 14:50:46 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 17) in 34 ms on ubuntu (executor driver) (1/4)\n",
      "24/09/06 14:50:46 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 19) in 34 ms on ubuntu (executor driver) (2/4)\n",
      "24/09/06 14:50:46 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 18) in 36 ms on ubuntu (executor driver) (3/4)\n",
      "24/09/06 14:50:46 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 20) in 37 ms on ubuntu (executor driver) (4/4)\n",
      "24/09/06 14:50:46 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:46 INFO DAGScheduler: ResultStage 7 (show at cell16.sc:2) finished in 0,094 s\n",
      "24/09/06 14:50:46 INFO BlockManagerInfo: Removed broadcast_2_piece0 on ubuntu:37607 in memory (size: 5.8 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Job 7 finished: show at cell16.sc:2, took 0,104813 s\n",
      "24/09/06 14:50:46 INFO BlockManagerInfo: Removed broadcast_1_piece0 on ubuntu:37607 in memory (size: 5.8 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:46 INFO SparkContext: Starting job: show at cell16.sc:2\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Got job 8 (show at cell16.sc:2) with 3 output partitions\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Final stage: ResultStage 8 (show at cell16.sc:2)\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:46 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ubuntu:37607 in memory (size: 5.8 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[10] at show at cell16.sc:2), which has no missing parents\n",
      "24/09/06 14:50:46 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:46 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:46 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on ubuntu:37607 (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:46 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:46 INFO BlockManagerInfo: Removed broadcast_5_piece0 on ubuntu:37607 in memory (size: 9.8 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 8 (MapPartitionsRDD[10] at show at cell16.sc:2) (first 15 tasks are for partitions Vector(5, 6, 7))\n",
      "24/09/06 14:50:46 INFO TaskSchedulerImpl: Adding task set 8.0 with 3 tasks resource profile 0\n",
      "24/09/06 14:50:46 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 21) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/06 14:50:46 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 22) (ubuntu, executor driver, partition 6, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/06 14:50:46 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 23) (ubuntu, executor driver, partition 7, PROCESS_LOCAL, 7667 bytes) \n",
      "24/09/06 14:50:46 INFO Executor: Running task 0.0 in stage 8.0 (TID 21)\n",
      "24/09/06 14:50:46 INFO Executor: Running task 2.0 in stage 8.0 (TID 23)\n",
      "24/09/06 14:50:46 INFO Executor: Running task 1.0 in stage 8.0 (TID 22)\n",
      "24/09/06 14:50:46 INFO BlockManagerInfo: Removed broadcast_3_piece0 on ubuntu:37607 in memory (size: 9.8 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:46 INFO Executor: Finished task 0.0 in stage 8.0 (TID 21). 1447 bytes result sent to driver\n",
      "24/09/06 14:50:46 INFO Executor: Finished task 2.0 in stage 8.0 (TID 23). 1447 bytes result sent to driver\n",
      "24/09/06 14:50:46 INFO Executor: Finished task 1.0 in stage 8.0 (TID 22). 1409 bytes result sent to driver\n",
      "24/09/06 14:50:46 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 21) in 25 ms on ubuntu (executor driver) (1/3)\n",
      "24/09/06 14:50:46 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 22) in 28 ms on ubuntu (executor driver) (2/3)\n",
      "24/09/06 14:50:46 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 23) in 30 ms on ubuntu (executor driver) (3/3)\n",
      "24/09/06 14:50:46 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:46 INFO DAGScheduler: ResultStage 8 (show at cell16.sc:2) finished in 0,052 s\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "24/09/06 14:50:46 INFO DAGScheduler: Job 8 finished: show at cell16.sc:2, took 0,077742 s\n",
      "24/09/06 14:50:46 INFO BlockManagerInfo: Removed broadcast_4_piece0 on ubuntu:37607 in memory (size: 9.8 KiB, free: 4.5 GiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| _1|   _2|\n",
      "+---+-----+\n",
      "|  1|Spark|\n",
      "|  2|Scala|\n",
      "|  3| Java|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfromRDD11\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_1: string, _2: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fromRDD11 = rdd1.toDF()\n",
    "fromRDD11.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02700001",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:47 INFO SparkContext: Starting job: show at cell17.sc:2\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Got job 9 (show at cell17.sc:2) with 1 output partitions\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Final stage: ResultStage 9 (show at cell17.sc:2)\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[13] at show at cell17.sc:2), which has no missing parents\n",
      "24/09/06 14:50:47 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:47 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:47 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on ubuntu:37607 (size: 5.8 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:47 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[13] at show at cell17.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:47 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:47 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 24) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/06 14:50:47 INFO Executor: Running task 0.0 in stage 9.0 (TID 24)\n",
      "24/09/06 14:50:47 INFO Executor: Finished task 0.0 in stage 9.0 (TID 24). 1409 bytes result sent to driver\n",
      "24/09/06 14:50:47 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 24) in 10 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:47 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:47 INFO DAGScheduler: ResultStage 9 (show at cell17.sc:2) finished in 0,023 s\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Job 9 finished: show at cell17.sc:2, took 0,029415 s\n",
      "24/09/06 14:50:47 INFO SparkContext: Starting job: show at cell17.sc:2\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Got job 10 (show at cell17.sc:2) with 4 output partitions\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Final stage: ResultStage 10 (show at cell17.sc:2)\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[13] at show at cell17.sc:2), which has no missing parents\n",
      "24/09/06 14:50:47 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:47 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:47 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on ubuntu:37607 (size: 5.8 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:47 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 10 (MapPartitionsRDD[13] at show at cell17.sc:2) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\n",
      "24/09/06 14:50:47 INFO TaskSchedulerImpl: Adding task set 10.0 with 4 tasks resource profile 0\n",
      "24/09/06 14:50:47 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 25) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/06 14:50:47 INFO TaskSetManager: Starting task 1.0 in stage 10.0 (TID 26) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/06 14:50:47 INFO TaskSetManager: Starting task 2.0 in stage 10.0 (TID 27) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/06 14:50:47 INFO TaskSetManager: Starting task 3.0 in stage 10.0 (TID 28) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/06 14:50:47 INFO Executor: Running task 2.0 in stage 10.0 (TID 27)\n",
      "24/09/06 14:50:47 INFO Executor: Running task 0.0 in stage 10.0 (TID 25)\n",
      "24/09/06 14:50:47 INFO Executor: Running task 1.0 in stage 10.0 (TID 26)\n",
      "24/09/06 14:50:47 INFO Executor: Running task 3.0 in stage 10.0 (TID 28)\n",
      "24/09/06 14:50:47 INFO Executor: Finished task 2.0 in stage 10.0 (TID 27). 1409 bytes result sent to driver\n",
      "24/09/06 14:50:47 INFO Executor: Finished task 1.0 in stage 10.0 (TID 26). 1490 bytes result sent to driver\n",
      "24/09/06 14:50:47 INFO Executor: Finished task 0.0 in stage 10.0 (TID 25). 1409 bytes result sent to driver\n",
      "24/09/06 14:50:47 INFO TaskSetManager: Finished task 2.0 in stage 10.0 (TID 27) in 15 ms on ubuntu (executor driver) (1/4)\n",
      "24/09/06 14:50:47 INFO TaskSetManager: Finished task 1.0 in stage 10.0 (TID 26) in 17 ms on ubuntu (executor driver) (2/4)\n",
      "24/09/06 14:50:47 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 25) in 19 ms on ubuntu (executor driver) (3/4)\n",
      "24/09/06 14:50:47 INFO Executor: Finished task 3.0 in stage 10.0 (TID 28). 1409 bytes result sent to driver\n",
      "24/09/06 14:50:47 INFO TaskSetManager: Finished task 3.0 in stage 10.0 (TID 28) in 22 ms on ubuntu (executor driver) (4/4)\n",
      "24/09/06 14:50:47 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:47 INFO DAGScheduler: ResultStage 10 (show at cell17.sc:2) finished in 0,036 s\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Job 10 finished: show at cell17.sc:2, took 0,044257 s\n",
      "24/09/06 14:50:47 INFO SparkContext: Starting job: show at cell17.sc:2\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Got job 11 (show at cell17.sc:2) with 3 output partitions\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Final stage: ResultStage 11 (show at cell17.sc:2)\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[13] at show at cell17.sc:2), which has no missing parents\n",
      "24/09/06 14:50:47 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:47 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:47 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on ubuntu:37607 (size: 5.8 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:47 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 11 (MapPartitionsRDD[13] at show at cell17.sc:2) (first 15 tasks are for partitions Vector(5, 6, 7))\n",
      "24/09/06 14:50:47 INFO TaskSchedulerImpl: Adding task set 11.0 with 3 tasks resource profile 0\n",
      "24/09/06 14:50:47 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 29) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/06 14:50:47 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 30) (ubuntu, executor driver, partition 6, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/06 14:50:47 INFO TaskSetManager: Starting task 2.0 in stage 11.0 (TID 31) (ubuntu, executor driver, partition 7, PROCESS_LOCAL, 7667 bytes) \n",
      "24/09/06 14:50:47 INFO Executor: Running task 2.0 in stage 11.0 (TID 31)\n",
      "24/09/06 14:50:47 INFO Executor: Running task 1.0 in stage 11.0 (TID 30)\n",
      "24/09/06 14:50:47 INFO Executor: Running task 0.0 in stage 11.0 (TID 29)\n",
      "24/09/06 14:50:47 INFO Executor: Finished task 2.0 in stage 11.0 (TID 31). 1447 bytes result sent to driver\n",
      "24/09/06 14:50:47 INFO Executor: Finished task 0.0 in stage 11.0 (TID 29). 1447 bytes result sent to driver\n",
      "24/09/06 14:50:47 INFO Executor: Finished task 1.0 in stage 11.0 (TID 30). 1452 bytes result sent to driver\n",
      "24/09/06 14:50:47 INFO TaskSetManager: Finished task 2.0 in stage 11.0 (TID 31) in 15 ms on ubuntu (executor driver) (1/3)\n",
      "24/09/06 14:50:47 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 29) in 23 ms on ubuntu (executor driver) (2/3)\n",
      "24/09/06 14:50:47 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 30) in 23 ms on ubuntu (executor driver) (3/3)\n",
      "24/09/06 14:50:47 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:47 INFO DAGScheduler: ResultStage 11 (show at cell17.sc:2) finished in 0,038 s\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Job 11 finished: show at cell17.sc:2, took 0,047254 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|StudentID|Course|\n",
      "+---------+------+\n",
      "|        1| Spark|\n",
      "|        2| Scala|\n",
      "|        3|  Java|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfromRDD12\u001b[39m: \u001b[32mDataFrame\u001b[39m = [StudentID: string, Course: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fromRDD12 = rdd1.toDF(columns: _*)\n",
    "fromRDD12.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8949d910",
   "metadata": {},
   "source": [
    "### fromFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24814ffa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:47 INFO InMemoryFileIndex: It took 14 ms to list leaf files for 1 paths.\n",
      "24/09/06 14:50:47 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
      "24/09/06 14:50:47 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:47 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:47 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 198.5 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:47 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:47 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on ubuntu:37607 (size: 33.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:47 INFO SparkContext: Created broadcast 12 from load at cell18.sc:1\n",
      "24/09/06 14:50:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:47 INFO SparkContext: Starting job: load at cell18.sc:1\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Got job 12 (load at cell18.sc:1) with 1 output partitions\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Final stage: ResultStage 12 (load at cell18.sc:1)\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[17] at load at cell18.sc:1), which has no missing parents\n",
      "24/09/06 14:50:47 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 15.8 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:47 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:47 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on ubuntu:37607 (size: 7.4 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:47 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[17] at load at cell18.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:47 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:47 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 32) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/06 14:50:47 INFO Executor: Running task 0.0 in stage 12.0 (TID 32)\n",
      "24/09/06 14:50:47 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:47 INFO CodeGenerator: Code generated in 7.393876 ms\n",
      "24/09/06 14:50:47 INFO Executor: Finished task 0.0 in stage 12.0 (TID 32). 2076 bytes result sent to driver\n",
      "24/09/06 14:50:47 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 32) in 133 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:47 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:47 INFO DAGScheduler: ResultStage 12 (load at cell18.sc:1) finished in 0,162 s\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
      "24/09/06 14:50:47 INFO DAGScheduler: Job 12 finished: load at cell18.sc:1, took 0,169123 s\n",
      "24/09/06 14:50:47 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:47 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:48 INFO CodeGenerator: Code generated in 13.992865 ms\n",
      "24/09/06 14:50:48 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:48 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:48 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:48 INFO SparkContext: Created broadcast 14 from show at cell18.sc:2\n",
      "24/09/06 14:50:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:48 INFO SparkContext: Starting job: show at cell18.sc:2\n",
      "24/09/06 14:50:48 INFO DAGScheduler: Got job 13 (show at cell18.sc:2) with 1 output partitions\n",
      "24/09/06 14:50:48 INFO DAGScheduler: Final stage: ResultStage 13 (show at cell18.sc:2)\n",
      "24/09/06 14:50:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:48 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[21] at show at cell18.sc:2), which has no missing parents\n",
      "24/09/06 14:50:48 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 17.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:48 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:48 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on ubuntu:37607 (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:48 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[21] at show at cell18.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:48 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:48 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 33) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/06 14:50:48 INFO Executor: Running task 0.0 in stage 13.0 (TID 33)\n",
      "24/09/06 14:50:48 INFO CodeGenerator: Code generated in 11.459782 ms\n",
      "24/09/06 14:50:48 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:48 INFO CodeGenerator: Code generated in 11.183348 ms\n",
      "24/09/06 14:50:48 INFO Executor: Finished task 0.0 in stage 13.0 (TID 33). 2480 bytes result sent to driver\n",
      "24/09/06 14:50:48 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 33) in 64 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:48 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:48 INFO DAGScheduler: ResultStage 13 (show at cell18.sc:2) finished in 0,076 s\n",
      "24/09/06 14:50:48 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
      "24/09/06 14:50:48 INFO DAGScheduler: Job 13 finished: show at cell18.sc:2, took 0,080365 s\n",
      "24/09/06 14:50:48 INFO CodeGenerator: Code generated in 8.312624 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "|Address                                               |Birthdate          |Country       |CustomerID|Email                   |Name           |Username        |\n",
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "|Unit 1047 Box 4089\\nDPO AA 57348                      |1994-02-20 00:46:27|United Kingdom|12346     |cooperalexis@hotmail.com|Lindsay Cowan  |valenciajennifer|\n",
      "|55711 Janet Plaza Apt. 865\\nChristinachester, CT 62716|1988-06-21 00:15:34|Iceland       |12347     |timothy78@hotmail.com   |Katherine David|hillrachel      |\n",
      "|Unit 2676 Box 9352\\nDPO AA 38560                      |1974-11-26 15:30:20|Finland       |12348     |tcrawford@gmail.com     |Leslie Martinez|serranobrian    |\n",
      "|2765 Powers Meadow\\nHeatherfurt, CT 53165             |1977-05-06 23:57:35|Italy         |12349     |dustin37@yahoo.com      |Brad Cardenas  |charleshudson   |\n",
      "|17677 Mark Crest\\nWalterberg, IA 39017                |1996-09-13 19:14:27|Norway        |12350     |amyholland@yahoo.com    |Natalie Ford   |gregoryharrison |\n",
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfromFile1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Address: string, Birthdate: string ... 5 more fields]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fromFile1 = spark.read.format(\"json\").load(\"data/customer_data.json\")\n",
    "fromFile1.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea6cd10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Birthdate: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fromFile1.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbd6f48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mfileSchema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Address\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Birthdate\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Country\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CustomerID\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Email\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Name\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Username\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {})\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fileSchema = StructType(\n",
    "                    StructField(\"Address\", StringType, true) ::\n",
    "                    StructField(\"Birthdate\", StringType, true) ::\n",
    "                    StructField(\"Country\", StringType, true) ::\n",
    "                    StructField(\"CustomerID\", StringType, true) ::\n",
    "                    StructField(\"Email\", StringType, true) ::\n",
    "                    StructField(\"Name\", StringType, true) ::\n",
    "                    StructField(\"Username\", StringType, true) ::\n",
    "                    Nil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d51c6e9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:48 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "24/09/06 14:50:48 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:48 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:48 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:49 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:49 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:49 INFO SparkContext: Created broadcast 16 from show at cell21.sc:2\n",
      "24/09/06 14:50:49 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:49 INFO SparkContext: Starting job: show at cell21.sc:2\n",
      "24/09/06 14:50:49 INFO DAGScheduler: Got job 14 (show at cell21.sc:2) with 1 output partitions\n",
      "24/09/06 14:50:49 INFO DAGScheduler: Final stage: ResultStage 14 (show at cell21.sc:2)\n",
      "24/09/06 14:50:49 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:49 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:49 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[25] at show at cell21.sc:2), which has no missing parents\n",
      "24/09/06 14:50:49 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 17.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:49 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:49 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on ubuntu:37607 (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:49 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[25] at show at cell21.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:49 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:49 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 34) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/06 14:50:49 INFO Executor: Running task 0.0 in stage 14.0 (TID 34)\n",
      "24/09/06 14:50:49 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:49 INFO Executor: Finished task 0.0 in stage 14.0 (TID 34). 2480 bytes result sent to driver\n",
      "24/09/06 14:50:49 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 34) in 16 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:49 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:49 INFO DAGScheduler: ResultStage 14 (show at cell21.sc:2) finished in 0,026 s\n",
      "24/09/06 14:50:49 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
      "24/09/06 14:50:49 INFO DAGScheduler: Job 14 finished: show at cell21.sc:2, took 0,030120 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "|Address                                               |Birthdate          |Country       |CustomerID|Email                   |Name           |Username        |\n",
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "|Unit 1047 Box 4089\\nDPO AA 57348                      |1994-02-20 00:46:27|United Kingdom|12346     |cooperalexis@hotmail.com|Lindsay Cowan  |valenciajennifer|\n",
      "|55711 Janet Plaza Apt. 865\\nChristinachester, CT 62716|1988-06-21 00:15:34|Iceland       |12347     |timothy78@hotmail.com   |Katherine David|hillrachel      |\n",
      "|Unit 2676 Box 9352\\nDPO AA 38560                      |1974-11-26 15:30:20|Finland       |12348     |tcrawford@gmail.com     |Leslie Martinez|serranobrian    |\n",
      "|2765 Powers Meadow\\nHeatherfurt, CT 53165             |1977-05-06 23:57:35|Italy         |12349     |dustin37@yahoo.com      |Brad Cardenas  |charleshudson   |\n",
      "|17677 Mark Crest\\nWalterberg, IA 39017                |1996-09-13 19:14:27|Norway        |12350     |amyholland@yahoo.com    |Natalie Ford   |gregoryharrison |\n",
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfromFile2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Address: string, Birthdate: string ... 5 more fields]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fromFile2 = spark.read.format(\"json\").schema(fileSchema).load(\"data/customer_data.json\")\n",
    "fromFile2.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab7d210",
   "metadata": {},
   "source": [
    "## 2 Основные операции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "067a8858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:49 INFO BlockManagerInfo: Removed broadcast_10_piece0 on ubuntu:37607 in memory (size: 5.8 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:49 INFO BlockManagerInfo: Removed broadcast_7_piece0 on ubuntu:37607 in memory (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:49 INFO BlockManagerInfo: Removed broadcast_17_piece0 on ubuntu:37607 in memory (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:49 INFO BlockManagerInfo: Removed broadcast_14_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:49 INFO BlockManagerInfo: Removed broadcast_13_piece0 on ubuntu:37607 in memory (size: 7.4 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:49 INFO BlockManagerInfo: Removed broadcast_12_piece0 on ubuntu:37607 in memory (size: 33.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:49 INFO BlockManagerInfo: Removed broadcast_9_piece0 on ubuntu:37607 in memory (size: 5.8 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:49 INFO BlockManagerInfo: Removed broadcast_15_piece0 on ubuntu:37607 in memory (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:49 INFO BlockManagerInfo: Removed broadcast_11_piece0 on ubuntu:37607 in memory (size: 5.8 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:49 INFO BlockManagerInfo: Removed broadcast_8_piece0 on ubuntu:37607 in memory (size: 5.7 KiB, free: 4.5 GiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mcustomerDf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Address: string, Birthdate: string ... 5 more fields]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val customerDf = fromFile1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c371153c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Birthdate: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e5bcb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres24\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Address\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Birthdate\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Country\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CustomerID\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Email\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Name\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Username\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {})\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerDf.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7245a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres25\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m\"Address\"\u001b[39m, \u001b[32m\"StringType\"\u001b[39m),\n",
       "  (\u001b[32m\"Birthdate\"\u001b[39m, \u001b[32m\"StringType\"\u001b[39m),\n",
       "  (\u001b[32m\"Country\"\u001b[39m, \u001b[32m\"StringType\"\u001b[39m),\n",
       "  (\u001b[32m\"CustomerID\"\u001b[39m, \u001b[32m\"StringType\"\u001b[39m),\n",
       "  (\u001b[32m\"Email\"\u001b[39m, \u001b[32m\"StringType\"\u001b[39m),\n",
       "  (\u001b[32m\"Name\"\u001b[39m, \u001b[32m\"StringType\"\u001b[39m),\n",
       "  (\u001b[32m\"Username\"\u001b[39m, \u001b[32m\"StringType\"\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerDf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cc65050",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:50 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:50 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:50 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:50 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:50 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:50 INFO SparkContext: Created broadcast 18 from head at cell26.sc:1\n",
      "24/09/06 14:50:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:50 INFO SparkContext: Starting job: head at cell26.sc:1\n",
      "24/09/06 14:50:50 INFO DAGScheduler: Got job 15 (head at cell26.sc:1) with 1 output partitions\n",
      "24/09/06 14:50:50 INFO DAGScheduler: Final stage: ResultStage 15 (head at cell26.sc:1)\n",
      "24/09/06 14:50:50 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:50 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:50 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[28] at head at cell26.sc:1), which has no missing parents\n",
      "24/09/06 14:50:50 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 11.6 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:50 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:50 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on ubuntu:37607 (size: 6.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:50 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[28] at head at cell26.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:50 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:50 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 35) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/06 14:50:50 INFO Executor: Running task 0.0 in stage 15.0 (TID 35)\n",
      "24/09/06 14:50:50 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:50 INFO Executor: Finished task 0.0 in stage 15.0 (TID 35). 1623 bytes result sent to driver\n",
      "24/09/06 14:50:50 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 35) in 12 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:50 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:50 INFO DAGScheduler: ResultStage 15 (head at cell26.sc:1) finished in 0,027 s\n",
      "24/09/06 14:50:50 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished\n",
      "24/09/06 14:50:50 INFO DAGScheduler: Job 15 finished: head at cell26.sc:1, took 0,033646 s\n",
      "24/09/06 14:50:50 INFO CodeGenerator: Code generated in 11.989237 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mh\u001b[39m: \u001b[32mRow\u001b[39m = [Unit 1047 Box 4089\n",
       "DPO AA 57348,1994-02-20 00:46:27,United Kingdom,12346,cooperalexis@hotmail.com,Lindsay Cowan,valenciajennifer]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val h = customerDf.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "80a93dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres27\u001b[39m: \u001b[32mAny\u001b[39m = \u001b[32m\"1994-02-20 00:46:27\"\u001b[39m"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a9c61d",
   "metadata": {},
   "source": [
    "#### Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b78c5d8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:50 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:50 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:50 INFO CodeGenerator: Code generated in 5.004826 ms\n",
      "24/09/06 14:50:50 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:50 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:50 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:50 INFO SparkContext: Created broadcast 20 from show at cell28.sc:1\n",
      "24/09/06 14:50:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:50 INFO SparkContext: Starting job: show at cell28.sc:1\n",
      "24/09/06 14:50:50 INFO DAGScheduler: Got job 16 (show at cell28.sc:1) with 1 output partitions\n",
      "24/09/06 14:50:50 INFO DAGScheduler: Final stage: ResultStage 16 (show at cell28.sc:1)\n",
      "24/09/06 14:50:50 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:50 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:50 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[32] at show at cell28.sc:1), which has no missing parents\n",
      "24/09/06 14:50:50 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 14.6 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:50 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:50 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on ubuntu:37607 (size: 7.1 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:50 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[32] at show at cell28.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:50 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:50 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 36) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/06 14:50:50 INFO Executor: Running task 0.0 in stage 16.0 (TID 36)\n",
      "24/09/06 14:50:50 INFO CodeGenerator: Code generated in 7.121751 ms\n",
      "24/09/06 14:50:50 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:50 INFO CodeGenerator: Code generated in 4.816846 ms\n",
      "24/09/06 14:50:50 INFO Executor: Finished task 0.0 in stage 16.0 (TID 36). 2153 bytes result sent to driver\n",
      "24/09/06 14:50:50 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 36) in 25 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:50 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:50 INFO DAGScheduler: ResultStage 16 (show at cell28.sc:1) finished in 0,032 s\n",
      "24/09/06 14:50:50 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished\n",
      "24/09/06 14:50:50 INFO DAGScheduler: Job 16 finished: show at cell28.sc:1, took 0,037573 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+\n",
      "|          Birthdate|       Country|\n",
      "+-------------------+--------------+\n",
      "|1994-02-20 00:46:27|United Kingdom|\n",
      "|1988-06-21 00:15:34|       Iceland|\n",
      "|1974-11-26 15:30:20|       Finland|\n",
      "|1977-05-06 23:57:35|         Italy|\n",
      "|1996-09-13 19:14:27|        Norway|\n",
      "|1969-06-21 03:39:20|        Norway|\n",
      "|1993-02-25 18:37:29|       Bahrain|\n",
      "|1993-03-13 12:37:34|         Spain|\n",
      "|1972-11-10 12:01:08|       Bahrain|\n",
      "|1973-01-13 17:17:26|      Portugal|\n",
      "|1989-11-24 17:12:54|   Switzerland|\n",
      "|1977-06-19 22:35:52|       Austria|\n",
      "|1983-09-21 05:22:18|        Cyprus|\n",
      "|1980-10-28 17:25:59|       Austria|\n",
      "|1982-09-01 09:12:57|       Belgium|\n",
      "|1979-02-03 03:42:47|       Belgium|\n",
      "|1974-12-21 13:27:20|   Unspecified|\n",
      "|1990-07-17 15:47:12|       Belgium|\n",
      "|1981-07-10 00:35:00|        Cyprus|\n",
      "|1989-12-26 00:58:01|       Denmark|\n",
      "+-------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.select(\"Birthdate\", \"Country\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1822538d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:51 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:51 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:51 INFO CodeGenerator: Code generated in 6.349976 ms\n",
      "24/09/06 14:50:51 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO SparkContext: Created broadcast 22 from show at cell29.sc:1\n",
      "24/09/06 14:50:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:51 INFO SparkContext: Starting job: show at cell29.sc:1\n",
      "24/09/06 14:50:51 INFO DAGScheduler: Got job 17 (show at cell29.sc:1) with 1 output partitions\n",
      "24/09/06 14:50:51 INFO DAGScheduler: Final stage: ResultStage 17 (show at cell29.sc:1)\n",
      "24/09/06 14:50:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:51 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:51 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[36] at show at cell29.sc:1), which has no missing parents\n",
      "24/09/06 14:50:51 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 14.2 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on ubuntu:37607 (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[36] at show at cell29.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:51 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:51 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 37) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/06 14:50:51 INFO Executor: Running task 0.0 in stage 17.0 (TID 37)\n",
      "24/09/06 14:50:51 INFO CodeGenerator: Code generated in 6.974534 ms\n",
      "24/09/06 14:50:51 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:51 INFO Executor: Finished task 0.0 in stage 17.0 (TID 37). 1753 bytes result sent to driver\n",
      "24/09/06 14:50:51 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 37) in 19 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:51 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:51 INFO DAGScheduler: ResultStage 17 (show at cell29.sc:1) finished in 0,030 s\n",
      "24/09/06 14:50:51 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished\n",
      "24/09/06 14:50:51 INFO DAGScheduler: Job 17 finished: show at cell29.sc:1, took 0,037092 s\n",
      "24/09/06 14:50:51 INFO CodeGenerator: Code generated in 6.103382 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|       Country|\n",
      "+--------------+\n",
      "|United Kingdom|\n",
      "|       Iceland|\n",
      "|       Finland|\n",
      "|         Italy|\n",
      "|        Norway|\n",
      "|        Norway|\n",
      "|       Bahrain|\n",
      "|         Spain|\n",
      "|       Bahrain|\n",
      "|      Portugal|\n",
      "|   Switzerland|\n",
      "|       Austria|\n",
      "|        Cyprus|\n",
      "|       Austria|\n",
      "|       Belgium|\n",
      "|       Belgium|\n",
      "|   Unspecified|\n",
      "|       Belgium|\n",
      "|        Cyprus|\n",
      "|       Denmark|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.select(col(\"Country\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ee0f2ed4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:51 INFO BlockManagerInfo: Removed broadcast_23_piece0 on ubuntu:37607 in memory (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO BlockManagerInfo: Removed broadcast_20_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO BlockManagerInfo: Removed broadcast_19_piece0 on ubuntu:37607 in memory (size: 6.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO BlockManagerInfo: Removed broadcast_21_piece0 on ubuntu:37607 in memory (size: 7.1 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO BlockManagerInfo: Removed broadcast_16_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO BlockManagerInfo: Removed broadcast_18_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:51 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:51 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO SparkContext: Created broadcast 24 from show at cell30.sc:1\n",
      "24/09/06 14:50:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:51 INFO SparkContext: Starting job: show at cell30.sc:1\n",
      "24/09/06 14:50:51 INFO DAGScheduler: Got job 18 (show at cell30.sc:1) with 1 output partitions\n",
      "24/09/06 14:50:51 INFO DAGScheduler: Final stage: ResultStage 18 (show at cell30.sc:1)\n",
      "24/09/06 14:50:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:51 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:51 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[40] at show at cell30.sc:1), which has no missing parents\n",
      "24/09/06 14:50:51 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 14.2 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on ubuntu:37607 (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[40] at show at cell30.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:51 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:51 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 38) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/06 14:50:51 INFO Executor: Running task 0.0 in stage 18.0 (TID 38)\n",
      "24/09/06 14:50:51 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:51 INFO Executor: Finished task 0.0 in stage 18.0 (TID 38). 1899 bytes result sent to driver\n",
      "24/09/06 14:50:51 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 38) in 15 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:51 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:51 INFO DAGScheduler: ResultStage 18 (show at cell30.sc:1) finished in 0,026 s\n",
      "24/09/06 14:50:51 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished\n",
      "24/09/06 14:50:51 INFO DAGScheduler: Job 18 finished: show at cell30.sc:1, took 0,032030 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               Date|\n",
      "+-------------------+\n",
      "|1994-02-20 00:46:27|\n",
      "|1988-06-21 00:15:34|\n",
      "|1974-11-26 15:30:20|\n",
      "|1977-05-06 23:57:35|\n",
      "|1996-09-13 19:14:27|\n",
      "|1969-06-21 03:39:20|\n",
      "|1993-02-25 18:37:29|\n",
      "|1993-03-13 12:37:34|\n",
      "|1972-11-10 12:01:08|\n",
      "|1973-01-13 17:17:26|\n",
      "|1989-11-24 17:12:54|\n",
      "|1977-06-19 22:35:52|\n",
      "|1983-09-21 05:22:18|\n",
      "|1980-10-28 17:25:59|\n",
      "|1982-09-01 09:12:57|\n",
      "|1979-02-03 03:42:47|\n",
      "|1974-12-21 13:27:20|\n",
      "|1990-07-17 15:47:12|\n",
      "|1981-07-10 00:35:00|\n",
      "|1989-12-26 00:58:01|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.selectExpr(\"Birthdate as Date\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "250879d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:51 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:51 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:51 INFO CodeGenerator: Code generated in 9.595799 ms\n",
      "24/09/06 14:50:51 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO SparkContext: Created broadcast 26 from show at cell31.sc:1\n",
      "24/09/06 14:50:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:51 INFO SparkContext: Starting job: show at cell31.sc:1\n",
      "24/09/06 14:50:51 INFO DAGScheduler: Got job 19 (show at cell31.sc:1) with 1 output partitions\n",
      "24/09/06 14:50:51 INFO DAGScheduler: Final stage: ResultStage 19 (show at cell31.sc:1)\n",
      "24/09/06 14:50:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:51 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:51 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[44] at show at cell31.sc:1), which has no missing parents\n",
      "24/09/06 14:50:51 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 17.1 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on ubuntu:37607 (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:51 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[44] at show at cell31.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:51 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:51 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 39) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/06 14:50:51 INFO Executor: Running task 0.0 in stage 19.0 (TID 39)\n",
      "24/09/06 14:50:51 INFO CodeGenerator: Code generated in 9.985652 ms\n",
      "24/09/06 14:50:51 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:52 INFO Executor: Finished task 0.0 in stage 19.0 (TID 39). 4695 bytes result sent to driver\n",
      "24/09/06 14:50:52 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 39) in 29 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:52 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:52 INFO DAGScheduler: ResultStage 19 (show at cell31.sc:1) finished in 0,046 s\n",
      "24/09/06 14:50:52 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished\n",
      "24/09/06 14:50:52 INFO DAGScheduler: Job 19 finished: show at cell31.sc:1, took 0,049134 s\n",
      "24/09/06 14:50:52 INFO CodeGenerator: Code generated in 9.237065 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+----+\n",
      "|             Address|          Birthdate|       Country|CustomerID|               Email|             Name|        Username|Flag|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+----+\n",
      "|Unit 1047 Box 408...|1994-02-20 00:46:27|United Kingdom|     12346|cooperalexis@hotm...|    Lindsay Cowan|valenciajennifer|true|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|  Katherine David|      hillrachel|true|\n",
      "|Unit 2676 Box 935...|1974-11-26 15:30:20|       Finland|     12348| tcrawford@gmail.com|  Leslie Martinez|    serranobrian|true|\n",
      "|2765 Powers Meado...|1977-05-06 23:57:35|         Italy|     12349|  dustin37@yahoo.com|    Brad Cardenas|   charleshudson|true|\n",
      "|17677 Mark Crest\\...|1996-09-13 19:14:27|        Norway|     12350|amyholland@yahoo.com|     Natalie Ford| gregoryharrison|true|\n",
      "|50047 Smith Point...|1969-06-21 03:39:20|        Norway|     12352| vcarter@hotmail.com|      Dana Clarke|          hmyers|true|\n",
      "|633 Miller Turnpi...|1993-02-25 18:37:29|       Bahrain|     12353|   laura34@yahoo.com|     Gary Nichols|  andrewhamilton|true|\n",
      "|38456 Rachael Cau...|1993-03-13 12:37:34|         Spain|     12354|   zmelton@gmail.com|       John Parks|      matthewray|true|\n",
      "|4140 Pamela Hollo...|1972-11-10 12:01:08|       Bahrain|     12355|   scott50@yahoo.com|Jennifer Lawrence|          glopez|true|\n",
      "|8681 Karen Roads ...|1973-01-13 17:17:26|      Portugal|     12356|josephmacias@hotm...|    James Sanchez|        wesley20|true|\n",
      "|18637 Jessica Rid...|1989-11-24 17:12:54|   Switzerland|     12357|michael16@hotmail...|     Ashley Lopez|     thomasdavid|true|\n",
      "|2129 Joel Rapids\\...|1977-06-19 22:35:52|       Austria|     12358|michaelespinoza@g...| Dr. Angela Brown|      patricia44|true|\n",
      "|86636 Maria Viadu...|1983-09-21 05:22:18|        Cyprus|     12359|  ryanpena@yahoo.com|        John Vega|     nelsonmaria|true|\n",
      "|1579 Young Trail\\...|1980-10-28 17:25:59|       Austria|     12360|briannafrost@yaho...|     Lauren Clark|   portermichael|true|\n",
      "|USNS Howard\\nFPO ...|1982-09-01 09:12:57|       Belgium|     12361|virginia36@hotmai...|Jacqueline Haynes|   johnsonshelly|true|\n",
      "|70092 Adams Prair...|1979-02-03 03:42:47|       Belgium|     12362|   april04@gmail.com|     Brian Flores|    hunterdaniel|true|\n",
      "|7322 Owens Inlet ...|1974-12-21 13:27:20|   Unspecified|     12363|   omolina@gmail.com|Christopher Gomez|         james75|true|\n",
      "|86176 Katherine C...|1990-07-17 15:47:12|       Belgium|     12364|barbaraduncan@gma...|     Robert Burns|          eric10|true|\n",
      "|932 Jeremy Spring...|1981-07-10 00:35:00|        Cyprus|     12365|nicoleanderson@ho...|    Joshua Parker|     millerrenee|true|\n",
      "|USNV Chavez\\nFPO ...|1989-12-26 00:58:01|       Denmark|     12367|   aaron99@yahoo.com|Christine Douglas|       michael58|true|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.withColumn(\"Flag\", lit(true)).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7df0d8f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:52 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:52 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:52 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:52 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:52 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:52 INFO SparkContext: Created broadcast 28 from show at cell32.sc:2\n",
      "24/09/06 14:50:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:52 INFO SparkContext: Starting job: show at cell32.sc:2\n",
      "24/09/06 14:50:52 INFO DAGScheduler: Got job 20 (show at cell32.sc:2) with 1 output partitions\n",
      "24/09/06 14:50:52 INFO DAGScheduler: Final stage: ResultStage 20 (show at cell32.sc:2)\n",
      "24/09/06 14:50:52 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:52 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:52 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[48] at show at cell32.sc:2), which has no missing parents\n",
      "24/09/06 14:50:52 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 17.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:52 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:52 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on ubuntu:37607 (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:52 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[48] at show at cell32.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:52 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:52 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 40) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/06 14:50:52 INFO Executor: Running task 0.0 in stage 20.0 (TID 40)\n",
      "24/09/06 14:50:52 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:52 INFO Executor: Finished task 0.0 in stage 20.0 (TID 40). 4581 bytes result sent to driver\n",
      "24/09/06 14:50:52 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 40) in 14 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:52 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:52 INFO DAGScheduler: ResultStage 20 (show at cell32.sc:2) finished in 0,021 s\n",
      "24/09/06 14:50:52 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished\n",
      "24/09/06 14:50:52 INFO DAGScheduler: Job 20 finished: show at cell32.sc:2, took 0,026113 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "|             Address|               Date|       Country|CustomerID|               Email|             Name|        Username|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "|Unit 1047 Box 408...|1994-02-20 00:46:27|United Kingdom|     12346|cooperalexis@hotm...|    Lindsay Cowan|valenciajennifer|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|  Katherine David|      hillrachel|\n",
      "|Unit 2676 Box 935...|1974-11-26 15:30:20|       Finland|     12348| tcrawford@gmail.com|  Leslie Martinez|    serranobrian|\n",
      "|2765 Powers Meado...|1977-05-06 23:57:35|         Italy|     12349|  dustin37@yahoo.com|    Brad Cardenas|   charleshudson|\n",
      "|17677 Mark Crest\\...|1996-09-13 19:14:27|        Norway|     12350|amyholland@yahoo.com|     Natalie Ford| gregoryharrison|\n",
      "|50047 Smith Point...|1969-06-21 03:39:20|        Norway|     12352| vcarter@hotmail.com|      Dana Clarke|          hmyers|\n",
      "|633 Miller Turnpi...|1993-02-25 18:37:29|       Bahrain|     12353|   laura34@yahoo.com|     Gary Nichols|  andrewhamilton|\n",
      "|38456 Rachael Cau...|1993-03-13 12:37:34|         Spain|     12354|   zmelton@gmail.com|       John Parks|      matthewray|\n",
      "|4140 Pamela Hollo...|1972-11-10 12:01:08|       Bahrain|     12355|   scott50@yahoo.com|Jennifer Lawrence|          glopez|\n",
      "|8681 Karen Roads ...|1973-01-13 17:17:26|      Portugal|     12356|josephmacias@hotm...|    James Sanchez|        wesley20|\n",
      "|18637 Jessica Rid...|1989-11-24 17:12:54|   Switzerland|     12357|michael16@hotmail...|     Ashley Lopez|     thomasdavid|\n",
      "|2129 Joel Rapids\\...|1977-06-19 22:35:52|       Austria|     12358|michaelespinoza@g...| Dr. Angela Brown|      patricia44|\n",
      "|86636 Maria Viadu...|1983-09-21 05:22:18|        Cyprus|     12359|  ryanpena@yahoo.com|        John Vega|     nelsonmaria|\n",
      "|1579 Young Trail\\...|1980-10-28 17:25:59|       Austria|     12360|briannafrost@yaho...|     Lauren Clark|   portermichael|\n",
      "|USNS Howard\\nFPO ...|1982-09-01 09:12:57|       Belgium|     12361|virginia36@hotmai...|Jacqueline Haynes|   johnsonshelly|\n",
      "|70092 Adams Prair...|1979-02-03 03:42:47|       Belgium|     12362|   april04@gmail.com|     Brian Flores|    hunterdaniel|\n",
      "|7322 Owens Inlet ...|1974-12-21 13:27:20|   Unspecified|     12363|   omolina@gmail.com|Christopher Gomez|         james75|\n",
      "|86176 Katherine C...|1990-07-17 15:47:12|       Belgium|     12364|barbaraduncan@gma...|     Robert Burns|          eric10|\n",
      "|932 Jeremy Spring...|1981-07-10 00:35:00|        Cyprus|     12365|nicoleanderson@ho...|    Joshua Parker|     millerrenee|\n",
      "|USNV Chavez\\nFPO ...|1989-12-26 00:58:01|       Denmark|     12367|   aaron99@yahoo.com|Christine Douglas|       michael58|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdfRenamed\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Address: string, Date: string ... 5 more fields]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfRenamed = customerDf.withColumnRenamed(\"Birthdate\", \"Date\")\n",
    "dfRenamed.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4086588b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres33_0\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Address\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Birthdate\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Country\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CustomerID\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Email\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Name\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Username\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {})\n",
       ")\n",
       "\u001b[36mres33_1\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Address\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Date\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Country\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CustomerID\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Email\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Name\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Username\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {})\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerDf.schema\n",
    "dfRenamed.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39bab1b",
   "metadata": {},
   "source": [
    "#### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30bc0966",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:52 INFO FileSourceStrategy: Pushed Filters: IsNotNull(Country),EqualTo(Country,Norway)\n",
      "24/09/06 14:50:52 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(Country#178),(Country#178 = Norway)\n",
      "24/09/06 14:50:52 INFO CodeGenerator: Code generated in 9.89809 ms\n",
      "24/09/06 14:50:52 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:52 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:52 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:52 INFO SparkContext: Created broadcast 30 from show at cell34.sc:1\n",
      "24/09/06 14:50:52 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:52 INFO SparkContext: Starting job: show at cell34.sc:1\n",
      "24/09/06 14:50:52 INFO DAGScheduler: Got job 21 (show at cell34.sc:1) with 1 output partitions\n",
      "24/09/06 14:50:52 INFO DAGScheduler: Final stage: ResultStage 21 (show at cell34.sc:1)\n",
      "24/09/06 14:50:52 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:52 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:52 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[52] at show at cell34.sc:1), which has no missing parents\n",
      "24/09/06 14:50:52 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 17.7 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:52 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:52 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on ubuntu:37607 (size: 7.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:52 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[52] at show at cell34.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:52 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:52 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 41) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/06 14:50:52 INFO Executor: Running task 0.0 in stage 21.0 (TID 41)\n",
      "24/09/06 14:50:52 INFO CodeGenerator: Code generated in 8.867638 ms\n",
      "24/09/06 14:50:52 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:52 INFO CodeGenerator: Code generated in 4.294017 ms\n",
      "24/09/06 14:50:52 INFO Executor: Finished task 0.0 in stage 21.0 (TID 41). 3059 bytes result sent to driver\n",
      "24/09/06 14:50:52 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 41) in 47 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:52 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:52 INFO DAGScheduler: ResultStage 21 (show at cell34.sc:1) finished in 0,058 s\n",
      "24/09/06 14:50:52 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished\n",
      "24/09/06 14:50:52 INFO DAGScheduler: Job 21 finished: show at cell34.sc:1, took 0,062905 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------+----------+--------------------+----------------+---------------+\n",
      "|             Address|          Birthdate|Country|CustomerID|               Email|            Name|       Username|\n",
      "+--------------------+-------------------+-------+----------+--------------------+----------------+---------------+\n",
      "|17677 Mark Crest\\...|1996-09-13 19:14:27| Norway|     12350|amyholland@yahoo.com|    Natalie Ford|gregoryharrison|\n",
      "|50047 Smith Point...|1969-06-21 03:39:20| Norway|     12352| vcarter@hotmail.com|     Dana Clarke|         hmyers|\n",
      "|0297 Jacob Ranch ...|1990-06-01 14:49:52| Norway|     12381|douglaschavez@hot...|   Matthew Jones|    stephanie68|\n",
      "|3102 Hopkins Walk...|1976-06-19 08:10:24| Norway|     12430|crystalromero@hot...|       Lisa Tate|       pgilbert|\n",
      "|637 Philip Lock S...|1984-06-06 09:36:14| Norway|     12432|jessica87@hotmail...|  Cheryl Herring|mathewsnicholas|\n",
      "|546 Tyler Prairie...|1985-05-27 10:39:47| Norway|     12433|mariahmcpherson@g...|  Kaitlin Miller|         lyoung|\n",
      "|6270 Jennifer Pra...|1977-06-01 20:40:04| Norway|     12436|lynncynthia@hotma...|    Rodney Giles|       swiggins|\n",
      "|415 Megan Parkway...|1971-08-29 06:21:22| Norway|     12438|  jeff42@hotmail.com| Thomas Figueroa|  matthewharris|\n",
      "|PSC 4266, Box 099...|1971-09-03 05:42:49| Norway|     12444| richard20@gmail.com|     Meghan Wood|   salazarbilly|\n",
      "|1333 Michael Vill...|1995-03-09 03:25:02| Norway|     12752|seanrobles@gmail.com|Lauren Hernandez|    morrowhenry|\n",
      "+--------------------+-------------------+-------+----------+--------------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.filter(\"Country = 'Norway'\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb032124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:53 INFO FileSourceStrategy: Pushed Filters: IsNotNull(Country),Not(EqualTo(Country,Iceland))\n",
      "24/09/06 14:50:53 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(Country#178),NOT (Country#178 = Iceland)\n",
      "24/09/06 14:50:53 INFO CodeGenerator: Code generated in 8.789156 ms\n",
      "24/09/06 14:50:53 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:53 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:53 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:53 INFO SparkContext: Created broadcast 32 from show at cell35.sc:1\n",
      "24/09/06 14:50:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:53 INFO SparkContext: Starting job: show at cell35.sc:1\n",
      "24/09/06 14:50:53 INFO DAGScheduler: Got job 22 (show at cell35.sc:1) with 1 output partitions\n",
      "24/09/06 14:50:53 INFO DAGScheduler: Final stage: ResultStage 22 (show at cell35.sc:1)\n",
      "24/09/06 14:50:53 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:53 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:53 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[56] at show at cell35.sc:1), which has no missing parents\n",
      "24/09/06 14:50:53 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 17.8 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:53 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:53 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on ubuntu:37607 (size: 8.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:53 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[56] at show at cell35.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:53 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:53 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 42) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/06 14:50:53 INFO Executor: Running task 0.0 in stage 22.0 (TID 42)\n",
      "24/09/06 14:50:53 INFO CodeGenerator: Code generated in 8.457202 ms\n",
      "24/09/06 14:50:53 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:53 INFO CodeGenerator: Code generated in 3.882301 ms\n",
      "24/09/06 14:50:53 INFO Executor: Finished task 0.0 in stage 22.0 (TID 42). 4489 bytes result sent to driver\n",
      "24/09/06 14:50:53 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 42) in 29 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:53 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:53 INFO DAGScheduler: ResultStage 22 (show at cell35.sc:1) finished in 0,036 s\n",
      "24/09/06 14:50:53 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished\n",
      "24/09/06 14:50:53 INFO DAGScheduler: Job 22 finished: show at cell35.sc:1, took 0,039475 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+-------------------+--------------+----------+--------------------------+-----------------+----------------+\n",
      "|Address                                              |Birthdate          |Country       |CustomerID|Email                     |Name             |Username        |\n",
      "+-----------------------------------------------------+-------------------+--------------+----------+--------------------------+-----------------+----------------+\n",
      "|Unit 1047 Box 4089\\nDPO AA 57348                     |1994-02-20 00:46:27|United Kingdom|12346     |cooperalexis@hotmail.com  |Lindsay Cowan    |valenciajennifer|\n",
      "|Unit 2676 Box 9352\\nDPO AA 38560                     |1974-11-26 15:30:20|Finland       |12348     |tcrawford@gmail.com       |Leslie Martinez  |serranobrian    |\n",
      "|2765 Powers Meadow\\nHeatherfurt, CT 53165            |1977-05-06 23:57:35|Italy         |12349     |dustin37@yahoo.com        |Brad Cardenas    |charleshudson   |\n",
      "|17677 Mark Crest\\nWalterberg, IA 39017               |1996-09-13 19:14:27|Norway        |12350     |amyholland@yahoo.com      |Natalie Ford     |gregoryharrison |\n",
      "|50047 Smith Point Suite 162\\nWilkinsstad, PA 04106   |1969-06-21 03:39:20|Norway        |12352     |vcarter@hotmail.com       |Dana Clarke      |hmyers          |\n",
      "|633 Miller Turnpike\\nJonathanland, OR 62874          |1993-02-25 18:37:29|Bahrain       |12353     |laura34@yahoo.com         |Gary Nichols     |andrewhamilton  |\n",
      "|38456 Rachael Causeway Apt. 735\\nEvanfort, AR 33893  |1993-03-13 12:37:34|Spain         |12354     |zmelton@gmail.com         |John Parks       |matthewray      |\n",
      "|4140 Pamela Hollow Apt. 849\\nEast Elizabeth, TN 29566|1972-11-10 12:01:08|Bahrain       |12355     |scott50@yahoo.com         |Jennifer Lawrence|glopez          |\n",
      "|8681 Karen Roads Apt. 096\\nLowehaven, IA 19798       |1973-01-13 17:17:26|Portugal      |12356     |josephmacias@hotmail.com  |James Sanchez    |wesley20        |\n",
      "|18637 Jessica Ridge Apt. 157\\nGrossberg, ME 84127    |1989-11-24 17:12:54|Switzerland   |12357     |michael16@hotmail.com     |Ashley Lopez     |thomasdavid     |\n",
      "|2129 Joel Rapids\\nLisahaven, NE 08609                |1977-06-19 22:35:52|Austria       |12358     |michaelespinoza@gmail.com |Dr. Angela Brown |patricia44      |\n",
      "|86636 Maria Viaduct\\nKennethhaven, SD 21876          |1983-09-21 05:22:18|Cyprus        |12359     |ryanpena@yahoo.com        |John Vega        |nelsonmaria     |\n",
      "|1579 Young Trail\\nJessechester, OH 88328             |1980-10-28 17:25:59|Austria       |12360     |briannafrost@yahoo.com    |Lauren Clark     |portermichael   |\n",
      "|USNS Howard\\nFPO AP 30863                            |1982-09-01 09:12:57|Belgium       |12361     |virginia36@hotmail.com    |Jacqueline Haynes|johnsonshelly   |\n",
      "|70092 Adams Prairie\\nTurnerborough, TX 38603         |1979-02-03 03:42:47|Belgium       |12362     |april04@gmail.com         |Brian Flores     |hunterdaniel    |\n",
      "|7322 Owens Inlet Apt. 688\\nPort Leslie, OR 81893     |1974-12-21 13:27:20|Unspecified   |12363     |omolina@gmail.com         |Christopher Gomez|james75         |\n",
      "|86176 Katherine Common\\nWebbhaven, WA 51980          |1990-07-17 15:47:12|Belgium       |12364     |barbaraduncan@gmail.com   |Robert Burns     |eric10          |\n",
      "|932 Jeremy Springs Suite 144\\nJohnmouth, NM 02561    |1981-07-10 00:35:00|Cyprus        |12365     |nicoleanderson@hotmail.com|Joshua Parker    |millerrenee     |\n",
      "|USNV Chavez\\nFPO AP 78727                            |1989-12-26 00:58:01|Denmark       |12367     |aaron99@yahoo.com         |Christine Douglas|michael58       |\n",
      "|565 Hodge Motorway Suite 101\\nWendyberg, FL 57099    |1973-12-21 03:33:47|Cyprus        |12370     |qgibson@hotmail.com       |Derek Curtis     |zsanders        |\n",
      "+-----------------------------------------------------+-------------------+--------------+----------+--------------------------+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.where($\"Country\" =!= \"Iceland\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5044e9ab",
   "metadata": {},
   "source": [
    "#### Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5c70ef1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:53 INFO BlockManagerInfo: Removed broadcast_28_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:53 INFO BlockManagerInfo: Removed broadcast_24_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:53 INFO BlockManagerInfo: Removed broadcast_30_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:53 INFO BlockManagerInfo: Removed broadcast_22_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:53 INFO BlockManagerInfo: Removed broadcast_31_piece0 on ubuntu:37607 in memory (size: 7.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:53 INFO BlockManagerInfo: Removed broadcast_26_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:53 INFO BlockManagerInfo: Removed broadcast_29_piece0 on ubuntu:37607 in memory (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:53 INFO BlockManagerInfo: Removed broadcast_27_piece0 on ubuntu:37607 in memory (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:53 INFO BlockManagerInfo: Removed broadcast_25_piece0 on ubuntu:37607 in memory (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:53 INFO BlockManagerInfo: Removed broadcast_33_piece0 on ubuntu:37607 in memory (size: 8.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:53 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:53 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:53 INFO CodeGenerator: Code generated in 7.942675 ms\n",
      "24/09/06 14:50:53 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:53 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:53 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:53 INFO SparkContext: Created broadcast 34 from show at cell36.sc:1\n",
      "24/09/06 14:50:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:53 INFO SparkContext: Starting job: show at cell36.sc:1\n",
      "24/09/06 14:50:53 INFO DAGScheduler: Got job 23 (show at cell36.sc:1) with 1 output partitions\n",
      "24/09/06 14:50:53 INFO DAGScheduler: Final stage: ResultStage 23 (show at cell36.sc:1)\n",
      "24/09/06 14:50:53 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:53 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:53 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[60] at show at cell36.sc:1), which has no missing parents\n",
      "24/09/06 14:50:53 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 14.3 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:53 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:53 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on ubuntu:37607 (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:53 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[60] at show at cell36.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:53 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:53 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 43) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/06 14:50:53 INFO Executor: Running task 0.0 in stage 23.0 (TID 43)\n",
      "24/09/06 14:50:53 INFO CodeGenerator: Code generated in 6.09621 ms\n",
      "24/09/06 14:50:53 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:53 INFO Executor: Finished task 0.0 in stage 23.0 (TID 43). 6279 bytes result sent to driver\n",
      "24/09/06 14:50:53 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 43) in 43 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:53 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:53 INFO DAGScheduler: ResultStage 23 (show at cell36.sc:1) finished in 0,049 s\n",
      "24/09/06 14:50:53 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished\n",
      "24/09/06 14:50:53 INFO DAGScheduler: Job 23 finished: show at cell36.sc:1, took 0,053878 s\n",
      "24/09/06 14:50:53 INFO CodeGenerator: Code generated in 6.185839 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------+----------+--------------------+--------------------+-----------------+\n",
      "|             Address|          Birthdate|       Country|CustomerID|               Email|                Name|         Username|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+--------------------+-----------------+\n",
      "|6942 Connie Skywa...|1973-10-24 00:52:10|United Kingdom|     12989| amber97@hotmail.com|   Brandon Contreras|           ecasey|\n",
      "|79375 David Neck\\...|1971-05-04 22:20:10|United Kingdom|     12988|   erica98@gmail.com|      Gabriel Romero|          qknight|\n",
      "|00881 West Flat\\n...|1997-03-05 19:20:57|United Kingdom|     12987|    vkeith@yahoo.com|Christopher Lawrence|        smcintyre|\n",
      "|499 Jonathan Stre...|1987-10-24 20:05:15|United Kingdom|     12985| fredsmith@yahoo.com|        Xavier Myers|stricklandjeffery|\n",
      "|9505 Melissa Stre...|1975-09-22 15:21:58|United Kingdom|     12984|scottjonathan@yah...|        Brandy Huang|   amandawilliams|\n",
      "|399 Fuentes Roads...|1986-09-30 18:12:45|United Kingdom|     12982|cynthia31@hotmail...|      Linda Stephens|     davidsonomar|\n",
      "|1573 Jessica Glen...|1984-07-23 03:09:18|United Kingdom|     12981|  esharp@hotmail.com|          Dawn Woods|         steven67|\n",
      "|153 Tara Ridges S...|1974-03-03 07:52:15|United Kingdom|     12980| jessica87@gmail.com|         Sean Brooks|        kristen26|\n",
      "|62134 Chen Valley...|1990-10-09 01:29:02|United Kingdom|     12977| fmatthews@gmail.com|          Kyle Simon|          emily28|\n",
      "|7521 Christopher ...|1973-10-10 23:57:51|United Kingdom|     12976|williamsheidi@yah...|         Hannah Rose|         eugene04|\n",
      "|00679 Lucero Moun...|1987-10-13 12:41:52|United Kingdom|     12974|thomasreyes@yahoo...|     Daniel Fletcher|  velazquezangela|\n",
      "|885 Zamora Hills\\...|1986-11-14 14:18:47|United Kingdom|     12971|   cwilcox@yahoo.com|       Caitlin Walls|         ashley11|\n",
      "|4539 Powers Orcha...|1990-09-11 06:01:18|United Kingdom|     12970|edwardspeter@yaho...|      Jonathan Hines|          mmiller|\n",
      "|335 Lewis Land\\nL...|1994-04-25 16:59:48|United Kingdom|     12968| mmurray@hotmail.com|         Susan Davis|   jacksoncolleen|\n",
      "|Unit 3978 Box 615...|1969-05-28 22:09:26|United Kingdom|     12967|thomasjames@gmail...|    Amber Williamson|    justinjohnson|\n",
      "|PSC 6600, Box 447...|1975-10-14 17:46:59|United Kingdom|     12966|colinward@hotmail...|        Amy Robinson|     sarathompson|\n",
      "|0240 Ernest Under...|1968-06-14 23:10:47|United Kingdom|     12965|jeremy10@hotmail.com|      Raymond Patton|      meganbrewer|\n",
      "|2433 Amy Shoals\\n...|1975-11-05 04:34:04|United Kingdom|     12963|  tjohnson@yahoo.com|        Sheila Parks|      dominique55|\n",
      "|833 Wilson Street...|1978-12-25 10:12:45|United Kingdom|     12962|kathyphillips@yah...|        Cheryl Burns|       diazsharon|\n",
      "|809 Robert Plain ...|1978-07-26 19:48:26|United Kingdom|     12957|  steven78@gmail.com|     Reginald Wright|         nathan71|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.sort(col(\"CustomerID\").desc).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99396b99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:53 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:53 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:53 INFO CodeGenerator: Code generated in 4.968198 ms\n",
      "24/09/06 14:50:54 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:54 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:54 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:54 INFO SparkContext: Created broadcast 36 from show at cell37.sc:1\n",
      "24/09/06 14:50:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:54 INFO SparkContext: Starting job: show at cell37.sc:1\n",
      "24/09/06 14:50:54 INFO DAGScheduler: Got job 24 (show at cell37.sc:1) with 1 output partitions\n",
      "24/09/06 14:50:54 INFO DAGScheduler: Final stage: ResultStage 24 (show at cell37.sc:1)\n",
      "24/09/06 14:50:54 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:54 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:54 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[64] at show at cell37.sc:1), which has no missing parents\n",
      "24/09/06 14:50:54 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 14.3 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:54 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:54 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on ubuntu:37607 (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:54 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[64] at show at cell37.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:54 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:54 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 44) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/06 14:50:54 INFO Executor: Running task 0.0 in stage 24.0 (TID 44)\n",
      "24/09/06 14:50:54 INFO CodeGenerator: Code generated in 4.016582 ms\n",
      "24/09/06 14:50:54 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:54 INFO Executor: Finished task 0.0 in stage 24.0 (TID 44). 6119 bytes result sent to driver\n",
      "24/09/06 14:50:54 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 44) in 26 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:54 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:54 INFO DAGScheduler: ResultStage 24 (show at cell37.sc:1) finished in 0,031 s\n",
      "24/09/06 14:50:54 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:54 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished\n",
      "24/09/06 14:50:54 INFO DAGScheduler: Job 24 finished: show at cell37.sc:1, took 0,034608 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "|             Address|          Birthdate|       Country|CustomerID|               Email|             Name|        Username|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "|Unit 1047 Box 408...|1994-02-20 00:46:27|United Kingdom|     12346|cooperalexis@hotm...|    Lindsay Cowan|valenciajennifer|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|  Katherine David|      hillrachel|\n",
      "|Unit 2676 Box 935...|1974-11-26 15:30:20|       Finland|     12348| tcrawford@gmail.com|  Leslie Martinez|    serranobrian|\n",
      "|2765 Powers Meado...|1977-05-06 23:57:35|         Italy|     12349|  dustin37@yahoo.com|    Brad Cardenas|   charleshudson|\n",
      "|17677 Mark Crest\\...|1996-09-13 19:14:27|        Norway|     12350|amyholland@yahoo.com|     Natalie Ford| gregoryharrison|\n",
      "|50047 Smith Point...|1969-06-21 03:39:20|        Norway|     12352| vcarter@hotmail.com|      Dana Clarke|          hmyers|\n",
      "|633 Miller Turnpi...|1993-02-25 18:37:29|       Bahrain|     12353|   laura34@yahoo.com|     Gary Nichols|  andrewhamilton|\n",
      "|38456 Rachael Cau...|1993-03-13 12:37:34|         Spain|     12354|   zmelton@gmail.com|       John Parks|      matthewray|\n",
      "|4140 Pamela Hollo...|1972-11-10 12:01:08|       Bahrain|     12355|   scott50@yahoo.com|Jennifer Lawrence|          glopez|\n",
      "|8681 Karen Roads ...|1973-01-13 17:17:26|      Portugal|     12356|josephmacias@hotm...|    James Sanchez|        wesley20|\n",
      "|18637 Jessica Rid...|1989-11-24 17:12:54|   Switzerland|     12357|michael16@hotmail...|     Ashley Lopez|     thomasdavid|\n",
      "|2129 Joel Rapids\\...|1977-06-19 22:35:52|       Austria|     12358|michaelespinoza@g...| Dr. Angela Brown|      patricia44|\n",
      "|86636 Maria Viadu...|1983-09-21 05:22:18|        Cyprus|     12359|  ryanpena@yahoo.com|        John Vega|     nelsonmaria|\n",
      "|1579 Young Trail\\...|1980-10-28 17:25:59|       Austria|     12360|briannafrost@yaho...|     Lauren Clark|   portermichael|\n",
      "|USNS Howard\\nFPO ...|1982-09-01 09:12:57|       Belgium|     12361|virginia36@hotmai...|Jacqueline Haynes|   johnsonshelly|\n",
      "|70092 Adams Prair...|1979-02-03 03:42:47|       Belgium|     12362|   april04@gmail.com|     Brian Flores|    hunterdaniel|\n",
      "|7322 Owens Inlet ...|1974-12-21 13:27:20|   Unspecified|     12363|   omolina@gmail.com|Christopher Gomez|         james75|\n",
      "|86176 Katherine C...|1990-07-17 15:47:12|       Belgium|     12364|barbaraduncan@gma...|     Robert Burns|          eric10|\n",
      "|932 Jeremy Spring...|1981-07-10 00:35:00|        Cyprus|     12365|nicoleanderson@ho...|    Joshua Parker|     millerrenee|\n",
      "|USNV Chavez\\nFPO ...|1989-12-26 00:58:01|       Denmark|     12367|   aaron99@yahoo.com|Christine Douglas|       michael58|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.orderBy(\"CustomerID\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdc9ec6",
   "metadata": {},
   "source": [
    "#### Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1910596b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:54 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:54 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:54 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:54 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:54 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:54 INFO SparkContext: Created broadcast 38 from rdd at cell38.sc:2\n",
      "24/09/06 14:50:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old num partitions: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:54 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:54 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:54 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:54 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:54 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:54 INFO SparkContext: Created broadcast 39 from rdd at cell38.sc:3\n",
      "24/09/06 14:50:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:54 INFO DAGScheduler: Registering RDD 72 (rdd at cell38.sc:3) as input to shuffle 0\n",
      "24/09/06 14:50:54 INFO DAGScheduler: Got map stage job 25 (rdd at cell38.sc:3) with 1 output partitions\n",
      "24/09/06 14:50:54 INFO DAGScheduler: Final stage: ShuffleMapStage 25 (rdd at cell38.sc:3)\n",
      "24/09/06 14:50:54 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:54 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:54 INFO DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[72] at rdd at cell38.sc:3), which has no missing parents\n",
      "24/09/06 14:50:54 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 14.5 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:54 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:54 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on ubuntu:37607 (size: 7.3 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:54 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:54 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[72] at rdd at cell38.sc:3) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:54 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:54 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 45) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8235 bytes) \n",
      "24/09/06 14:50:54 INFO Executor: Running task 0.0 in stage 25.0 (TID 45)\n",
      "24/09/06 14:50:54 INFO CodeGenerator: Code generated in 9.238078 ms\n",
      "24/09/06 14:50:54 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:54 INFO Executor: Finished task 0.0 in stage 25.0 (TID 45). 1756 bytes result sent to driver\n",
      "24/09/06 14:50:54 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 45) in 75 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:54 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:54 INFO DAGScheduler: ShuffleMapStage 25 (rdd at cell38.sc:3) finished in 0,092 s\n",
      "24/09/06 14:50:54 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/06 14:50:54 INFO DAGScheduler: running: Set()\n",
      "24/09/06 14:50:54 INFO DAGScheduler: waiting: Set()\n",
      "24/09/06 14:50:54 INFO DAGScheduler: failed: Set()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New num partitions: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrepartitionedDf\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [Address: string, Birthdate: string ... 5 more fields]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val repartitionedDf = customerDf.repartition(5, col(\"Country\"))\n",
    "println(s\"Old num partitions: ${customerDf.rdd.getNumPartitions}\")\n",
    "println(s\"New num partitions: ${repartitionedDf.rdd.getNumPartitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "44b583d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:54 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:54 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:54 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:54 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:54 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:54 INFO SparkContext: Created broadcast 41 from rdd at cell39.sc:1\n",
      "24/09/06 14:50:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:54 INFO DAGScheduler: Registering RDD 79 (rdd at cell39.sc:1) as input to shuffle 1\n",
      "24/09/06 14:50:54 INFO DAGScheduler: Got map stage job 26 (rdd at cell39.sc:1) with 1 output partitions\n",
      "24/09/06 14:50:54 INFO DAGScheduler: Final stage: ShuffleMapStage 26 (rdd at cell39.sc:1)\n",
      "24/09/06 14:50:54 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:54 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:54 INFO DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[79] at rdd at cell39.sc:1), which has no missing parents\n",
      "24/09/06 14:50:54 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 14.5 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:54 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:54 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on ubuntu:37607 (size: 7.3 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:54 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:54 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[79] at rdd at cell39.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:54 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:54 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 46) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8235 bytes) \n",
      "24/09/06 14:50:54 INFO Executor: Running task 0.0 in stage 26.0 (TID 46)\n",
      "24/09/06 14:50:54 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:54 INFO Executor: Finished task 0.0 in stage 26.0 (TID 46). 1713 bytes result sent to driver\n",
      "24/09/06 14:50:54 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 46) in 26 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:54 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:54 INFO DAGScheduler: ShuffleMapStage 26 (rdd at cell39.sc:1) finished in 0,033 s\n",
      "24/09/06 14:50:54 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/06 14:50:54 INFO DAGScheduler: running: Set()\n",
      "24/09/06 14:50:54 INFO DAGScheduler: waiting: Set()\n",
      "24/09/06 14:50:54 INFO DAGScheduler: failed: Set()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num partitions after coalesce: 1\n"
     ]
    }
   ],
   "source": [
    "println(s\"Num partitions after coalesce: ${repartitionedDf.coalesce(1).rdd.getNumPartitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97975bbd",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e35c3a43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:55 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:55 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:55 INFO CodeGenerator: Code generated in 14.498205 ms\n",
      "24/09/06 14:50:55 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:55 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:55 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:55 INFO SparkContext: Created broadcast 43 from show at cell40.sc:2\n",
      "24/09/06 14:50:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:55 INFO SparkContext: Starting job: show at cell40.sc:2\n",
      "24/09/06 14:50:55 INFO DAGScheduler: Got job 27 (show at cell40.sc:2) with 1 output partitions\n",
      "24/09/06 14:50:55 INFO DAGScheduler: Final stage: ResultStage 27 (show at cell40.sc:2)\n",
      "24/09/06 14:50:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:55 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:55 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[88] at show at cell40.sc:2), which has no missing parents\n",
      "24/09/06 14:50:55 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 16.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:55 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:55 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on ubuntu:37607 (size: 7.7 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:55 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[88] at show at cell40.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:55 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:55 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 47) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/06 14:50:55 INFO Executor: Running task 0.0 in stage 27.0 (TID 47)\n",
      "24/09/06 14:50:55 INFO CodeGenerator: Code generated in 9.583652 ms\n",
      "24/09/06 14:50:55 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:55 INFO Executor: Finished task 0.0 in stage 27.0 (TID 47). 1705 bytes result sent to driver\n",
      "24/09/06 14:50:55 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 47) in 26 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:55 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:55 INFO DAGScheduler: ResultStage 27 (show at cell40.sc:2) finished in 0,035 s\n",
      "24/09/06 14:50:55 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished\n",
      "24/09/06 14:50:55 INFO DAGScheduler: Job 27 finished: show at cell40.sc:2, took 0,038708 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|          Birthdate|        bd|\n",
      "+-------------------+----------+\n",
      "|1994-02-20 00:46:27|1994-02-20|\n",
      "|1988-06-21 00:15:34|1988-06-21|\n",
      "|1974-11-26 15:30:20|1974-11-26|\n",
      "|1977-05-06 23:57:35|1977-05-06|\n",
      "|1996-09-13 19:14:27|1996-09-13|\n",
      "+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdatDf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Birthdate: string, bd: string]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datDf = customerDf.select($\"Birthdate\", date_format(col(\"Birthdate\"), \"yyyy-MM-dd\").alias(\"bd\"))\n",
    "datDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74467040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Birthdate: string (nullable = true)\n",
      " |-- bd: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datDf.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1d84b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:55 INFO BlockManagerInfo: Removed broadcast_40_piece0 on ubuntu:37607 in memory (size: 7.3 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:55 INFO BlockManagerInfo: Removed broadcast_37_piece0 on ubuntu:37607 in memory (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:55 INFO BlockManagerInfo: Removed broadcast_34_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:55 INFO BlockManagerInfo: Removed broadcast_35_piece0 on ubuntu:37607 in memory (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:55 INFO BlockManagerInfo: Removed broadcast_42_piece0 on ubuntu:37607 in memory (size: 7.3 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:55 INFO BlockManagerInfo: Removed broadcast_44_piece0 on ubuntu:37607 in memory (size: 7.7 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:55 INFO BlockManagerInfo: Removed broadcast_36_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:55 INFO BlockManagerInfo: Removed broadcast_32_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:55 INFO BlockManagerInfo: Removed broadcast_41_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Birthdate: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Username: string (nullable = true)\n",
      " |-- Identity: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.withColumn(\"Identity\", array($\"Name\", $\"Username\")).printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d4294e1-47ff-4300-8514-b2930b8dd9ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:55 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:55 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:56 INFO CodeGenerator: Code generated in 7.96632 ms\n",
      "24/09/06 14:50:56 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:56 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:56 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:56 INFO SparkContext: Created broadcast 45 from show at cell43.sc:4\n",
      "24/09/06 14:50:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:56 INFO SparkContext: Starting job: show at cell43.sc:4\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Got job 28 (show at cell43.sc:4) with 1 output partitions\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Final stage: ResultStage 28 (show at cell43.sc:4)\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[92] at show at cell43.sc:4), which has no missing parents\n",
      "24/09/06 14:50:56 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 16.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:56 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:56 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on ubuntu:37607 (size: 7.7 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:56 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[92] at show at cell43.sc:4) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:56 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:56 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 48) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/06 14:50:56 INFO Executor: Running task 0.0 in stage 28.0 (TID 48)\n",
      "24/09/06 14:50:56 INFO CodeGenerator: Code generated in 6.503664 ms\n",
      "24/09/06 14:50:56 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:56 INFO Executor: Finished task 0.0 in stage 28.0 (TID 48). 1875 bytes result sent to driver\n",
      "24/09/06 14:50:56 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 48) in 18 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:56 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:56 INFO DAGScheduler: ResultStage 28 (show at cell43.sc:4) finished in 0,024 s\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Job 28 finished: show at cell43.sc:4, took 0,026234 s\n",
      "24/09/06 14:50:56 INFO CodeGenerator: Code generated in 5.95878 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+---------------------------------+\n",
      "|Name           |Username        |Identity                         |\n",
      "+---------------+----------------+---------------------------------+\n",
      "|Lindsay Cowan  |valenciajennifer|[Lindsay Cowan, valenciajennifer]|\n",
      "|Katherine David|hillrachel      |[Katherine David, hillrachel]    |\n",
      "|Leslie Martinez|serranobrian    |[Leslie Martinez, serranobrian]  |\n",
      "|Brad Cardenas  |charleshudson   |[Brad Cardenas, charleshudson]   |\n",
      "|Natalie Ford   |gregoryharrison |[Natalie Ford, gregoryharrison]  |\n",
      "+---------------+----------------+---------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf\n",
    "    .withColumn(\"Identity\", array($\"Name\", $\"Username\"))\n",
    "    .select(\"Name\", \"Username\", \"Identity\")\n",
    "    .show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b17c1f-3197-43bd-b043-a96f256bff3c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3 Агрегирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "97bfc928-6e23-4c15-b3d3-e9b314fc23e2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:56 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:56 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:56 INFO CodeGenerator: Code generated in 35.974706 ms\n",
      "24/09/06 14:50:56 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:56 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:56 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:56 INFO SparkContext: Created broadcast 47 from show at cell44.sc:1\n",
      "24/09/06 14:50:56 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Registering RDD 96 (show at cell44.sc:1) as input to shuffle 2\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Got map stage job 29 (show at cell44.sc:1) with 1 output partitions\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Final stage: ShuffleMapStage 29 (show at cell44.sc:1)\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[96] at show at cell44.sc:1), which has no missing parents\n",
      "24/09/06 14:50:56 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 38.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:56 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:56 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on ubuntu:37607 (size: 17.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:56 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[96] at show at cell44.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:56 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:56 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 49) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8235 bytes) \n",
      "24/09/06 14:50:56 INFO Executor: Running task 0.0 in stage 29.0 (TID 49)\n",
      "24/09/06 14:50:56 INFO CodeGenerator: Code generated in 25.931627 ms\n",
      "24/09/06 14:50:56 INFO CodeGenerator: Code generated in 8.225783 ms\n",
      "24/09/06 14:50:56 INFO CodeGenerator: Code generated in 4.425685 ms\n",
      "24/09/06 14:50:56 INFO CodeGenerator: Code generated in 3.293819 ms\n",
      "24/09/06 14:50:56 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:56 INFO Executor: Finished task 0.0 in stage 29.0 (TID 49). 2716 bytes result sent to driver\n",
      "24/09/06 14:50:56 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 49) in 87 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:56 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:56 INFO DAGScheduler: ShuffleMapStage 29 (show at cell44.sc:1) finished in 0,095 s\n",
      "24/09/06 14:50:56 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/06 14:50:56 INFO DAGScheduler: running: Set()\n",
      "24/09/06 14:50:56 INFO DAGScheduler: waiting: Set()\n",
      "24/09/06 14:50:56 INFO DAGScheduler: failed: Set()\n",
      "24/09/06 14:50:56 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/09/06 14:50:56 INFO CodeGenerator: Code generated in 4.759009 ms\n",
      "24/09/06 14:50:56 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/09/06 14:50:56 INFO CodeGenerator: Code generated in 9.883414 ms\n",
      "24/09/06 14:50:56 INFO SparkContext: Starting job: show at cell44.sc:1\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Got job 30 (show at cell44.sc:1) with 1 output partitions\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Final stage: ResultStage 31 (show at cell44.sc:1)\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[100] at show at cell44.sc:1), which has no missing parents\n",
      "24/09/06 14:50:56 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 40.9 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:56 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:56 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on ubuntu:37607 (size: 19.4 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:56 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[100] at show at cell44.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:56 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:56 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 50) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/06 14:50:56 INFO Executor: Running task 0.0 in stage 31.0 (TID 50)\n",
      "24/09/06 14:50:56 INFO CodeGenerator: Code generated in 4.638365 ms\n",
      "24/09/06 14:50:56 INFO ShuffleBlockFetcherIterator: Getting 1 (2.2 KiB) non-empty blocks including 1 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/06 14:50:56 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms\n",
      "24/09/06 14:50:56 INFO CodeGenerator: Code generated in 8.983588 ms\n",
      "24/09/06 14:50:56 INFO Executor: Finished task 0.0 in stage 31.0 (TID 50). 6167 bytes result sent to driver\n",
      "24/09/06 14:50:56 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 50) in 73 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:56 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:56 INFO DAGScheduler: ResultStage 31 (show at cell44.sc:1) finished in 0,083 s\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 31: Stage finished\n",
      "24/09/06 14:50:56 INFO DAGScheduler: Job 30 finished: show at cell44.sc:1, took 0,088983 s\n",
      "24/09/06 14:50:56 INFO CodeGenerator: Code generated in 7.844408 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             Country|count|\n",
      "+--------------------+-----+\n",
      "|           Singapore|    1|\n",
      "|                 RSA|    1|\n",
      "|             Iceland|    1|\n",
      "|        Saudi Arabia|    1|\n",
      "|United Arab Emirates|    1|\n",
      "|      Czech Republic|    1|\n",
      "|              Brazil|    1|\n",
      "|             Lebanon|    1|\n",
      "|              Greece|    2|\n",
      "|         Unspecified|    2|\n",
      "|             Bahrain|    2|\n",
      "|              Israel|    4|\n",
      "|                 USA|    4|\n",
      "|              Poland|    6|\n",
      "|              Sweden|    7|\n",
      "|              Cyprus|    7|\n",
      "|             Denmark|    8|\n",
      "|               Japan|    8|\n",
      "|           Australia|    8|\n",
      "|         Netherlands|    8|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.groupBy(\"Country\").count().orderBy(\"count\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8e740a2d-6fcc-4a8c-b2bf-3854475701c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:57 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:57 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 13.765544 ms\n",
      "24/09/06 14:50:57 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO SparkContext: Created broadcast 50 from show at cell45.sc:4\n",
      "24/09/06 14:50:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Registering RDD 105 (show at cell45.sc:4) as input to shuffle 3\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Got map stage job 31 (show at cell45.sc:4) with 1 output partitions\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Final stage: ShuffleMapStage 32 (show at cell45.sc:4)\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[105] at show at cell45.sc:4), which has no missing parents\n",
      "24/09/06 14:50:57 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 34.6 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on ubuntu:37607 (size: 16.4 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[105] at show at cell45.sc:4) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:57 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:57 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 51) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8235 bytes) \n",
      "24/09/06 14:50:57 INFO Executor: Running task 0.0 in stage 32.0 (TID 51)\n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 10.708029 ms\n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 4.953536 ms\n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 6.881807 ms\n",
      "24/09/06 14:50:57 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 5.042567 ms\n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 4.111104 ms\n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 5.461249 ms\n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 5.587529 ms\n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 3.696665 ms\n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 6.244076 ms\n",
      "24/09/06 14:50:57 INFO Executor: Finished task 0.0 in stage 32.0 (TID 51). 2523 bytes result sent to driver\n",
      "24/09/06 14:50:57 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 51) in 194 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:57 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:57 INFO DAGScheduler: ShuffleMapStage 32 (show at cell45.sc:4) finished in 0,211 s\n",
      "24/09/06 14:50:57 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/06 14:50:57 INFO DAGScheduler: running: Set()\n",
      "24/09/06 14:50:57 INFO DAGScheduler: waiting: Set()\n",
      "24/09/06 14:50:57 INFO DAGScheduler: failed: Set()\n",
      "24/09/06 14:50:57 INFO ShufflePartitionsUtil: For shuffle(3), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 5.490948 ms\n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 8.443744 ms\n",
      "24/09/06 14:50:57 INFO SparkContext: Starting job: show at cell45.sc:4\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Got job 32 (show at cell45.sc:4) with 1 output partitions\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Final stage: ResultStage 34 (show at cell45.sc:4)\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 33)\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[110] at show at cell45.sc:4), which has no missing parents\n",
      "24/09/06 14:50:57 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 43.6 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 20.1 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on ubuntu:37607 (size: 20.1 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 34 (MapPartitionsRDD[110] at show at cell45.sc:4) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:57 INFO TaskSchedulerImpl: Adding task set 34.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:57 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 52) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/06 14:50:57 INFO Executor: Running task 0.0 in stage 34.0 (TID 52)\n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 5.645023 ms\n",
      "24/09/06 14:50:57 INFO ShuffleBlockFetcherIterator: Getting 1 (36.1 KiB) non-empty blocks including 1 (36.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/06 14:50:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 8.077766 ms\n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 5.7026 ms\n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 6.569132 ms\n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 4.685413 ms\n",
      "24/09/06 14:50:57 INFO Executor: Finished task 0.0 in stage 34.0 (TID 52). 7995 bytes result sent to driver\n",
      "24/09/06 14:50:57 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 52) in 67 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:57 INFO DAGScheduler: ResultStage 34 (show at cell45.sc:4) finished in 0,077 s\n",
      "24/09/06 14:50:57 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:57 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Job 32 finished: show at cell45.sc:4, took 0,085884 s\n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 6.148904 ms\n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 7.244235 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------------+---------------+\n",
      "|  Country|        bd|min(CustomerID)|max(CustomerID)|\n",
      "+---------+----------+---------------+---------------+\n",
      "|Australia|1966-09-17|          12422|          12422|\n",
      "|Australia|1967-11-29|          12424|          12424|\n",
      "|Australia|1985-12-30|          12386|          12386|\n",
      "|Australia|1986-08-28|          12434|          12434|\n",
      "|Australia|1987-02-26|          12393|          12393|\n",
      "|Australia|1989-08-28|          12415|          12415|\n",
      "|Australia|1993-01-31|          12431|          12431|\n",
      "|Australia|1993-04-02|          12388|          12388|\n",
      "|  Austria|1970-08-06|          12865|          12865|\n",
      "|  Austria|1973-07-16|          12818|          12818|\n",
      "|  Austria|1973-12-21|          12370|          12370|\n",
      "|  Austria|1976-06-13|          12374|          12374|\n",
      "|  Austria|1976-09-03|          12373|          12373|\n",
      "|  Austria|1977-06-19|          12358|          12358|\n",
      "|  Austria|1977-11-30|          12453|          12453|\n",
      "|  Austria|1980-10-28|          12360|          12360|\n",
      "|  Austria|1982-07-01|          12414|          12414|\n",
      "|  Austria|1983-02-24|          12429|          12429|\n",
      "|  Austria|1996-03-28|          12817|          12817|\n",
      "|  Bahrain|1972-11-10|          12355|          12355|\n",
      "+---------+----------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf\n",
    "    .groupBy($\"Country\", date_format(col(\"Birthdate\"), \"yyyy-MM-dd\").alias(\"bd\"))\n",
    "    .agg(min(\"CustomerID\"), max(\"CustomerID\"))\n",
    "    .orderBy(\"Country\")\n",
    "    .show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd0b5c-0c8e-4d5f-8b32-27c6ef1793b9",
   "metadata": {},
   "source": [
    "## 4 Операции над несколькими датафреймами"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9d6226",
   "metadata": {},
   "source": [
    "### Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "803b493e-9fdf-4e4a-b5ae-ed4f3d5f242c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:57 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:57 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 4.50848 ms\n",
      "24/09/06 14:50:57 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO SparkContext: Created broadcast 53 from count at cell46.sc:1\n",
      "24/09/06 14:50:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Registering RDD 114 (count at cell46.sc:1) as input to shuffle 4\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Got map stage job 33 (count at cell46.sc:1) with 1 output partitions\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Final stage: ShuffleMapStage 35 (count at cell46.sc:1)\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Submitting ShuffleMapStage 35 (MapPartitionsRDD[114] at count at cell46.sc:1), which has no missing parents\n",
      "24/09/06 14:50:57 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 17.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on ubuntu:37607 (size: 8.3 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[114] at count at cell46.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:57 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:57 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 53) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8235 bytes) \n",
      "24/09/06 14:50:57 INFO Executor: Running task 0.0 in stage 35.0 (TID 53)\n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 4.510659 ms\n",
      "24/09/06 14:50:57 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 2.822516 ms\n",
      "24/09/06 14:50:57 INFO Executor: Finished task 0.0 in stage 35.0 (TID 53). 1922 bytes result sent to driver\n",
      "24/09/06 14:50:57 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 53) in 21 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:57 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:57 INFO DAGScheduler: ShuffleMapStage 35 (count at cell46.sc:1) finished in 0,028 s\n",
      "24/09/06 14:50:57 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/06 14:50:57 INFO DAGScheduler: running: Set()\n",
      "24/09/06 14:50:57 INFO DAGScheduler: waiting: Set()\n",
      "24/09/06 14:50:57 INFO DAGScheduler: failed: Set()\n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 4.881447 ms\n",
      "24/09/06 14:50:57 INFO SparkContext: Starting job: count at cell46.sc:1\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Got job 34 (count at cell46.sc:1) with 1 output partitions\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Final stage: ResultStage 37 (count at cell46.sc:1)\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[117] at count at cell46.sc:1), which has no missing parents\n",
      "24/09/06 14:50:57 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 12.5 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on ubuntu:37607 (size: 5.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[117] at count at cell46.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:57 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:57 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 54) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/06 14:50:57 INFO Executor: Running task 0.0 in stage 37.0 (TID 54)\n",
      "24/09/06 14:50:57 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/06 14:50:57 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/09/06 14:50:57 INFO CodeGenerator: Code generated in 7.457107 ms\n",
      "24/09/06 14:50:57 INFO Executor: Finished task 0.0 in stage 37.0 (TID 54). 3995 bytes result sent to driver\n",
      "24/09/06 14:50:57 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 54) in 17 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:57 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:57 INFO DAGScheduler: ResultStage 37 (count at cell46.sc:1) finished in 0,028 s\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished\n",
      "24/09/06 14:50:57 INFO DAGScheduler: Job 34 finished: count at cell46.sc:1, took 0,030860 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres46\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m507L\u001b[39m"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerDf.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37cd41a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:57 INFO BlockManagerInfo: Removed broadcast_51_piece0 on ubuntu:37607 in memory (size: 16.4 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO BlockManagerInfo: Removed broadcast_43_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO BlockManagerInfo: Removed broadcast_46_piece0 on ubuntu:37607 in memory (size: 7.7 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO BlockManagerInfo: Removed broadcast_45_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO BlockManagerInfo: Removed broadcast_49_piece0 on ubuntu:37607 in memory (size: 19.4 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO BlockManagerInfo: Removed broadcast_54_piece0 on ubuntu:37607 in memory (size: 8.3 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO BlockManagerInfo: Removed broadcast_48_piece0 on ubuntu:37607 in memory (size: 17.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO BlockManagerInfo: Removed broadcast_47_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO BlockManagerInfo: Removed broadcast_50_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO BlockManagerInfo: Removed broadcast_55_piece0 on ubuntu:37607 in memory (size: 5.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:57 INFO BlockManagerInfo: Removed broadcast_52_piece0 on ubuntu:37607 in memory (size: 20.1 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:58 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:58 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:58 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO SparkContext: Created broadcast 56 from count at cell47.sc:1\n",
      "24/09/06 14:50:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO SparkContext: Created broadcast 57 from count at cell47.sc:1\n",
      "24/09/06 14:50:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Registering RDD 124 (count at cell47.sc:1) as input to shuffle 5\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Got map stage job 35 (count at cell47.sc:1) with 2 output partitions\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Final stage: ShuffleMapStage 38 (count at cell47.sc:1)\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[124] at count at cell47.sc:1), which has no missing parents\n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 18.1 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on ubuntu:37607 (size: 8.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[124] at count at cell47.sc:1) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/09/06 14:50:58 INFO TaskSchedulerImpl: Adding task set 38.0 with 2 tasks resource profile 0\n",
      "24/09/06 14:50:58 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 55) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8344 bytes) \n",
      "24/09/06 14:50:58 INFO TaskSetManager: Starting task 1.0 in stage 38.0 (TID 56) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 8344 bytes) \n",
      "24/09/06 14:50:58 INFO Executor: Running task 1.0 in stage 38.0 (TID 56)\n",
      "24/09/06 14:50:58 INFO Executor: Running task 0.0 in stage 38.0 (TID 55)\n",
      "24/09/06 14:50:58 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:58 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:58 INFO Executor: Finished task 1.0 in stage 38.0 (TID 56). 1978 bytes result sent to driver\n",
      "24/09/06 14:50:58 INFO TaskSetManager: Finished task 1.0 in stage 38.0 (TID 56) in 16 ms on ubuntu (executor driver) (1/2)\n",
      "24/09/06 14:50:58 INFO Executor: Finished task 0.0 in stage 38.0 (TID 55). 1978 bytes result sent to driver\n",
      "24/09/06 14:50:58 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 55) in 20 ms on ubuntu (executor driver) (2/2)\n",
      "24/09/06 14:50:58 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:58 INFO DAGScheduler: ShuffleMapStage 38 (count at cell47.sc:1) finished in 0,029 s\n",
      "24/09/06 14:50:58 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/06 14:50:58 INFO DAGScheduler: running: Set()\n",
      "24/09/06 14:50:58 INFO DAGScheduler: waiting: Set()\n",
      "24/09/06 14:50:58 INFO DAGScheduler: failed: Set()\n",
      "24/09/06 14:50:58 INFO SparkContext: Starting job: count at cell47.sc:1\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Got job 36 (count at cell47.sc:1) with 1 output partitions\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Final stage: ResultStage 40 (count at cell47.sc:1)\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[127] at count at cell47.sc:1), which has no missing parents\n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 12.5 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on ubuntu:37607 (size: 5.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[127] at count at cell47.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:58 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:58 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 57) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/06 14:50:58 INFO Executor: Running task 0.0 in stage 40.0 (TID 57)\n",
      "24/09/06 14:50:58 INFO ShuffleBlockFetcherIterator: Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/06 14:50:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "24/09/06 14:50:58 INFO Executor: Finished task 0.0 in stage 40.0 (TID 57). 3995 bytes result sent to driver\n",
      "24/09/06 14:50:58 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 57) in 8 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:58 INFO DAGScheduler: ResultStage 40 (count at cell47.sc:1) finished in 0,013 s\n",
      "24/09/06 14:50:58 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:58 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Job 36 finished: count at cell47.sc:1, took 0,016853 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres47\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m1014L\u001b[39m"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerDf.union(customerDf).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2ca4043-76ae-462d-b676-b1e4d2dfebb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:58 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:58 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:58 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:58 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:58 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:58 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO SparkContext: Created broadcast 60 from count at cell48.sc:1\n",
      "24/09/06 14:50:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO SparkContext: Created broadcast 61 from count at cell48.sc:1\n",
      "24/09/06 14:50:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO SparkContext: Created broadcast 62 from count at cell48.sc:1\n",
      "24/09/06 14:50:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Registering RDD 136 (count at cell48.sc:1) as input to shuffle 6\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Got map stage job 37 (count at cell48.sc:1) with 3 output partitions\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Final stage: ShuffleMapStage 41 (count at cell48.sc:1)\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Submitting ShuffleMapStage 41 (MapPartitionsRDD[136] at count at cell48.sc:1), which has no missing parents\n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 18.9 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on ubuntu:37607 (size: 9.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[136] at count at cell48.sc:1) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/09/06 14:50:58 INFO TaskSchedulerImpl: Adding task set 41.0 with 3 tasks resource profile 0\n",
      "24/09/06 14:50:58 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 58) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8344 bytes) \n",
      "24/09/06 14:50:58 INFO TaskSetManager: Starting task 1.0 in stage 41.0 (TID 59) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 8344 bytes) \n",
      "24/09/06 14:50:58 INFO TaskSetManager: Starting task 2.0 in stage 41.0 (TID 60) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 8344 bytes) \n",
      "24/09/06 14:50:58 INFO Executor: Running task 2.0 in stage 41.0 (TID 60)\n",
      "24/09/06 14:50:58 INFO Executor: Running task 0.0 in stage 41.0 (TID 58)\n",
      "24/09/06 14:50:58 INFO Executor: Running task 1.0 in stage 41.0 (TID 59)\n",
      "24/09/06 14:50:58 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:58 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:58 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:58 INFO Executor: Finished task 2.0 in stage 41.0 (TID 60). 2034 bytes result sent to driver\n",
      "24/09/06 14:50:58 INFO Executor: Finished task 0.0 in stage 41.0 (TID 58). 2034 bytes result sent to driver\n",
      "24/09/06 14:50:58 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 58) in 19 ms on ubuntu (executor driver) (1/3)\n",
      "24/09/06 14:50:58 INFO Executor: Finished task 1.0 in stage 41.0 (TID 59). 2034 bytes result sent to driver\n",
      "24/09/06 14:50:58 INFO TaskSetManager: Finished task 2.0 in stage 41.0 (TID 60) in 21 ms on ubuntu (executor driver) (2/3)\n",
      "24/09/06 14:50:58 INFO TaskSetManager: Finished task 1.0 in stage 41.0 (TID 59) in 22 ms on ubuntu (executor driver) (3/3)\n",
      "24/09/06 14:50:58 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:58 INFO DAGScheduler: ShuffleMapStage 41 (count at cell48.sc:1) finished in 0,030 s\n",
      "24/09/06 14:50:58 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/06 14:50:58 INFO DAGScheduler: running: Set()\n",
      "24/09/06 14:50:58 INFO DAGScheduler: waiting: Set()\n",
      "24/09/06 14:50:58 INFO DAGScheduler: failed: Set()\n",
      "24/09/06 14:50:58 INFO SparkContext: Starting job: count at cell48.sc:1\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Got job 38 (count at cell48.sc:1) with 1 output partitions\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Final stage: ResultStage 43 (count at cell48.sc:1)\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 42)\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Submitting ResultStage 43 (MapPartitionsRDD[139] at count at cell48.sc:1), which has no missing parents\n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 12.5 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on ubuntu:37607 (size: 6.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[139] at count at cell48.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:58 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:58 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 61) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/06 14:50:58 INFO Executor: Running task 0.0 in stage 43.0 (TID 61)\n",
      "24/09/06 14:50:58 INFO ShuffleBlockFetcherIterator: Getting 3 (180.0 B) non-empty blocks including 3 (180.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/06 14:50:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/09/06 14:50:58 INFO Executor: Finished task 0.0 in stage 43.0 (TID 61). 3995 bytes result sent to driver\n",
      "24/09/06 14:50:58 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 61) in 6 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:58 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:58 INFO DAGScheduler: ResultStage 43 (count at cell48.sc:1) finished in 0,012 s\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 43: Stage finished\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Job 38 finished: count at cell48.sc:1, took 0,014232 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres48\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m1521L\u001b[39m"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerDf.union(customerDf).union(customerDf).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c156a8b9-8a7f-4f1f-a5ed-a41c6155c4ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:58 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:58 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO SparkContext: Created broadcast 65 from count at cell49.sc:1\n",
      "24/09/06 14:50:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Registering RDD 143 (count at cell49.sc:1) as input to shuffle 7\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Got map stage job 39 (count at cell49.sc:1) with 1 output partitions\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Final stage: ShuffleMapStage 44 (count at cell49.sc:1)\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Submitting ShuffleMapStage 44 (MapPartitionsRDD[143] at count at cell49.sc:1), which has no missing parents\n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 17.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on ubuntu:37607 (size: 8.3 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 44 (MapPartitionsRDD[143] at count at cell49.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:58 INFO TaskSchedulerImpl: Adding task set 44.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:58 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 62) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8235 bytes) \n",
      "24/09/06 14:50:58 INFO Executor: Running task 0.0 in stage 44.0 (TID 62)\n",
      "24/09/06 14:50:58 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:58 INFO Executor: Finished task 0.0 in stage 44.0 (TID 62). 1922 bytes result sent to driver\n",
      "24/09/06 14:50:58 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 62) in 13 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:58 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:58 INFO DAGScheduler: ShuffleMapStage 44 (count at cell49.sc:1) finished in 0,023 s\n",
      "24/09/06 14:50:58 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/06 14:50:58 INFO DAGScheduler: running: Set()\n",
      "24/09/06 14:50:58 INFO DAGScheduler: waiting: Set()\n",
      "24/09/06 14:50:58 INFO DAGScheduler: failed: Set()\n",
      "24/09/06 14:50:58 INFO SparkContext: Starting job: count at cell49.sc:1\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Got job 40 (count at cell49.sc:1) with 1 output partitions\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Final stage: ResultStage 46 (count at cell49.sc:1)\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 45)\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Submitting ResultStage 46 (MapPartitionsRDD[146] at count at cell49.sc:1), which has no missing parents\n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 12.5 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on ubuntu:37607 (size: 5.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:58 INFO SparkContext: Created broadcast 67 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[146] at count at cell49.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:58 INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:58 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 63) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/06 14:50:58 INFO Executor: Running task 0.0 in stage 46.0 (TID 63)\n",
      "24/09/06 14:50:58 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/06 14:50:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/09/06 14:50:58 INFO Executor: Finished task 0.0 in stage 46.0 (TID 63). 3995 bytes result sent to driver\n",
      "24/09/06 14:50:58 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 63) in 6 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:58 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:58 INFO DAGScheduler: ResultStage 46 (count at cell49.sc:1) finished in 0,012 s\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 46: Stage finished\n",
      "24/09/06 14:50:58 INFO DAGScheduler: Job 40 finished: count at cell49.sc:1, took 0,016781 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres49\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m2535L\u001b[39m"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerDf.count * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee6cfc92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:59 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:59 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:59 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:59 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:59 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:59 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:59 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:59 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:59 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:59 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:59 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO SparkContext: Created broadcast 68 from count at cell50.sc:1\n",
      "24/09/06 14:50:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:59 INFO MemoryStore: Block broadcast_69 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO MemoryStore: Block broadcast_69_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Added broadcast_69_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO SparkContext: Created broadcast 69 from count at cell50.sc:1\n",
      "24/09/06 14:50:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:59 INFO MemoryStore: Block broadcast_70 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO MemoryStore: Block broadcast_70_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Added broadcast_70_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO SparkContext: Created broadcast 70 from count at cell50.sc:1\n",
      "24/09/06 14:50:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:59 INFO MemoryStore: Block broadcast_71 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO MemoryStore: Block broadcast_71_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Added broadcast_71_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO SparkContext: Created broadcast 71 from count at cell50.sc:1\n",
      "24/09/06 14:50:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:59 INFO MemoryStore: Block broadcast_72 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO MemoryStore: Block broadcast_72_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Added broadcast_72_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO SparkContext: Created broadcast 72 from count at cell50.sc:1\n",
      "24/09/06 14:50:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Registering RDD 159 (count at cell50.sc:1) as input to shuffle 8\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Got map stage job 41 (count at cell50.sc:1) with 5 output partitions\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Final stage: ShuffleMapStage 47 (count at cell50.sc:1)\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Submitting ShuffleMapStage 47 (MapPartitionsRDD[159] at count at cell50.sc:1), which has no missing parents\n",
      "24/09/06 14:50:59 INFO MemoryStore: Block broadcast_73 stored as values in memory (estimated size 20.3 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO MemoryStore: Block broadcast_73_piece0 stored as bytes in memory (estimated size 9.1 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Added broadcast_73_piece0 in memory on ubuntu:37607 (size: 9.1 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO SparkContext: Created broadcast 73 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 47 (MapPartitionsRDD[159] at count at cell50.sc:1) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/09/06 14:50:59 INFO TaskSchedulerImpl: Adding task set 47.0 with 5 tasks resource profile 0\n",
      "24/09/06 14:50:59 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 64) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8344 bytes) \n",
      "24/09/06 14:50:59 INFO TaskSetManager: Starting task 1.0 in stage 47.0 (TID 65) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 8344 bytes) \n",
      "24/09/06 14:50:59 INFO TaskSetManager: Starting task 2.0 in stage 47.0 (TID 66) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 8344 bytes) \n",
      "24/09/06 14:50:59 INFO TaskSetManager: Starting task 3.0 in stage 47.0 (TID 67) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 8344 bytes) \n",
      "24/09/06 14:50:59 INFO TaskSetManager: Starting task 4.0 in stage 47.0 (TID 68) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 8344 bytes) \n",
      "24/09/06 14:50:59 INFO Executor: Running task 3.0 in stage 47.0 (TID 67)\n",
      "24/09/06 14:50:59 INFO Executor: Running task 0.0 in stage 47.0 (TID 64)\n",
      "24/09/06 14:50:59 INFO Executor: Running task 4.0 in stage 47.0 (TID 68)\n",
      "24/09/06 14:50:59 INFO Executor: Running task 2.0 in stage 47.0 (TID 66)\n",
      "24/09/06 14:50:59 INFO Executor: Running task 1.0 in stage 47.0 (TID 65)\n",
      "24/09/06 14:50:59 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:59 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:59 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:59 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:59 INFO Executor: Finished task 4.0 in stage 47.0 (TID 68). 2146 bytes result sent to driver\n",
      "24/09/06 14:50:59 INFO Executor: Finished task 0.0 in stage 47.0 (TID 64). 2146 bytes result sent to driver\n",
      "24/09/06 14:50:59 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:50:59 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 64) in 21 ms on ubuntu (executor driver) (1/5)\n",
      "24/09/06 14:50:59 INFO TaskSetManager: Finished task 4.0 in stage 47.0 (TID 68) in 23 ms on ubuntu (executor driver) (2/5)\n",
      "24/09/06 14:50:59 INFO Executor: Finished task 3.0 in stage 47.0 (TID 67). 2146 bytes result sent to driver\n",
      "24/09/06 14:50:59 INFO Executor: Finished task 1.0 in stage 47.0 (TID 65). 2146 bytes result sent to driver\n",
      "24/09/06 14:50:59 INFO Executor: Finished task 2.0 in stage 47.0 (TID 66). 2146 bytes result sent to driver\n",
      "24/09/06 14:50:59 INFO TaskSetManager: Finished task 3.0 in stage 47.0 (TID 67) in 26 ms on ubuntu (executor driver) (3/5)\n",
      "24/09/06 14:50:59 INFO TaskSetManager: Finished task 2.0 in stage 47.0 (TID 66) in 28 ms on ubuntu (executor driver) (4/5)\n",
      "24/09/06 14:50:59 INFO TaskSetManager: Finished task 1.0 in stage 47.0 (TID 65) in 28 ms on ubuntu (executor driver) (5/5)\n",
      "24/09/06 14:50:59 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:59 INFO DAGScheduler: ShuffleMapStage 47 (count at cell50.sc:1) finished in 0,036 s\n",
      "24/09/06 14:50:59 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/06 14:50:59 INFO DAGScheduler: running: Set()\n",
      "24/09/06 14:50:59 INFO DAGScheduler: waiting: Set()\n",
      "24/09/06 14:50:59 INFO DAGScheduler: failed: Set()\n",
      "24/09/06 14:50:59 INFO SparkContext: Starting job: count at cell50.sc:1\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Got job 42 (count at cell50.sc:1) with 1 output partitions\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Final stage: ResultStage 49 (count at cell50.sc:1)\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 48)\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[162] at count at cell50.sc:1), which has no missing parents\n",
      "24/09/06 14:50:59 INFO MemoryStore: Block broadcast_74 stored as values in memory (estimated size 12.5 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO MemoryStore: Block broadcast_74_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Added broadcast_74_piece0 in memory on ubuntu:37607 (size: 5.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO SparkContext: Created broadcast 74 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[162] at count at cell50.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:50:59 INFO TaskSchedulerImpl: Adding task set 49.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:50:59 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 69) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/06 14:50:59 INFO Executor: Running task 0.0 in stage 49.0 (TID 69)\n",
      "24/09/06 14:50:59 INFO ShuffleBlockFetcherIterator: Getting 5 (300.0 B) non-empty blocks including 5 (300.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/06 14:50:59 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/09/06 14:50:59 INFO Executor: Finished task 0.0 in stage 49.0 (TID 69). 3995 bytes result sent to driver\n",
      "24/09/06 14:50:59 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 69) in 7 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:50:59 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:59 INFO DAGScheduler: ResultStage 49 (count at cell50.sc:1) finished in 0,014 s\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 49: Stage finished\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Job 42 finished: count at cell50.sc:1, took 0,019312 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres50\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m2535L\u001b[39m"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 to 5).map(a => customerDf).reduce((x, y) => x.union(y)).count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a00e4b2",
   "metadata": {},
   "source": [
    "### Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "40cf39a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:50:59 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "24/09/06 14:50:59 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "24/09/06 14:50:59 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:50:59 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:50:59 INFO MemoryStore: Block broadcast_75 stored as values in memory (estimated size 198.5 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO MemoryStore: Block broadcast_75_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Added broadcast_75_piece0 in memory on ubuntu:37607 (size: 33.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO SparkContext: Created broadcast 75 from load at cell51.sc:1\n",
      "24/09/06 14:50:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:50:59 INFO SparkContext: Starting job: load at cell51.sc:1\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Got job 43 (load at cell51.sc:1) with 2 output partitions\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Final stage: ResultStage 50 (load at cell51.sc:1)\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Submitting ResultStage 50 (MapPartitionsRDD[166] at load at cell51.sc:1), which has no missing parents\n",
      "24/09/06 14:50:59 INFO MemoryStore: Block broadcast_76 stored as values in memory (estimated size 15.8 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO MemoryStore: Block broadcast_76_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Added broadcast_76_piece0 in memory on ubuntu:37607 (size: 7.4 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO SparkContext: Created broadcast 76 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 50 (MapPartitionsRDD[166] at load at cell51.sc:1) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/09/06 14:50:59 INFO TaskSchedulerImpl: Adding task set 50.0 with 2 tasks resource profile 0\n",
      "24/09/06 14:50:59 INFO TaskSetManager: Starting task 0.0 in stage 50.0 (TID 70) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8244 bytes) \n",
      "24/09/06 14:50:59 INFO TaskSetManager: Starting task 1.0 in stage 50.0 (TID 71) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 8244 bytes) \n",
      "24/09/06 14:50:59 INFO Executor: Running task 0.0 in stage 50.0 (TID 70)\n",
      "24/09/06 14:50:59 INFO Executor: Running task 1.0 in stage 50.0 (TID 71)\n",
      "24/09/06 14:50:59 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/retail_data.json, range: 4194304-7997578, partition values: [empty row]\n",
      "24/09/06 14:50:59 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/retail_data.json, range: 0-4194304, partition values: [empty row]\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Removed broadcast_59_piece0 on ubuntu:37607 in memory (size: 5.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Removed broadcast_62_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Removed broadcast_56_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Removed broadcast_73_piece0 on ubuntu:37607 in memory (size: 9.1 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Removed broadcast_61_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Removed broadcast_58_piece0 on ubuntu:37607 in memory (size: 8.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Removed broadcast_67_piece0 on ubuntu:37607 in memory (size: 5.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Removed broadcast_74_piece0 on ubuntu:37607 in memory (size: 5.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Removed broadcast_70_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Removed broadcast_68_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Removed broadcast_69_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Removed broadcast_71_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Removed broadcast_64_piece0 on ubuntu:37607 in memory (size: 6.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Removed broadcast_57_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Removed broadcast_65_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Removed broadcast_60_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Removed broadcast_63_piece0 on ubuntu:37607 in memory (size: 9.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Removed broadcast_72_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Removed broadcast_66_piece0 on ubuntu:37607 in memory (size: 8.3 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO BlockManagerInfo: Removed broadcast_53_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:50:59 INFO Executor: Finished task 1.0 in stage 50.0 (TID 71). 2136 bytes result sent to driver\n",
      "24/09/06 14:50:59 INFO TaskSetManager: Finished task 1.0 in stage 50.0 (TID 71) in 202 ms on ubuntu (executor driver) (1/2)\n",
      "24/09/06 14:50:59 INFO Executor: Finished task 0.0 in stage 50.0 (TID 70). 2136 bytes result sent to driver\n",
      "24/09/06 14:50:59 INFO TaskSetManager: Finished task 0.0 in stage 50.0 (TID 70) in 227 ms on ubuntu (executor driver) (2/2)\n",
      "24/09/06 14:50:59 INFO TaskSchedulerImpl: Removed TaskSet 50.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:50:59 INFO DAGScheduler: ResultStage 50 (load at cell51.sc:1) finished in 0,231 s\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Job 43 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:50:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 50: Stage finished\n",
      "24/09/06 14:50:59 INFO DAGScheduler: Job 43 finished: load at cell51.sc:1, took 0,233745 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mretailDf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CustomerID: string, Description: string ... 5 more fields]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val retailDf = spark.read.format(\"json\").load(\"data/retail_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "916d34e1-e562-468d-bf51-47370b85fa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Birthdate: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "20889378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- UnitPrice: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retailDf.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fe8b62e3-0793-4cf4-86eb-03be86191551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres54\u001b[39m: \u001b[32mSet\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mSet\u001b[39m(\u001b[32m\"CustomerID\"\u001b[39m)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(customerDf.dtypes.map(_._1)).toSet.intersect((retailDf.dtypes.map(_._1)).toSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "12be704d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:51:00 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/09/06 14:51:00 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#179)\n",
      "24/09/06 14:51:00 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/09/06 14:51:00 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#865)\n",
      "24/09/06 14:51:00 INFO CodeGenerator: Code generated in 4.797589 ms\n",
      "24/09/06 14:51:00 INFO MemoryStore: Block broadcast_77 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:00 INFO MemoryStore: Block broadcast_77_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:00 INFO BlockManagerInfo: Added broadcast_77_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:00 INFO SparkContext: Created broadcast 77 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/06 14:51:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:51:00 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/06 14:51:00 INFO DAGScheduler: Got job 44 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "24/09/06 14:51:00 INFO DAGScheduler: Final stage: ResultStage 51 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "24/09/06 14:51:00 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:51:00 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:51:00 INFO DAGScheduler: Submitting ResultStage 51 (MapPartitionsRDD[170] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "24/09/06 14:51:00 INFO MemoryStore: Block broadcast_78 stored as values in memory (estimated size 16.8 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:00 INFO MemoryStore: Block broadcast_78_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:00 INFO BlockManagerInfo: Added broadcast_78_piece0 in memory on ubuntu:37607 (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:00 INFO SparkContext: Created broadcast 78 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:51:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 51 (MapPartitionsRDD[170] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:51:00 INFO TaskSchedulerImpl: Adding task set 51.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:51:00 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 72) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/06 14:51:00 INFO Executor: Running task 0.0 in stage 51.0 (TID 72)\n",
      "24/09/06 14:51:00 INFO CodeGenerator: Code generated in 5.458408 ms\n",
      "24/09/06 14:51:00 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:51:00 INFO CodeGenerator: Code generated in 3.639249 ms\n",
      "24/09/06 14:51:00 INFO Executor: Finished task 0.0 in stage 51.0 (TID 72). 63135 bytes result sent to driver\n",
      "24/09/06 14:51:00 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 72) in 28 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:51:00 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:51:00 INFO DAGScheduler: ResultStage 51 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0,031 s\n",
      "24/09/06 14:51:00 INFO DAGScheduler: Job 44 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:51:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 51: Stage finished\n",
      "24/09/06 14:51:00 INFO DAGScheduler: Job 44 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0,035287 s\n",
      "24/09/06 14:51:00 INFO CodeGenerator: Code generated in 3.704699 ms\n",
      "24/09/06 14:51:00 INFO MemoryStore: Block broadcast_79 stored as values in memory (estimated size 32.0 MiB, free 4.5 GiB)\n",
      "24/09/06 14:51:00 INFO MemoryStore: Block broadcast_79_piece0 stored as bytes in memory (estimated size 65.5 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:00 INFO BlockManagerInfo: Added broadcast_79_piece0 in memory on ubuntu:37607 (size: 65.5 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:00 INFO SparkContext: Created broadcast 79 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/06 14:51:00 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/09/06 14:51:00 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#865)\n",
      "24/09/06 14:51:00 INFO CodeGenerator: Code generated in 7.786952 ms\n",
      "24/09/06 14:51:00 INFO MemoryStore: Block broadcast_80 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:00 INFO MemoryStore: Block broadcast_80_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:00 INFO BlockManagerInfo: Added broadcast_80_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:00 INFO SparkContext: Created broadcast 80 from show at cell55.sc:1\n",
      "24/09/06 14:51:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:51:00 INFO SparkContext: Starting job: show at cell55.sc:1\n",
      "24/09/06 14:51:00 INFO DAGScheduler: Got job 45 (show at cell55.sc:1) with 1 output partitions\n",
      "24/09/06 14:51:00 INFO DAGScheduler: Final stage: ResultStage 52 (show at cell55.sc:1)\n",
      "24/09/06 14:51:00 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:51:00 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:51:00 INFO DAGScheduler: Submitting ResultStage 52 (MapPartitionsRDD[174] at show at cell55.sc:1), which has no missing parents\n",
      "24/09/06 14:51:00 INFO MemoryStore: Block broadcast_81 stored as values in memory (estimated size 20.9 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:00 INFO MemoryStore: Block broadcast_81_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:00 INFO BlockManagerInfo: Added broadcast_81_piece0 in memory on ubuntu:37607 (size: 8.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:00 INFO SparkContext: Created broadcast 81 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:51:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[174] at show at cell55.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:51:00 INFO TaskSchedulerImpl: Adding task set 52.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:51:00 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 73) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8244 bytes) \n",
      "24/09/06 14:51:00 INFO Executor: Running task 0.0 in stage 52.0 (TID 73)\n",
      "24/09/06 14:51:00 INFO CodeGenerator: Code generated in 10.507387 ms\n",
      "24/09/06 14:51:00 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/retail_data.json, range: 0-4194304, partition values: [empty row]\n",
      "24/09/06 14:51:00 INFO CodeGenerator: Code generated in 2.228008 ms\n",
      "24/09/06 14:51:00 INFO Executor: Finished task 0.0 in stage 52.0 (TID 73). 2730 bytes result sent to driver\n",
      "24/09/06 14:51:00 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 73) in 28 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:51:00 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:51:00 INFO DAGScheduler: ResultStage 52 (show at cell55.sc:1) finished in 0,036 s\n",
      "24/09/06 14:51:00 INFO DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:51:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 52: Stage finished\n",
      "24/09/06 14:51:00 INFO DAGScheduler: Job 45 finished: show at cell55.sc:1, took 0,039184 s\n",
      "24/09/06 14:51:00 INFO CodeGenerator: Code generated in 8.78065 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+--------------+--------------------+---------------+----------------+--------------------+---------------+---------+--------+---------+---------+\n",
      "|CustomerID|             Address|          Birthdate|       Country|               Email|           Name|        Username|         Description|    InvoiceDate|InvoiceNo|Quantity|StockCode|UnitPrice|\n",
      "+----------+--------------------+-------------------+--------------+--------------------+---------------+----------------+--------------------+---------------+---------+--------+---------+---------+\n",
      "|     12346|Unit 1047 Box 408...|1994-02-20 00:46:27|United Kingdom|cooperalexis@hotm...|  Lindsay Cowan|valenciajennifer|MEDIUM CERAMIC TO...|1/18/2011 10:01|   541431|   74215|    23166|     1.04|\n",
      "|     12346|Unit 1047 Box 408...|1994-02-20 00:46:27|United Kingdom|cooperalexis@hotm...|  Lindsay Cowan|valenciajennifer|MEDIUM CERAMIC TO...|1/18/2011 10:17|  C541433|  -74215|    23166|     1.04|\n",
      "|     12347|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|timothy78@hotmail...|Katherine David|      hillrachel|BLACK CANDELABRA ...|12/7/2010 14:57|   537626|      12|    85116|      2.1|\n",
      "|     12347|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|timothy78@hotmail...|Katherine David|      hillrachel|AIRLINE BAG VINTA...|12/7/2010 14:57|   537626|       4|    22375|     4.25|\n",
      "|     12347|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|timothy78@hotmail...|Katherine David|      hillrachel|COLOUR GLASS. STA...|12/7/2010 14:57|   537626|      12|    71477|     3.25|\n",
      "|     12347|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|timothy78@hotmail...|Katherine David|      hillrachel|MINI PAINT SET VI...|12/7/2010 14:57|   537626|      36|    22492|     0.65|\n",
      "|     12347|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|timothy78@hotmail...|Katherine David|      hillrachel|CLEAR DRAWER KNOB...|12/7/2010 14:57|   537626|      12|    22771|     1.25|\n",
      "|     12347|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|timothy78@hotmail...|Katherine David|      hillrachel|PINK DRAWER KNOB ...|12/7/2010 14:57|   537626|      12|    22772|     1.25|\n",
      "|     12347|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|timothy78@hotmail...|Katherine David|      hillrachel|GREEN DRAWER KNOB...|12/7/2010 14:57|   537626|      12|    22773|     1.25|\n",
      "|     12347|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|timothy78@hotmail...|Katherine David|      hillrachel|RED DRAWER KNOB A...|12/7/2010 14:57|   537626|      12|    22774|     1.25|\n",
      "+----------+--------------------+-------------------+--------------+--------------------+---------------+----------------+--------------------+---------------+---------+--------+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.join(retailDf, Seq(\"CustomerID\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "90400c03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:51:01 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:51:01 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:51:01 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/09/06 14:51:01 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#865)\n",
      "24/09/06 14:51:01 INFO CodeGenerator: Code generated in 6.81269 ms\n",
      "24/09/06 14:51:01 INFO MemoryStore: Block broadcast_82 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO SparkContext: Created broadcast 82 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/06 14:51:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:51:01 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/06 14:51:01 INFO DAGScheduler: Got job 46 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions\n",
      "24/09/06 14:51:01 INFO DAGScheduler: Final stage: ResultStage 53 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "24/09/06 14:51:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:51:01 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:51:01 INFO DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[178] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "24/09/06 14:51:01 INFO MemoryStore: Block broadcast_83 stored as values in memory (estimated size 17.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on ubuntu:37607 (size: 7.7 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO SparkContext: Created broadcast 83 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:51:01 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 53 (MapPartitionsRDD[178] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/09/06 14:51:01 INFO TaskSchedulerImpl: Adding task set 53.0 with 2 tasks resource profile 0\n",
      "24/09/06 14:51:01 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 74) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8244 bytes) \n",
      "24/09/06 14:51:01 INFO TaskSetManager: Starting task 1.0 in stage 53.0 (TID 75) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 8244 bytes) \n",
      "24/09/06 14:51:01 INFO Executor: Running task 0.0 in stage 53.0 (TID 74)\n",
      "24/09/06 14:51:01 INFO Executor: Running task 1.0 in stage 53.0 (TID 75)\n",
      "24/09/06 14:51:01 INFO CodeGenerator: Code generated in 9.296631 ms\n",
      "24/09/06 14:51:01 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/retail_data.json, range: 0-4194304, partition values: [empty row]\n",
      "24/09/06 14:51:01 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/retail_data.json, range: 4194304-7997578, partition values: [empty row]\n",
      "24/09/06 14:51:01 INFO BlockManagerInfo: Removed broadcast_78_piece0 on ubuntu:37607 in memory (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO BlockManagerInfo: Removed broadcast_75_piece0 on ubuntu:37607 in memory (size: 33.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO BlockManagerInfo: Removed broadcast_79_piece0 on ubuntu:37607 in memory (size: 65.5 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO BlockManagerInfo: Removed broadcast_77_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO BlockManagerInfo: Removed broadcast_76_piece0 on ubuntu:37607 in memory (size: 7.4 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO BlockManagerInfo: Removed broadcast_81_piece0 on ubuntu:37607 in memory (size: 8.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO BlockManagerInfo: Removed broadcast_80_piece0 on ubuntu:37607 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO Executor: Finished task 1.0 in stage 53.0 (TID 75). 980502 bytes result sent to driver\n",
      "24/09/06 14:51:01 INFO TaskSetManager: Finished task 1.0 in stage 53.0 (TID 75) in 188 ms on ubuntu (executor driver) (1/2)\n",
      "24/09/06 14:51:01 INFO MemoryStore: Block taskresult_74 stored as bytes in memory (estimated size 1080.2 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO BlockManagerInfo: Added taskresult_74 in memory on ubuntu:37607 (size: 1080.2 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO Executor: Finished task 0.0 in stage 53.0 (TID 74). 1106112 bytes result sent via BlockManager)\n",
      "24/09/06 14:51:01 INFO TransportClientFactory: Successfully created connection to ubuntu/192.168.88.20:37607 after 26 ms (0 ms spent in bootstraps)\n",
      "24/09/06 14:51:01 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 74) in 309 ms on ubuntu (executor driver) (2/2)\n",
      "24/09/06 14:51:01 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:51:01 INFO DAGScheduler: ResultStage 53 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0,313 s\n",
      "24/09/06 14:51:01 INFO BlockManagerInfo: Removed taskresult_74 on ubuntu:37607 in memory (size: 1080.2 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:51:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 53: Stage finished\n",
      "24/09/06 14:51:01 INFO DAGScheduler: Job 46 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0,315280 s\n",
      "24/09/06 14:51:01 INFO CodeGenerator: Code generated in 7.24165 ms\n",
      "24/09/06 14:51:01 INFO MemoryStore: Block broadcast_84 stored as values in memory (estimated size 34.0 MiB, free 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 2.2 MiB, free 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on ubuntu:37607 (size: 2.2 MiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO SparkContext: Created broadcast 84 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/06 14:51:01 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:51:01 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:51:01 INFO CodeGenerator: Code generated in 16.157872 ms\n",
      "24/09/06 14:51:01 INFO MemoryStore: Block broadcast_85 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO SparkContext: Created broadcast 85 from show at cell56.sc:4\n",
      "24/09/06 14:51:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:51:01 INFO SparkContext: Starting job: show at cell56.sc:4\n",
      "24/09/06 14:51:01 INFO DAGScheduler: Got job 47 (show at cell56.sc:4) with 1 output partitions\n",
      "24/09/06 14:51:01 INFO DAGScheduler: Final stage: ResultStage 54 (show at cell56.sc:4)\n",
      "24/09/06 14:51:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:51:01 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:51:01 INFO DAGScheduler: Submitting ResultStage 54 (MapPartitionsRDD[182] at show at cell56.sc:4), which has no missing parents\n",
      "24/09/06 14:51:01 INFO MemoryStore: Block broadcast_86 stored as values in memory (estimated size 22.5 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 9.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on ubuntu:37607 (size: 9.4 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:01 INFO SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:51:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 54 (MapPartitionsRDD[182] at show at cell56.sc:4) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:51:01 INFO TaskSchedulerImpl: Adding task set 54.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:51:01 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 76) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/06 14:51:01 INFO Executor: Running task 0.0 in stage 54.0 (TID 76)\n",
      "24/09/06 14:51:01 INFO CodeGenerator: Code generated in 11.813124 ms\n",
      "24/09/06 14:51:01 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:51:01 INFO Executor: Finished task 0.0 in stage 54.0 (TID 76). 2750 bytes result sent to driver\n",
      "24/09/06 14:51:01 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 76) in 22 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:51:01 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:51:01 INFO DAGScheduler: ResultStage 54 (show at cell56.sc:4) finished in 0,029 s\n",
      "24/09/06 14:51:01 INFO DAGScheduler: Job 47 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:51:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 54: Stage finished\n",
      "24/09/06 14:51:01 INFO DAGScheduler: Job 47 finished: show at cell56.sc:4, took 0,032951 s\n",
      "24/09/06 14:51:01 INFO CodeGenerator: Code generated in 6.334588 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------+----------+--------------------+---------------+----------------+---------------+--------------------+---------------+---------+--------+---------+---------+\n",
      "|             Address|          Birthdate|       Country|CustomerID|               Email|           Name|        Username|rightCustomerID|         Description|    InvoiceDate|InvoiceNo|Quantity|StockCode|UnitPrice|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+---------------+----------------+---------------+--------------------+---------------+---------+--------+---------+---------+\n",
      "|Unit 1047 Box 408...|1994-02-20 00:46:27|United Kingdom|     12346|cooperalexis@hotm...|  Lindsay Cowan|valenciajennifer|          12346|MEDIUM CERAMIC TO...|1/18/2011 10:17|  C541433|  -74215|    23166|     1.04|\n",
      "|Unit 1047 Box 408...|1994-02-20 00:46:27|United Kingdom|     12346|cooperalexis@hotm...|  Lindsay Cowan|valenciajennifer|          12346|MEDIUM CERAMIC TO...|1/18/2011 10:01|   541431|   74215|    23166|     1.04|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|Katherine David|      hillrachel|          12347|MINI PLAYING CARD...|12/7/2011 15:52|   581180|      20|    23508|     0.42|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|Katherine David|      hillrachel|          12347|MINI PLAYING CARD...|12/7/2011 15:52|   581180|      20|    23506|     0.42|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|Katherine David|      hillrachel|          12347|CHRISTMAS TABLE S...|12/7/2011 15:52|   581180|      16|    23271|     0.83|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|Katherine David|      hillrachel|          12347|PINK GOOSE FEATHE...|12/7/2011 15:52|   581180|      12|    21265|     1.95|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|Katherine David|      hillrachel|          12347|WOODLAND CHARLOTT...|12/7/2011 15:52|   581180|      10|    20719|     0.85|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|Katherine David|      hillrachel|          12347|  RABBIT NIGHT LIGHT|12/7/2011 15:52|   581180|      24|    23084|     1.79|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|Katherine David|      hillrachel|          12347|RED TOADSTOOL LED...|12/7/2011 15:52|   581180|      24|    21731|     1.65|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|Katherine David|      hillrachel|          12347|PINK NEW BAROQUEC...|12/7/2011 15:52|   581180|      24|   84625A|     0.85|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+---------------+----------------+---------------+--------------------+---------------+---------+--------+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mnewRetailDf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [rightCustomerID: string, Description: string ... 5 more fields]\n",
       "\u001b[36mcustomerRetailDf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Address: string, Birthdate: string ... 12 more fields]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newRetailDf = retailDf.withColumnRenamed(\"CustomerID\", \"rightCustomerID\")\n",
    "\n",
    "val customerRetailDf = customerDf.join(newRetailDf, customerDf(\"CustomerID\") === newRetailDf(\"rightCustomerID\"), \"left\")\n",
    "customerRetailDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a512977",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:51:02 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:51:02 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:51:02 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/09/06 14:51:02 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#865)\n",
      "24/09/06 14:51:02 INFO CodeGenerator: Code generated in 3.295787 ms\n",
      "24/09/06 14:51:02 INFO MemoryStore: Block broadcast_87 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:02 INFO MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:02 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:02 INFO SparkContext: Created broadcast 87 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/06 14:51:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:51:02 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/06 14:51:02 INFO DAGScheduler: Got job 48 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions\n",
      "24/09/06 14:51:02 INFO DAGScheduler: Final stage: ResultStage 55 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "24/09/06 14:51:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:51:02 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:51:02 INFO DAGScheduler: Submitting ResultStage 55 (MapPartitionsRDD[186] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "24/09/06 14:51:02 INFO MemoryStore: Block broadcast_88 stored as values in memory (estimated size 14.6 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:02 INFO MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:02 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on ubuntu:37607 (size: 7.1 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:02 INFO SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:51:02 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 55 (MapPartitionsRDD[186] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/09/06 14:51:02 INFO TaskSchedulerImpl: Adding task set 55.0 with 2 tasks resource profile 0\n",
      "24/09/06 14:51:02 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 77) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8244 bytes) \n",
      "24/09/06 14:51:02 INFO TaskSetManager: Starting task 1.0 in stage 55.0 (TID 78) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 8244 bytes) \n",
      "24/09/06 14:51:02 INFO Executor: Running task 1.0 in stage 55.0 (TID 78)\n",
      "24/09/06 14:51:02 INFO Executor: Running task 0.0 in stage 55.0 (TID 77)\n",
      "24/09/06 14:51:02 INFO CodeGenerator: Code generated in 5.303901 ms\n",
      "24/09/06 14:51:02 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/retail_data.json, range: 4194304-7997578, partition values: [empty row]\n",
      "24/09/06 14:51:02 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/retail_data.json, range: 0-4194304, partition values: [empty row]\n",
      "24/09/06 14:51:02 INFO Executor: Finished task 0.0 in stage 55.0 (TID 77). 7833 bytes result sent to driver\n",
      "24/09/06 14:51:02 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 77) in 77 ms on ubuntu (executor driver) (1/2)\n",
      "24/09/06 14:51:02 INFO Executor: Finished task 1.0 in stage 55.0 (TID 78). 7007 bytes result sent to driver\n",
      "24/09/06 14:51:02 INFO TaskSetManager: Finished task 1.0 in stage 55.0 (TID 78) in 95 ms on ubuntu (executor driver) (2/2)\n",
      "24/09/06 14:51:02 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:51:02 INFO DAGScheduler: ResultStage 55 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0,100 s\n",
      "24/09/06 14:51:02 INFO DAGScheduler: Job 48 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:51:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 55: Stage finished\n",
      "24/09/06 14:51:02 INFO DAGScheduler: Job 48 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0,102736 s\n",
      "24/09/06 14:51:02 INFO MemoryStore: Block broadcast_89 stored as values in memory (estimated size 34.0 MiB, free 4.5 GiB)\n",
      "24/09/06 14:51:02 INFO MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 178.7 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:02 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on ubuntu:37607 (size: 178.7 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:02 INFO SparkContext: Created broadcast 89 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/06 14:51:02 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:51:02 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:51:02 INFO CodeGenerator: Code generated in 6.733368 ms\n",
      "24/09/06 14:51:02 INFO MemoryStore: Block broadcast_90 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:02 INFO MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:02 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on ubuntu:37607 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:02 INFO SparkContext: Created broadcast 90 from show at cell57.sc:1\n",
      "24/09/06 14:51:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:51:02 INFO SparkContext: Starting job: show at cell57.sc:1\n",
      "24/09/06 14:51:02 INFO DAGScheduler: Got job 49 (show at cell57.sc:1) with 1 output partitions\n",
      "24/09/06 14:51:02 INFO DAGScheduler: Final stage: ResultStage 56 (show at cell57.sc:1)\n",
      "24/09/06 14:51:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:51:02 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:51:02 INFO DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[190] at show at cell57.sc:1), which has no missing parents\n",
      "24/09/06 14:51:02 INFO MemoryStore: Block broadcast_91 stored as values in memory (estimated size 16.7 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:02 INFO MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 4.5 GiB)\n",
      "24/09/06 14:51:02 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on ubuntu:37607 (size: 7.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:51:02 INFO SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:51:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 56 (MapPartitionsRDD[190] at show at cell57.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:51:02 INFO TaskSchedulerImpl: Adding task set 56.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:51:02 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 79) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/06 14:51:02 INFO Executor: Running task 0.0 in stage 56.0 (TID 79)\n",
      "24/09/06 14:51:02 INFO CodeGenerator: Code generated in 6.340301 ms\n",
      "24/09/06 14:51:02 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:51:02 INFO Executor: Finished task 0.0 in stage 56.0 (TID 79). 1633 bytes result sent to driver\n",
      "24/09/06 14:51:02 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 79) in 17 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:51:02 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:51:02 INFO DAGScheduler: ResultStage 56 (show at cell57.sc:1) finished in 0,026 s\n",
      "24/09/06 14:51:02 INFO DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:51:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 56: Stage finished\n",
      "24/09/06 14:51:02 INFO DAGScheduler: Job 49 finished: show at cell57.sc:1, took 0,030252 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|CustomerID|\n",
      "+----------+\n",
      "|     12346|\n",
      "|     12346|\n",
      "|     12347|\n",
      "|     12347|\n",
      "|     12347|\n",
      "|     12347|\n",
      "|     12347|\n",
      "|     12347|\n",
      "|     12347|\n",
      "|     12347|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerRetailDf.select(\"CustomerID\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45091db0-fbd1-4567-b5eb-faec32fd5325",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "25e2b3ef-f005-4fa1-b6e5-533bf3fd55e6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:51:02 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/09/06 14:51:02 INFO SparkUI: Stopped Spark web UI at http://ubuntu:4041\n",
      "24/09/06 14:51:02 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/09/06 14:51:02 INFO MemoryStore: MemoryStore cleared\n",
      "24/09/06 14:51:02 INFO BlockManager: BlockManager stopped\n",
      "24/09/06 14:51:02 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/09/06 14:51:02 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/09/06 14:51:02 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2f2362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
