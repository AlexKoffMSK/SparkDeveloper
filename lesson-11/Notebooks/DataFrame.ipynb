{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2515a1c-c4f9-4890-aa6d-641bf419cea6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.5.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dc99a5fd-2141-4ffe-afb1-4b0a595a75af",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\u001b[39m"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ab7dc3-9173-4d9e-9a91-97256abb3a30",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "49835da8-c4d0-461a-a2c8-3c0959118bb4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7df9de1d-1f63-41b1-999b-69614c723fae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@1bf7bb01\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "                .builder()\n",
    "                .master(\"local[*]\")\n",
    "                .appName(\"Dataframe API\")\n",
    "                .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59c101c3-900c-4a51-8f5f-bfc0ce535ef9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.version == 3.5.0\n"
     ]
    }
   ],
   "source": [
    "println(s\"spark.version == ${spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0928fb8-44ac-4f97-ab64-94faf88d9e6d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1 Создание DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74a8a2d",
   "metadata": {},
   "source": [
    "### createDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd693919",
   "metadata": {},
   "source": [
    "#### From List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fc8e4c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata1\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\u001b[32m\"1\"\u001b[39m, \u001b[32m\"Spark\"\u001b[39m),\n",
       "  (\u001b[32m\"2\"\u001b[39m, \u001b[32m\"Scala\"\u001b[39m),\n",
       "  (\u001b[32m\"3\"\u001b[39m, \u001b[32m\"Java\"\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data1 = Seq((\"1\", \"Spark\"), (\"2\", \"Scala\"), (\"3\", \"Java\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "607ab09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| _1|   _2|\n",
      "+---+-----+\n",
      "|  1|Spark|\n",
      "|  2|Scala|\n",
      "|  3| Java|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_1: string, _2: string]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = spark.createDataFrame(data1)\n",
    "df1.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ffde9fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|StudentID|Course|\n",
      "+---------+------+\n",
      "|        1| Spark|\n",
      "|        2| Scala|\n",
      "|        3|  Java|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf1WithNames\u001b[39m: \u001b[32mDataFrame\u001b[39m = [StudentID: string, Course: string]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1WithNames = df1.withColumnsRenamed(Map(\"_1\" -> \"StudentID\", \"_2\" -> \"Course\"))\n",
    "df1WithNames.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b603ce90",
   "metadata": {},
   "source": [
    "##### withSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7b8e03cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mschema1\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"StudentID\"\u001b[39m, StringType, \u001b[32mfalse\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Course\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {})\n",
       ")\n",
       "\u001b[36mschema2\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"StudentID\"\u001b[39m, StringType, \u001b[32mfalse\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Course\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {})\n",
       ")\n",
       "\u001b[36mschema3\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"StudentID\"\u001b[39m, StringType, \u001b[32mfalse\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Course\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {})\n",
       ")\n",
       "\u001b[36mres59_3\u001b[39m: \u001b[32mBoolean\u001b[39m = \u001b[32mtrue\u001b[39m\n",
       "\u001b[36mres59_4\u001b[39m: \u001b[32mBoolean\u001b[39m = \u001b[32mtrue\u001b[39m"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema1 = StructType(Array(StructField(\"StudentID\", StringType, false),\n",
    "                              StructField(\"Course\", StringType, true)))\n",
    "\n",
    "val schema2 = new StructType()\n",
    "                    .add(StructField(\"StudentID\", StringType, false))\n",
    "                    .add(StructField(\"Course\", StringType, true))\n",
    "\n",
    "val schema3 = StructType(\n",
    "                StructField(\"StudentID\", StringType, false) :: \n",
    "                StructField(\"Course\", StringType, true) :: \n",
    "                Nil)\n",
    "\n",
    "schema1.equals(schema2)\n",
    "schema2.equals(schema3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c308052f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|StudentID|Course|\n",
      "+---------+------+\n",
      "|        1| Spark|\n",
      "|        2| Scala|\n",
      "|        3|  Java|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mcollection.JavaConverters._\u001b[39m\n",
       "\u001b[36mdata2\u001b[39m: \u001b[32mjava\u001b[39m.\u001b[32mutil\u001b[39m.\u001b[32mList\u001b[39m[\u001b[32mRow\u001b[39m] = \u001b[33mSeqWrapper\u001b[39m(\u001b[33mList\u001b[39m([1,Spark], [2,Scala], [3,Java]))\n",
       "\u001b[36mdf2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [StudentID: string, Course: string]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collection.JavaConverters._\n",
    "\n",
    "val data2 = data1.map(s => Row(s._1, s._2)).asJava\n",
    "val df2 = spark.createDataFrame(data2, schema2)\n",
    "df2.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd336b",
   "metadata": {},
   "source": [
    "#### fromRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "eb8852af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:22:27 INFO SparkContext: Starting job: show at cell61.sc:3\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Got job 46 (show at cell61.sc:3) with 1 output partitions\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Final stage: ResultStage 52 (show at cell61.sc:3)\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Submitting ResultStage 52 (MapPartitionsRDD[173] at show at cell61.sc:3), which has no missing parents\n",
      "24/09/02 20:22:27 INFO MemoryStore: Block broadcast_82 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:22:27 INFO MemoryStore: Block broadcast_82_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 4.5 GiB)\n",
      "24/09/02 20:22:27 INFO BlockManagerInfo: Added broadcast_82_piece0 in memory on ubuntu:39321 (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:22:27 INFO SparkContext: Created broadcast 82 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[173] at show at cell61.sc:3) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:22:27 INFO TaskSchedulerImpl: Adding task set 52.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:22:27 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 73) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/02 20:22:27 INFO Executor: Running task 0.0 in stage 52.0 (TID 73)\n",
      "24/09/02 20:22:27 INFO Executor: Finished task 0.0 in stage 52.0 (TID 73). 1452 bytes result sent to driver\n",
      "24/09/02 20:22:27 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 73) in 27 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:22:27 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:22:27 INFO DAGScheduler: ResultStage 52 (show at cell61.sc:3) finished in 0,079 s\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:22:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 52: Stage finished\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Job 46 finished: show at cell61.sc:3, took 0,082965 s\n",
      "24/09/02 20:22:27 INFO SparkContext: Starting job: show at cell61.sc:3\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Got job 47 (show at cell61.sc:3) with 4 output partitions\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Final stage: ResultStage 53 (show at cell61.sc:3)\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Submitting ResultStage 53 (MapPartitionsRDD[173] at show at cell61.sc:3), which has no missing parents\n",
      "24/09/02 20:22:27 INFO MemoryStore: Block broadcast_83 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:22:27 INFO MemoryStore: Block broadcast_83_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 4.5 GiB)\n",
      "24/09/02 20:22:27 INFO BlockManagerInfo: Added broadcast_83_piece0 in memory on ubuntu:39321 (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:22:27 INFO SparkContext: Created broadcast 83 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 53 (MapPartitionsRDD[173] at show at cell61.sc:3) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\n",
      "24/09/02 20:22:27 INFO TaskSchedulerImpl: Adding task set 53.0 with 4 tasks resource profile 0\n",
      "24/09/02 20:22:27 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 74) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/02 20:22:27 INFO TaskSetManager: Starting task 1.0 in stage 53.0 (TID 75) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/02 20:22:27 INFO TaskSetManager: Starting task 2.0 in stage 53.0 (TID 76) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/02 20:22:27 INFO TaskSetManager: Starting task 3.0 in stage 53.0 (TID 77) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/02 20:22:27 INFO Executor: Running task 0.0 in stage 53.0 (TID 74)\n",
      "24/09/02 20:22:27 INFO Executor: Running task 1.0 in stage 53.0 (TID 75)\n",
      "24/09/02 20:22:27 INFO Executor: Running task 3.0 in stage 53.0 (TID 77)\n",
      "24/09/02 20:22:27 INFO Executor: Running task 2.0 in stage 53.0 (TID 76)\n",
      "24/09/02 20:22:27 INFO Executor: Finished task 0.0 in stage 53.0 (TID 74). 1409 bytes result sent to driver\n",
      "24/09/02 20:22:27 INFO Executor: Finished task 1.0 in stage 53.0 (TID 75). 1404 bytes result sent to driver\n",
      "24/09/02 20:22:27 INFO Executor: Finished task 3.0 in stage 53.0 (TID 77). 1452 bytes result sent to driver\n",
      "24/09/02 20:22:27 INFO Executor: Finished task 2.0 in stage 53.0 (TID 76). 1366 bytes result sent to driver\n",
      "24/09/02 20:22:27 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 74) in 9 ms on ubuntu (executor driver) (1/4)\n",
      "24/09/02 20:22:27 INFO BlockManagerInfo: Removed broadcast_82_piece0 on ubuntu:39321 in memory (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:22:27 INFO TaskSetManager: Finished task 3.0 in stage 53.0 (TID 77) in 36 ms on ubuntu (executor driver) (2/4)\n",
      "24/09/02 20:22:27 INFO TaskSetManager: Finished task 1.0 in stage 53.0 (TID 75) in 40 ms on ubuntu (executor driver) (3/4)\n",
      "24/09/02 20:22:27 INFO TaskSetManager: Finished task 2.0 in stage 53.0 (TID 76) in 40 ms on ubuntu (executor driver) (4/4)\n",
      "24/09/02 20:22:27 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:22:27 INFO DAGScheduler: ResultStage 53 (show at cell61.sc:3) finished in 0,050 s\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Job 47 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:22:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 53: Stage finished\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Job 47 finished: show at cell61.sc:3, took 0,055884 s\n",
      "24/09/02 20:22:27 INFO SparkContext: Starting job: show at cell61.sc:3\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Got job 48 (show at cell61.sc:3) with 3 output partitions\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Final stage: ResultStage 54 (show at cell61.sc:3)\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Submitting ResultStage 54 (MapPartitionsRDD[173] at show at cell61.sc:3), which has no missing parents\n",
      "24/09/02 20:22:27 INFO MemoryStore: Block broadcast_84 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:22:27 INFO MemoryStore: Block broadcast_84_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 4.5 GiB)\n",
      "24/09/02 20:22:27 INFO BlockManagerInfo: Added broadcast_84_piece0 in memory on ubuntu:39321 (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:22:27 INFO SparkContext: Created broadcast 84 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 54 (MapPartitionsRDD[173] at show at cell61.sc:3) (first 15 tasks are for partitions Vector(5, 6, 7))\n",
      "24/09/02 20:22:27 INFO TaskSchedulerImpl: Adding task set 54.0 with 3 tasks resource profile 0\n",
      "24/09/02 20:22:27 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 78) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/02 20:22:27 INFO TaskSetManager: Starting task 1.0 in stage 54.0 (TID 79) (ubuntu, executor driver, partition 6, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/02 20:22:27 INFO TaskSetManager: Starting task 2.0 in stage 54.0 (TID 80) (ubuntu, executor driver, partition 7, PROCESS_LOCAL, 7667 bytes) \n",
      "24/09/02 20:22:27 INFO Executor: Running task 1.0 in stage 54.0 (TID 79)\n",
      "24/09/02 20:22:27 INFO Executor: Running task 2.0 in stage 54.0 (TID 80)\n",
      "24/09/02 20:22:27 INFO Executor: Running task 0.0 in stage 54.0 (TID 78)\n",
      "24/09/02 20:22:27 INFO Executor: Finished task 1.0 in stage 54.0 (TID 79). 1409 bytes result sent to driver\n",
      "24/09/02 20:22:27 INFO Executor: Finished task 2.0 in stage 54.0 (TID 80). 1447 bytes result sent to driver\n",
      "24/09/02 20:22:27 INFO Executor: Finished task 0.0 in stage 54.0 (TID 78). 1404 bytes result sent to driver\n",
      "24/09/02 20:22:27 INFO TaskSetManager: Finished task 2.0 in stage 54.0 (TID 80) in 20 ms on ubuntu (executor driver) (1/3)\n",
      "24/09/02 20:22:27 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 78) in 32 ms on ubuntu (executor driver) (2/3)\n",
      "24/09/02 20:22:27 INFO TaskSetManager: Finished task 1.0 in stage 54.0 (TID 79) in 34 ms on ubuntu (executor driver) (3/3)\n",
      "24/09/02 20:22:27 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:22:27 INFO DAGScheduler: ResultStage 54 (show at cell61.sc:3) finished in 0,068 s\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Job 48 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:22:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 54: Stage finished\n",
      "24/09/02 20:22:27 INFO DAGScheduler: Job 48 finished: show at cell61.sc:3, took 0,083854 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| _1|   _2|\n",
      "+---+-----+\n",
      "|  1|Spark|\n",
      "|  2|Scala|\n",
      "|  3| Java|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd1\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = ParallelCollectionRDD[170] at parallelize at cell61.sc:1\n",
       "\u001b[36mfromRDD1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_1: string, _2: string]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = spark.sparkContext.parallelize(data1)\n",
    "val fromRDD1 = spark.createDataFrame(rdd1)\n",
    "fromRDD1.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e85415",
   "metadata": {},
   "source": [
    "##### withSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e4d1e502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:22:53 INFO SparkContext: Starting job: show at cell62.sc:3\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Got job 49 (show at cell62.sc:3) with 1 output partitions\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Final stage: ResultStage 55 (show at cell62.sc:3)\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Submitting ResultStage 55 (MapPartitionsRDD[177] at show at cell62.sc:3), which has no missing parents\n",
      "24/09/02 20:22:53 INFO MemoryStore: Block broadcast_85 stored as values in memory (estimated size 21.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:22:53 INFO MemoryStore: Block broadcast_85_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 4.5 GiB)\n",
      "24/09/02 20:22:53 INFO BlockManagerInfo: Added broadcast_85_piece0 in memory on ubuntu:39321 (size: 9.8 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:22:53 INFO SparkContext: Created broadcast 85 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 55 (MapPartitionsRDD[177] at show at cell62.sc:3) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:22:53 INFO TaskSchedulerImpl: Adding task set 55.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:22:53 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 81) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/02 20:22:53 INFO Executor: Running task 0.0 in stage 55.0 (TID 81)\n",
      "24/09/02 20:22:53 INFO Executor: Finished task 0.0 in stage 55.0 (TID 81). 1409 bytes result sent to driver\n",
      "24/09/02 20:22:53 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 81) in 22 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:22:53 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:22:53 INFO DAGScheduler: ResultStage 55 (show at cell62.sc:3) finished in 0,077 s\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Job 49 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:22:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 55: Stage finished\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Job 49 finished: show at cell62.sc:3, took 0,082404 s\n",
      "24/09/02 20:22:53 INFO SparkContext: Starting job: show at cell62.sc:3\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Got job 50 (show at cell62.sc:3) with 4 output partitions\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Final stage: ResultStage 56 (show at cell62.sc:3)\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[177] at show at cell62.sc:3), which has no missing parents\n",
      "24/09/02 20:22:53 INFO MemoryStore: Block broadcast_86 stored as values in memory (estimated size 21.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:22:53 INFO MemoryStore: Block broadcast_86_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 4.5 GiB)\n",
      "24/09/02 20:22:53 INFO BlockManagerInfo: Added broadcast_86_piece0 in memory on ubuntu:39321 (size: 9.8 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:22:53 INFO SparkContext: Created broadcast 86 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 56 (MapPartitionsRDD[177] at show at cell62.sc:3) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\n",
      "24/09/02 20:22:53 INFO TaskSchedulerImpl: Adding task set 56.0 with 4 tasks resource profile 0\n",
      "24/09/02 20:22:53 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 82) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/02 20:22:53 INFO TaskSetManager: Starting task 1.0 in stage 56.0 (TID 83) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/02 20:22:53 INFO TaskSetManager: Starting task 2.0 in stage 56.0 (TID 84) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/02 20:22:53 INFO TaskSetManager: Starting task 3.0 in stage 56.0 (TID 85) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/02 20:22:53 INFO Executor: Running task 0.0 in stage 56.0 (TID 82)\n",
      "24/09/02 20:22:53 INFO Executor: Running task 2.0 in stage 56.0 (TID 84)\n",
      "24/09/02 20:22:53 INFO Executor: Running task 3.0 in stage 56.0 (TID 85)\n",
      "24/09/02 20:22:53 INFO Executor: Running task 1.0 in stage 56.0 (TID 83)\n",
      "24/09/02 20:22:53 INFO Executor: Finished task 3.0 in stage 56.0 (TID 85). 1409 bytes result sent to driver\n",
      "24/09/02 20:22:53 INFO Executor: Finished task 2.0 in stage 56.0 (TID 84). 1366 bytes result sent to driver\n",
      "24/09/02 20:22:53 INFO Executor: Finished task 0.0 in stage 56.0 (TID 82). 1409 bytes result sent to driver\n",
      "24/09/02 20:22:53 INFO TaskSetManager: Finished task 3.0 in stage 56.0 (TID 85) in 12 ms on ubuntu (executor driver) (1/4)\n",
      "24/09/02 20:22:53 INFO TaskSetManager: Finished task 2.0 in stage 56.0 (TID 84) in 22 ms on ubuntu (executor driver) (2/4)\n",
      "24/09/02 20:22:53 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 82) in 28 ms on ubuntu (executor driver) (3/4)\n",
      "24/09/02 20:22:53 INFO Executor: Finished task 1.0 in stage 56.0 (TID 83). 1447 bytes result sent to driver\n",
      "24/09/02 20:22:53 INFO TaskSetManager: Finished task 1.0 in stage 56.0 (TID 83) in 43 ms on ubuntu (executor driver) (4/4)\n",
      "24/09/02 20:22:53 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:22:53 INFO DAGScheduler: ResultStage 56 (show at cell62.sc:3) finished in 0,067 s\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Job 50 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:22:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 56: Stage finished\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Job 50 finished: show at cell62.sc:3, took 0,079437 s\n",
      "24/09/02 20:22:53 INFO SparkContext: Starting job: show at cell62.sc:3\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Got job 51 (show at cell62.sc:3) with 3 output partitions\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Final stage: ResultStage 57 (show at cell62.sc:3)\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Submitting ResultStage 57 (MapPartitionsRDD[177] at show at cell62.sc:3), which has no missing parents\n",
      "24/09/02 20:22:53 INFO MemoryStore: Block broadcast_87 stored as values in memory (estimated size 21.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:22:53 INFO MemoryStore: Block broadcast_87_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 4.5 GiB)\n",
      "24/09/02 20:22:53 INFO BlockManagerInfo: Added broadcast_87_piece0 in memory on ubuntu:39321 (size: 9.8 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:22:53 INFO SparkContext: Created broadcast 87 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 57 (MapPartitionsRDD[177] at show at cell62.sc:3) (first 15 tasks are for partitions Vector(5, 6, 7))\n",
      "24/09/02 20:22:53 INFO TaskSchedulerImpl: Adding task set 57.0 with 3 tasks resource profile 0\n",
      "24/09/02 20:22:53 INFO TaskSetManager: Starting task 0.0 in stage 57.0 (TID 86) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/02 20:22:53 INFO TaskSetManager: Starting task 1.0 in stage 57.0 (TID 87) (ubuntu, executor driver, partition 6, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/02 20:22:53 INFO TaskSetManager: Starting task 2.0 in stage 57.0 (TID 88) (ubuntu, executor driver, partition 7, PROCESS_LOCAL, 7667 bytes) \n",
      "24/09/02 20:22:53 INFO Executor: Running task 2.0 in stage 57.0 (TID 88)\n",
      "24/09/02 20:22:53 INFO Executor: Running task 1.0 in stage 57.0 (TID 87)\n",
      "24/09/02 20:22:53 INFO Executor: Running task 0.0 in stage 57.0 (TID 86)\n",
      "24/09/02 20:22:53 INFO BlockManagerInfo: Removed broadcast_84_piece0 on ubuntu:39321 in memory (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:22:53 INFO Executor: Finished task 1.0 in stage 57.0 (TID 87). 1452 bytes result sent to driver\n",
      "24/09/02 20:22:53 INFO TaskSetManager: Finished task 1.0 in stage 57.0 (TID 87) in 68 ms on ubuntu (executor driver) (1/3)\n",
      "24/09/02 20:22:53 INFO Executor: Finished task 2.0 in stage 57.0 (TID 88). 1490 bytes result sent to driver\n",
      "24/09/02 20:22:53 INFO BlockManagerInfo: Removed broadcast_86_piece0 on ubuntu:39321 in memory (size: 9.8 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:22:53 INFO TaskSetManager: Finished task 2.0 in stage 57.0 (TID 88) in 76 ms on ubuntu (executor driver) (2/3)\n",
      "24/09/02 20:22:53 INFO Executor: Finished task 0.0 in stage 57.0 (TID 86). 1447 bytes result sent to driver\n",
      "24/09/02 20:22:53 INFO TaskSetManager: Finished task 0.0 in stage 57.0 (TID 86) in 86 ms on ubuntu (executor driver) (3/3)\n",
      "24/09/02 20:22:53 INFO TaskSchedulerImpl: Removed TaskSet 57.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:22:53 INFO BlockManagerInfo: Removed broadcast_85_piece0 on ubuntu:39321 in memory (size: 9.8 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:22:53 INFO DAGScheduler: ResultStage 57 (show at cell62.sc:3) finished in 0,124 s\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Job 51 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:22:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 57: Stage finished\n",
      "24/09/02 20:22:53 INFO DAGScheduler: Job 51 finished: show at cell62.sc:3, took 0,140246 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|StudentID|Course|\n",
      "+---------+------+\n",
      "|        1| Spark|\n",
      "|        2| Scala|\n",
      "|        3|  Java|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd2\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mRow\u001b[39m] = MapPartitionsRDD[174] at map at cell62.sc:1\n",
       "\u001b[36mfromRDD2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [StudentID: string, Course: string]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd2 = rdd1.map(s => Row(s._1, s._2))\n",
    "val fromRDD2 = spark.createDataFrame(rdd2, schema2)\n",
    "fromRDD2.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f7656",
   "metadata": {},
   "source": [
    "### toDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d021abd",
   "metadata": {},
   "source": [
    "#### From List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "010978b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| _1|   _2|\n",
      "+---+-----+\n",
      "|  1|Spark|\n",
      "|  2|Scala|\n",
      "|  3| Java|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf11\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_1: string, _2: string]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df11 = data1.toDF()\n",
    "df11.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a7da3bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|StudentID|Course|\n",
      "+---------+------+\n",
      "|        1| Spark|\n",
      "|        2| Scala|\n",
      "|        3|  Java|\n",
      "+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.toDF(\"StudentID\", \"Course\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b56c3330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|StudentID|Course|\n",
      "+---------+------+\n",
      "|        1| Spark|\n",
      "|        2| Scala|\n",
      "|        3|  Java|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mcolumns\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m\"StudentID\"\u001b[39m, \u001b[32m\"Course\"\u001b[39m)\n",
       "\u001b[36mdf12\u001b[39m: \u001b[32mDataFrame\u001b[39m = [StudentID: string, Course: string]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val columns = List(\"StudentID\", \"Course\")\n",
    "val df12 = data1.toDF(columns: _*)\n",
    "df12.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f508c0da",
   "metadata": {},
   "source": [
    "#### fromRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "14ca80f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:23:54 INFO SparkContext: Starting job: show at cell66.sc:2\n",
      "24/09/02 20:23:54 INFO DAGScheduler: Got job 52 (show at cell66.sc:2) with 1 output partitions\n",
      "24/09/02 20:23:54 INFO DAGScheduler: Final stage: ResultStage 58 (show at cell66.sc:2)\n",
      "24/09/02 20:23:54 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:23:54 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:23:54 INFO DAGScheduler: Submitting ResultStage 58 (MapPartitionsRDD[180] at show at cell66.sc:2), which has no missing parents\n",
      "24/09/02 20:23:54 INFO MemoryStore: Block broadcast_88 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:23:54 INFO MemoryStore: Block broadcast_88_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 4.5 GiB)\n",
      "24/09/02 20:23:54 INFO BlockManagerInfo: Added broadcast_88_piece0 in memory on ubuntu:39321 (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:23:54 INFO SparkContext: Created broadcast 88 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:23:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 58 (MapPartitionsRDD[180] at show at cell66.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:23:54 INFO TaskSchedulerImpl: Adding task set 58.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:23:54 INFO TaskSetManager: Starting task 0.0 in stage 58.0 (TID 89) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/02 20:23:54 INFO Executor: Running task 0.0 in stage 58.0 (TID 89)\n",
      "24/09/02 20:23:55 INFO Executor: Finished task 0.0 in stage 58.0 (TID 89). 1366 bytes result sent to driver\n",
      "24/09/02 20:23:55 INFO TaskSetManager: Finished task 0.0 in stage 58.0 (TID 89) in 11 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:23:55 INFO TaskSchedulerImpl: Removed TaskSet 58.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:23:55 INFO DAGScheduler: ResultStage 58 (show at cell66.sc:2) finished in 0,041 s\n",
      "24/09/02 20:23:55 INFO DAGScheduler: Job 52 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:23:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 58: Stage finished\n",
      "24/09/02 20:23:55 INFO DAGScheduler: Job 52 finished: show at cell66.sc:2, took 0,053333 s\n",
      "24/09/02 20:23:55 INFO SparkContext: Starting job: show at cell66.sc:2\n",
      "24/09/02 20:23:55 INFO DAGScheduler: Got job 53 (show at cell66.sc:2) with 4 output partitions\n",
      "24/09/02 20:23:55 INFO DAGScheduler: Final stage: ResultStage 59 (show at cell66.sc:2)\n",
      "24/09/02 20:23:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:23:55 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:23:55 INFO DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[180] at show at cell66.sc:2), which has no missing parents\n",
      "24/09/02 20:23:55 INFO MemoryStore: Block broadcast_89 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:23:55 INFO MemoryStore: Block broadcast_89_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 4.5 GiB)\n",
      "24/09/02 20:23:55 INFO BlockManagerInfo: Added broadcast_89_piece0 in memory on ubuntu:39321 (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:23:55 INFO SparkContext: Created broadcast 89 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:23:55 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 59 (MapPartitionsRDD[180] at show at cell66.sc:2) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\n",
      "24/09/02 20:23:55 INFO TaskSchedulerImpl: Adding task set 59.0 with 4 tasks resource profile 0\n",
      "24/09/02 20:23:55 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 90) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/02 20:23:55 INFO TaskSetManager: Starting task 1.0 in stage 59.0 (TID 91) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/02 20:23:55 INFO TaskSetManager: Starting task 2.0 in stage 59.0 (TID 92) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/02 20:23:55 INFO TaskSetManager: Starting task 3.0 in stage 59.0 (TID 93) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/02 20:23:55 INFO Executor: Running task 0.0 in stage 59.0 (TID 90)\n",
      "24/09/02 20:23:55 INFO Executor: Running task 2.0 in stage 59.0 (TID 92)\n",
      "24/09/02 20:23:55 INFO Executor: Running task 1.0 in stage 59.0 (TID 91)\n",
      "24/09/02 20:23:55 INFO Executor: Running task 3.0 in stage 59.0 (TID 93)\n",
      "24/09/02 20:23:55 INFO Executor: Finished task 3.0 in stage 59.0 (TID 93). 1366 bytes result sent to driver\n",
      "24/09/02 20:23:55 INFO TaskSetManager: Finished task 3.0 in stage 59.0 (TID 93) in 16 ms on ubuntu (executor driver) (1/4)\n",
      "24/09/02 20:23:55 INFO Executor: Finished task 1.0 in stage 59.0 (TID 91). 1490 bytes result sent to driver\n",
      "24/09/02 20:23:55 INFO Executor: Finished task 0.0 in stage 59.0 (TID 90). 1366 bytes result sent to driver\n",
      "24/09/02 20:23:55 INFO Executor: Finished task 2.0 in stage 59.0 (TID 92). 1409 bytes result sent to driver\n",
      "24/09/02 20:23:55 INFO TaskSetManager: Finished task 2.0 in stage 59.0 (TID 92) in 23 ms on ubuntu (executor driver) (2/4)\n",
      "24/09/02 20:23:55 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 90) in 26 ms on ubuntu (executor driver) (3/4)\n",
      "24/09/02 20:23:55 INFO TaskSetManager: Finished task 1.0 in stage 59.0 (TID 91) in 24 ms on ubuntu (executor driver) (4/4)\n",
      "24/09/02 20:23:55 INFO TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:23:55 INFO DAGScheduler: ResultStage 59 (show at cell66.sc:2) finished in 0,044 s\n",
      "24/09/02 20:23:55 INFO DAGScheduler: Job 53 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:23:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 59: Stage finished\n",
      "24/09/02 20:23:55 INFO DAGScheduler: Job 53 finished: show at cell66.sc:2, took 0,054918 s\n",
      "24/09/02 20:23:55 INFO SparkContext: Starting job: show at cell66.sc:2\n",
      "24/09/02 20:23:55 INFO DAGScheduler: Got job 54 (show at cell66.sc:2) with 3 output partitions\n",
      "24/09/02 20:23:55 INFO DAGScheduler: Final stage: ResultStage 60 (show at cell66.sc:2)\n",
      "24/09/02 20:23:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:23:55 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:23:55 INFO DAGScheduler: Submitting ResultStage 60 (MapPartitionsRDD[180] at show at cell66.sc:2), which has no missing parents\n",
      "24/09/02 20:23:55 INFO MemoryStore: Block broadcast_90 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:23:55 INFO MemoryStore: Block broadcast_90_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 4.5 GiB)\n",
      "24/09/02 20:23:55 INFO BlockManagerInfo: Added broadcast_90_piece0 in memory on ubuntu:39321 (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:23:55 INFO SparkContext: Created broadcast 90 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:23:55 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 60 (MapPartitionsRDD[180] at show at cell66.sc:2) (first 15 tasks are for partitions Vector(5, 6, 7))\n",
      "24/09/02 20:23:55 INFO TaskSchedulerImpl: Adding task set 60.0 with 3 tasks resource profile 0\n",
      "24/09/02 20:23:55 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 94) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/02 20:23:55 INFO TaskSetManager: Starting task 1.0 in stage 60.0 (TID 95) (ubuntu, executor driver, partition 6, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/02 20:23:55 INFO TaskSetManager: Starting task 2.0 in stage 60.0 (TID 96) (ubuntu, executor driver, partition 7, PROCESS_LOCAL, 7667 bytes) \n",
      "24/09/02 20:23:55 INFO Executor: Running task 0.0 in stage 60.0 (TID 94)\n",
      "24/09/02 20:23:55 INFO BlockManagerInfo: Removed broadcast_88_piece0 on ubuntu:39321 in memory (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:23:55 INFO Executor: Running task 2.0 in stage 60.0 (TID 96)\n",
      "24/09/02 20:23:55 INFO Executor: Running task 1.0 in stage 60.0 (TID 95)\n",
      "24/09/02 20:23:55 INFO Executor: Finished task 0.0 in stage 60.0 (TID 94). 1404 bytes result sent to driver\n",
      "24/09/02 20:23:55 INFO Executor: Finished task 1.0 in stage 60.0 (TID 95). 1366 bytes result sent to driver\n",
      "24/09/02 20:23:55 INFO BlockManagerInfo: Removed broadcast_89_piece0 on ubuntu:39321 in memory (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:23:55 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 94) in 12 ms on ubuntu (executor driver) (1/3)\n",
      "24/09/02 20:23:55 INFO TaskSetManager: Finished task 1.0 in stage 60.0 (TID 95) in 12 ms on ubuntu (executor driver) (2/3)\n",
      "24/09/02 20:23:55 INFO Executor: Finished task 2.0 in stage 60.0 (TID 96). 1447 bytes result sent to driver\n",
      "24/09/02 20:23:55 INFO TaskSetManager: Finished task 2.0 in stage 60.0 (TID 96) in 17 ms on ubuntu (executor driver) (3/3)\n",
      "24/09/02 20:23:55 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:23:55 INFO DAGScheduler: ResultStage 60 (show at cell66.sc:2) finished in 0,102 s\n",
      "24/09/02 20:23:55 INFO DAGScheduler: Job 54 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:23:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 60: Stage finished\n",
      "24/09/02 20:23:55 INFO DAGScheduler: Job 54 finished: show at cell66.sc:2, took 0,114695 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| _1|   _2|\n",
      "+---+-----+\n",
      "|  1|Spark|\n",
      "|  2|Scala|\n",
      "|  3| Java|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfromRDD11\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_1: string, _2: string]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fromRDD11 = rdd1.toDF()\n",
    "fromRDD11.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "02700001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:24:14 INFO SparkContext: Starting job: show at cell67.sc:2\n",
      "24/09/02 20:24:14 INFO DAGScheduler: Got job 55 (show at cell67.sc:2) with 1 output partitions\n",
      "24/09/02 20:24:14 INFO DAGScheduler: Final stage: ResultStage 61 (show at cell67.sc:2)\n",
      "24/09/02 20:24:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:24:14 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:24:14 INFO DAGScheduler: Submitting ResultStage 61 (MapPartitionsRDD[183] at show at cell67.sc:2), which has no missing parents\n",
      "24/09/02 20:24:14 INFO MemoryStore: Block broadcast_91 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:24:14 INFO MemoryStore: Block broadcast_91_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 4.5 GiB)\n",
      "24/09/02 20:24:14 INFO BlockManagerInfo: Added broadcast_91_piece0 in memory on ubuntu:39321 (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:24:14 INFO SparkContext: Created broadcast 91 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:24:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 61 (MapPartitionsRDD[183] at show at cell67.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:24:14 INFO TaskSchedulerImpl: Adding task set 61.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:24:14 INFO TaskSetManager: Starting task 0.0 in stage 61.0 (TID 97) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/02 20:24:14 INFO Executor: Running task 0.0 in stage 61.0 (TID 97)\n",
      "24/09/02 20:24:14 INFO Executor: Finished task 0.0 in stage 61.0 (TID 97). 1366 bytes result sent to driver\n",
      "24/09/02 20:24:14 INFO TaskSetManager: Finished task 0.0 in stage 61.0 (TID 97) in 11 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:24:14 INFO TaskSchedulerImpl: Removed TaskSet 61.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:24:14 INFO DAGScheduler: ResultStage 61 (show at cell67.sc:2) finished in 0,029 s\n",
      "24/09/02 20:24:14 INFO DAGScheduler: Job 55 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:24:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 61: Stage finished\n",
      "24/09/02 20:24:14 INFO DAGScheduler: Job 55 finished: show at cell67.sc:2, took 0,039936 s\n",
      "24/09/02 20:24:14 INFO SparkContext: Starting job: show at cell67.sc:2\n",
      "24/09/02 20:24:14 INFO DAGScheduler: Got job 56 (show at cell67.sc:2) with 4 output partitions\n",
      "24/09/02 20:24:14 INFO DAGScheduler: Final stage: ResultStage 62 (show at cell67.sc:2)\n",
      "24/09/02 20:24:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:24:14 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:24:14 INFO DAGScheduler: Submitting ResultStage 62 (MapPartitionsRDD[183] at show at cell67.sc:2), which has no missing parents\n",
      "24/09/02 20:24:15 INFO MemoryStore: Block broadcast_92 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:24:15 INFO MemoryStore: Block broadcast_92_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 4.5 GiB)\n",
      "24/09/02 20:24:15 INFO BlockManagerInfo: Added broadcast_92_piece0 in memory on ubuntu:39321 (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:24:15 INFO SparkContext: Created broadcast 92 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:24:15 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 62 (MapPartitionsRDD[183] at show at cell67.sc:2) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\n",
      "24/09/02 20:24:15 INFO TaskSchedulerImpl: Adding task set 62.0 with 4 tasks resource profile 0\n",
      "24/09/02 20:24:15 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 98) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/02 20:24:15 INFO TaskSetManager: Starting task 1.0 in stage 62.0 (TID 99) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/02 20:24:15 INFO TaskSetManager: Starting task 2.0 in stage 62.0 (TID 100) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/02 20:24:15 INFO TaskSetManager: Starting task 3.0 in stage 62.0 (TID 101) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/02 20:24:15 INFO Executor: Running task 2.0 in stage 62.0 (TID 100)\n",
      "24/09/02 20:24:15 INFO Executor: Running task 1.0 in stage 62.0 (TID 99)\n",
      "24/09/02 20:24:15 INFO Executor: Running task 3.0 in stage 62.0 (TID 101)\n",
      "24/09/02 20:24:15 INFO Executor: Finished task 1.0 in stage 62.0 (TID 99). 1404 bytes result sent to driver\n",
      "24/09/02 20:24:15 INFO Executor: Running task 0.0 in stage 62.0 (TID 98)\n",
      "24/09/02 20:24:15 INFO TaskSetManager: Finished task 1.0 in stage 62.0 (TID 99) in 12 ms on ubuntu (executor driver) (1/4)\n",
      "24/09/02 20:24:15 INFO Executor: Finished task 3.0 in stage 62.0 (TID 101). 1366 bytes result sent to driver\n",
      "24/09/02 20:24:15 INFO Executor: Finished task 0.0 in stage 62.0 (TID 98). 1366 bytes result sent to driver\n",
      "24/09/02 20:24:15 INFO Executor: Finished task 2.0 in stage 62.0 (TID 100). 1409 bytes result sent to driver\n",
      "24/09/02 20:24:15 INFO TaskSetManager: Finished task 3.0 in stage 62.0 (TID 101) in 15 ms on ubuntu (executor driver) (2/4)\n",
      "24/09/02 20:24:15 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 98) in 20 ms on ubuntu (executor driver) (3/4)\n",
      "24/09/02 20:24:15 INFO TaskSetManager: Finished task 2.0 in stage 62.0 (TID 100) in 18 ms on ubuntu (executor driver) (4/4)\n",
      "24/09/02 20:24:15 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:24:15 INFO DAGScheduler: ResultStage 62 (show at cell67.sc:2) finished in 0,034 s\n",
      "24/09/02 20:24:15 INFO DAGScheduler: Job 56 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:24:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 62: Stage finished\n",
      "24/09/02 20:24:15 INFO DAGScheduler: Job 56 finished: show at cell67.sc:2, took 0,046991 s\n",
      "24/09/02 20:24:15 INFO SparkContext: Starting job: show at cell67.sc:2\n",
      "24/09/02 20:24:15 INFO DAGScheduler: Got job 57 (show at cell67.sc:2) with 3 output partitions\n",
      "24/09/02 20:24:15 INFO DAGScheduler: Final stage: ResultStage 63 (show at cell67.sc:2)\n",
      "24/09/02 20:24:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:24:15 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:24:15 INFO DAGScheduler: Submitting ResultStage 63 (MapPartitionsRDD[183] at show at cell67.sc:2), which has no missing parents\n",
      "24/09/02 20:24:15 INFO MemoryStore: Block broadcast_93 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:24:15 INFO MemoryStore: Block broadcast_93_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 4.5 GiB)\n",
      "24/09/02 20:24:15 INFO BlockManagerInfo: Added broadcast_93_piece0 in memory on ubuntu:39321 (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:24:15 INFO SparkContext: Created broadcast 93 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:24:15 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 63 (MapPartitionsRDD[183] at show at cell67.sc:2) (first 15 tasks are for partitions Vector(5, 6, 7))\n",
      "24/09/02 20:24:15 INFO TaskSchedulerImpl: Adding task set 63.0 with 3 tasks resource profile 0\n",
      "24/09/02 20:24:15 INFO TaskSetManager: Starting task 0.0 in stage 63.0 (TID 102) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/02 20:24:15 INFO TaskSetManager: Starting task 1.0 in stage 63.0 (TID 103) (ubuntu, executor driver, partition 6, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/02 20:24:15 INFO TaskSetManager: Starting task 2.0 in stage 63.0 (TID 104) (ubuntu, executor driver, partition 7, PROCESS_LOCAL, 7667 bytes) \n",
      "24/09/02 20:24:15 INFO Executor: Running task 2.0 in stage 63.0 (TID 104)\n",
      "24/09/02 20:24:15 INFO Executor: Running task 0.0 in stage 63.0 (TID 102)\n",
      "24/09/02 20:24:15 INFO Executor: Running task 1.0 in stage 63.0 (TID 103)\n",
      "24/09/02 20:24:15 INFO Executor: Finished task 0.0 in stage 63.0 (TID 102). 1490 bytes result sent to driver\n",
      "24/09/02 20:24:15 INFO Executor: Finished task 2.0 in stage 63.0 (TID 104). 1490 bytes result sent to driver\n",
      "24/09/02 20:24:15 INFO TaskSetManager: Finished task 0.0 in stage 63.0 (TID 102) in 48 ms on ubuntu (executor driver) (1/3)\n",
      "24/09/02 20:24:15 INFO Executor: Finished task 1.0 in stage 63.0 (TID 103). 1452 bytes result sent to driver\n",
      "24/09/02 20:24:15 INFO BlockManagerInfo: Removed broadcast_91_piece0 on ubuntu:39321 in memory (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:24:15 INFO TaskSetManager: Finished task 2.0 in stage 63.0 (TID 104) in 50 ms on ubuntu (executor driver) (2/3)\n",
      "24/09/02 20:24:15 INFO TaskSetManager: Finished task 1.0 in stage 63.0 (TID 103) in 53 ms on ubuntu (executor driver) (3/3)\n",
      "24/09/02 20:24:15 INFO TaskSchedulerImpl: Removed TaskSet 63.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:24:15 INFO DAGScheduler: ResultStage 63 (show at cell67.sc:2) finished in 0,066 s\n",
      "24/09/02 20:24:15 INFO DAGScheduler: Job 57 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:24:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 63: Stage finished\n",
      "24/09/02 20:24:15 INFO DAGScheduler: Job 57 finished: show at cell67.sc:2, took 0,084406 s\n",
      "24/09/02 20:24:15 INFO BlockManagerInfo: Removed broadcast_92_piece0 on ubuntu:39321 in memory (size: 5.7 KiB, free: 4.5 GiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|StudentID|Course|\n",
      "+---------+------+\n",
      "|        1| Spark|\n",
      "|        2| Scala|\n",
      "|        3|  Java|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfromRDD12\u001b[39m: \u001b[32mDataFrame\u001b[39m = [StudentID: string, Course: string]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fromRDD12 = rdd1.toDF(columns: _*)\n",
    "fromRDD12.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8949d910",
   "metadata": {},
   "source": [
    "### fromFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "24814ffa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:24:29 INFO InMemoryFileIndex: It took 2 ms to list leaf files for 1 paths.\n",
      "24/09/02 20:24:29 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "24/09/02 20:24:29 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 20:24:29 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 20:24:29 INFO MemoryStore: Block broadcast_94 stored as values in memory (estimated size 198.5 KiB, free 4.5 GiB)\n",
      "24/09/02 20:24:29 INFO MemoryStore: Block broadcast_94_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 4.5 GiB)\n",
      "24/09/02 20:24:29 INFO BlockManagerInfo: Added broadcast_94_piece0 in memory on ubuntu:39321 (size: 33.9 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:24:29 INFO SparkContext: Created broadcast 94 from load at cell68.sc:1\n",
      "24/09/02 20:24:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 20:24:29 INFO SparkContext: Starting job: load at cell68.sc:1\n",
      "24/09/02 20:24:29 INFO DAGScheduler: Got job 58 (load at cell68.sc:1) with 1 output partitions\n",
      "24/09/02 20:24:29 INFO DAGScheduler: Final stage: ResultStage 64 (load at cell68.sc:1)\n",
      "24/09/02 20:24:29 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:24:29 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:24:29 INFO DAGScheduler: Submitting ResultStage 64 (MapPartitionsRDD[187] at load at cell68.sc:1), which has no missing parents\n",
      "24/09/02 20:24:29 INFO MemoryStore: Block broadcast_95 stored as values in memory (estimated size 15.8 KiB, free 4.5 GiB)\n",
      "24/09/02 20:24:29 INFO MemoryStore: Block broadcast_95_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:24:29 INFO BlockManagerInfo: Added broadcast_95_piece0 in memory on ubuntu:39321 (size: 7.4 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:24:29 INFO SparkContext: Created broadcast 95 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:24:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 64 (MapPartitionsRDD[187] at load at cell68.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:24:29 INFO TaskSchedulerImpl: Adding task set 64.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:24:29 INFO TaskSetManager: Starting task 0.0 in stage 64.0 (TID 105) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/02 20:24:29 INFO Executor: Running task 0.0 in stage 64.0 (TID 105)\n",
      "24/09/02 20:24:29 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 20:24:29 INFO Executor: Finished task 0.0 in stage 64.0 (TID 105). 2076 bytes result sent to driver\n",
      "24/09/02 20:24:29 INFO TaskSetManager: Finished task 0.0 in stage 64.0 (TID 105) in 73 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:24:29 INFO TaskSchedulerImpl: Removed TaskSet 64.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:24:29 INFO DAGScheduler: ResultStage 64 (load at cell68.sc:1) finished in 0,115 s\n",
      "24/09/02 20:24:29 INFO DAGScheduler: Job 58 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:24:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 64: Stage finished\n",
      "24/09/02 20:24:29 INFO DAGScheduler: Job 58 finished: load at cell68.sc:1, took 0,119007 s\n",
      "24/09/02 20:24:29 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 20:24:29 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 20:24:29 INFO MemoryStore: Block broadcast_96 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:24:29 INFO MemoryStore: Block broadcast_96_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:24:29 INFO BlockManagerInfo: Added broadcast_96_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:24:29 INFO SparkContext: Created broadcast 96 from show at cell68.sc:2\n",
      "24/09/02 20:24:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 20:24:29 INFO SparkContext: Starting job: show at cell68.sc:2\n",
      "24/09/02 20:24:29 INFO DAGScheduler: Got job 59 (show at cell68.sc:2) with 1 output partitions\n",
      "24/09/02 20:24:29 INFO DAGScheduler: Final stage: ResultStage 65 (show at cell68.sc:2)\n",
      "24/09/02 20:24:29 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:24:29 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:24:29 INFO DAGScheduler: Submitting ResultStage 65 (MapPartitionsRDD[191] at show at cell68.sc:2), which has no missing parents\n",
      "24/09/02 20:24:29 INFO MemoryStore: Block broadcast_97 stored as values in memory (estimated size 17.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:24:30 INFO MemoryStore: Block broadcast_97_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 4.5 GiB)\n",
      "24/09/02 20:24:30 INFO BlockManagerInfo: Added broadcast_97_piece0 in memory on ubuntu:39321 (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:24:30 INFO BlockManagerInfo: Removed broadcast_94_piece0 on ubuntu:39321 in memory (size: 33.9 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:24:30 INFO SparkContext: Created broadcast 97 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:24:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 65 (MapPartitionsRDD[191] at show at cell68.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:24:30 INFO TaskSchedulerImpl: Adding task set 65.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:24:30 INFO TaskSetManager: Starting task 0.0 in stage 65.0 (TID 106) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/02 20:24:30 INFO BlockManagerInfo: Removed broadcast_95_piece0 on ubuntu:39321 in memory (size: 7.4 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:24:30 INFO Executor: Running task 0.0 in stage 65.0 (TID 106)\n",
      "24/09/02 20:24:30 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 20:24:30 INFO Executor: Finished task 0.0 in stage 65.0 (TID 106). 2480 bytes result sent to driver\n",
      "24/09/02 20:24:30 INFO TaskSetManager: Finished task 0.0 in stage 65.0 (TID 106) in 41 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:24:30 INFO TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:24:30 INFO DAGScheduler: ResultStage 65 (show at cell68.sc:2) finished in 0,112 s\n",
      "24/09/02 20:24:30 INFO DAGScheduler: Job 59 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:24:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 65: Stage finished\n",
      "24/09/02 20:24:30 INFO DAGScheduler: Job 59 finished: show at cell68.sc:2, took 0,125168 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "|Address                                               |Birthdate          |Country       |CustomerID|Email                   |Name           |Username        |\n",
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "|Unit 1047 Box 4089\\nDPO AA 57348                      |1994-02-20 00:46:27|United Kingdom|12346     |cooperalexis@hotmail.com|Lindsay Cowan  |valenciajennifer|\n",
      "|55711 Janet Plaza Apt. 865\\nChristinachester, CT 62716|1988-06-21 00:15:34|Iceland       |12347     |timothy78@hotmail.com   |Katherine David|hillrachel      |\n",
      "|Unit 2676 Box 9352\\nDPO AA 38560                      |1974-11-26 15:30:20|Finland       |12348     |tcrawford@gmail.com     |Leslie Martinez|serranobrian    |\n",
      "|2765 Powers Meadow\\nHeatherfurt, CT 53165             |1977-05-06 23:57:35|Italy         |12349     |dustin37@yahoo.com      |Brad Cardenas  |charleshudson   |\n",
      "|17677 Mark Crest\\nWalterberg, IA 39017                |1996-09-13 19:14:27|Norway        |12350     |amyholland@yahoo.com    |Natalie Ford   |gregoryharrison |\n",
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfromFile1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Address: string, Birthdate: string ... 5 more fields]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fromFile1 = spark.read.format(\"json\").load(\"data/customer_data.json\")\n",
    "fromFile1.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ea6cd10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Birthdate: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fromFile1.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dbd6f48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mfileSchema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Address\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Birthdate\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Country\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CustomerID\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Email\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Name\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Username\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {})\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fileSchema = StructType(\n",
    "                    StructField(\"Address\", StringType, true) ::\n",
    "                    StructField(\"Birthdate\", StringType, true) ::\n",
    "                    StructField(\"Country\", StringType, true) ::\n",
    "                    StructField(\"CustomerID\", StringType, true) ::\n",
    "                    StructField(\"Email\", StringType, true) ::\n",
    "                    StructField(\"Name\", StringType, true) ::\n",
    "                    StructField(\"Username\", StringType, true) ::\n",
    "                    Nil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d51c6e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:25:10 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "24/09/02 20:25:10 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 20:25:10 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 20:25:10 INFO MemoryStore: Block broadcast_98 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:25:10 INFO MemoryStore: Block broadcast_98_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:25:10 INFO BlockManagerInfo: Added broadcast_98_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:25:10 INFO SparkContext: Created broadcast 98 from show at cell71.sc:2\n",
      "24/09/02 20:25:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 20:25:10 INFO SparkContext: Starting job: show at cell71.sc:2\n",
      "24/09/02 20:25:10 INFO DAGScheduler: Got job 60 (show at cell71.sc:2) with 1 output partitions\n",
      "24/09/02 20:25:10 INFO DAGScheduler: Final stage: ResultStage 66 (show at cell71.sc:2)\n",
      "24/09/02 20:25:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:25:10 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:25:10 INFO DAGScheduler: Submitting ResultStage 66 (MapPartitionsRDD[195] at show at cell71.sc:2), which has no missing parents\n",
      "24/09/02 20:25:10 INFO MemoryStore: Block broadcast_99 stored as values in memory (estimated size 17.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:25:10 INFO MemoryStore: Block broadcast_99_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 4.5 GiB)\n",
      "24/09/02 20:25:10 INFO BlockManagerInfo: Added broadcast_99_piece0 in memory on ubuntu:39321 (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:25:10 INFO SparkContext: Created broadcast 99 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:25:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 66 (MapPartitionsRDD[195] at show at cell71.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:25:10 INFO TaskSchedulerImpl: Adding task set 66.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:25:10 INFO TaskSetManager: Starting task 0.0 in stage 66.0 (TID 107) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/02 20:25:10 INFO Executor: Running task 0.0 in stage 66.0 (TID 107)\n",
      "24/09/02 20:25:10 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 20:25:10 INFO Executor: Finished task 0.0 in stage 66.0 (TID 107). 2480 bytes result sent to driver\n",
      "24/09/02 20:25:10 INFO TaskSetManager: Finished task 0.0 in stage 66.0 (TID 107) in 22 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:25:10 INFO DAGScheduler: ResultStage 66 (show at cell71.sc:2) finished in 0,050 s\n",
      "24/09/02 20:25:10 INFO DAGScheduler: Job 60 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:25:10 INFO TaskSchedulerImpl: Removed TaskSet 66.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:25:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 66: Stage finished\n",
      "24/09/02 20:25:10 INFO DAGScheduler: Job 60 finished: show at cell71.sc:2, took 0,056013 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "|Address                                               |Birthdate          |Country       |CustomerID|Email                   |Name           |Username        |\n",
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "|Unit 1047 Box 4089\\nDPO AA 57348                      |1994-02-20 00:46:27|United Kingdom|12346     |cooperalexis@hotmail.com|Lindsay Cowan  |valenciajennifer|\n",
      "|55711 Janet Plaza Apt. 865\\nChristinachester, CT 62716|1988-06-21 00:15:34|Iceland       |12347     |timothy78@hotmail.com   |Katherine David|hillrachel      |\n",
      "|Unit 2676 Box 9352\\nDPO AA 38560                      |1974-11-26 15:30:20|Finland       |12348     |tcrawford@gmail.com     |Leslie Martinez|serranobrian    |\n",
      "|2765 Powers Meadow\\nHeatherfurt, CT 53165             |1977-05-06 23:57:35|Italy         |12349     |dustin37@yahoo.com      |Brad Cardenas  |charleshudson   |\n",
      "|17677 Mark Crest\\nWalterberg, IA 39017                |1996-09-13 19:14:27|Norway        |12350     |amyholland@yahoo.com    |Natalie Ford   |gregoryharrison |\n",
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfromFile2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Address: string, Birthdate: string ... 5 more fields]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fromFile2 = spark.read.format(\"json\").schema(fileSchema).load(\"data/customer_data.json\")\n",
    "fromFile2.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab7d210",
   "metadata": {},
   "source": [
    "## 2 Основные операции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "067a8858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcustomerDf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Address: string, Birthdate: string ... 5 more fields]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val customerDf = fromFile1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c371153c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:34:55 INFO BlockManagerInfo: Removed broadcast_99_piece0 on ubuntu:39321 in memory (size: 7.6 KiB, free: 4.5 GiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Birthdate: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4e5bcb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres75\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Address\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Birthdate\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Country\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CustomerID\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Email\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Name\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Username\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {})\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerDf.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d7245a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres74\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m\"Address\"\u001b[39m, \u001b[32m\"StringType\"\u001b[39m),\n",
       "  (\u001b[32m\"Birthdate\"\u001b[39m, \u001b[32m\"StringType\"\u001b[39m),\n",
       "  (\u001b[32m\"Country\"\u001b[39m, \u001b[32m\"StringType\"\u001b[39m),\n",
       "  (\u001b[32m\"CustomerID\"\u001b[39m, \u001b[32m\"StringType\"\u001b[39m),\n",
       "  (\u001b[32m\"Email\"\u001b[39m, \u001b[32m\"StringType\"\u001b[39m),\n",
       "  (\u001b[32m\"Name\"\u001b[39m, \u001b[32m\"StringType\"\u001b[39m),\n",
       "  (\u001b[32m\"Username\"\u001b[39m, \u001b[32m\"StringType\"\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerDf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2cc65050",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:36:23 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 20:36:23 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 20:36:23 INFO MemoryStore: Block broadcast_102 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:36:23 INFO MemoryStore: Block broadcast_102_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:36:23 INFO BlockManagerInfo: Added broadcast_102_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:36:23 INFO SparkContext: Created broadcast 102 from head at cell77.sc:1\n",
      "24/09/02 20:36:23 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 20:36:23 INFO SparkContext: Starting job: head at cell77.sc:1\n",
      "24/09/02 20:36:23 INFO DAGScheduler: Got job 62 (head at cell77.sc:1) with 1 output partitions\n",
      "24/09/02 20:36:23 INFO DAGScheduler: Final stage: ResultStage 68 (head at cell77.sc:1)\n",
      "24/09/02 20:36:23 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:36:23 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:36:23 INFO DAGScheduler: Submitting ResultStage 68 (MapPartitionsRDD[201] at head at cell77.sc:1), which has no missing parents\n",
      "24/09/02 20:36:23 INFO MemoryStore: Block broadcast_103 stored as values in memory (estimated size 11.6 KiB, free 4.5 GiB)\n",
      "24/09/02 20:36:23 INFO MemoryStore: Block broadcast_103_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:36:23 INFO BlockManagerInfo: Added broadcast_103_piece0 in memory on ubuntu:39321 (size: 6.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:36:23 INFO SparkContext: Created broadcast 103 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:36:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 68 (MapPartitionsRDD[201] at head at cell77.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:36:23 INFO TaskSchedulerImpl: Adding task set 68.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:36:23 INFO TaskSetManager: Starting task 0.0 in stage 68.0 (TID 109) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/02 20:36:23 INFO Executor: Running task 0.0 in stage 68.0 (TID 109)\n",
      "24/09/02 20:36:23 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 20:36:23 INFO Executor: Finished task 0.0 in stage 68.0 (TID 109). 1623 bytes result sent to driver\n",
      "24/09/02 20:36:23 INFO TaskSetManager: Finished task 0.0 in stage 68.0 (TID 109) in 21 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:36:23 INFO TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:36:23 INFO DAGScheduler: ResultStage 68 (head at cell77.sc:1) finished in 0,039 s\n",
      "24/09/02 20:36:23 INFO DAGScheduler: Job 62 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:36:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 68: Stage finished\n",
      "24/09/02 20:36:23 INFO DAGScheduler: Job 62 finished: head at cell77.sc:1, took 0,044687 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mh\u001b[39m: \u001b[32mRow\u001b[39m = [Unit 1047 Box 4089\n",
       "DPO AA 57348,1994-02-20 00:46:27,United Kingdom,12346,cooperalexis@hotmail.com,Lindsay Cowan,valenciajennifer]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val h = customerDf.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "80a93dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres79\u001b[39m: \u001b[32mAny\u001b[39m = \u001b[32m\"1994-02-20 00:46:27\"\u001b[39m"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a9c61d",
   "metadata": {},
   "source": [
    "#### Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b78c5d8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:37:33 INFO BlockManagerInfo: Removed broadcast_105_piece0 on ubuntu:39321 in memory (size: 7.1 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:37:33 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 20:37:33 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 20:37:33 INFO BlockManagerInfo: Removed broadcast_104_piece0 on ubuntu:39321 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:37:33 INFO MemoryStore: Block broadcast_106 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:37:33 INFO MemoryStore: Block broadcast_106_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:37:33 INFO BlockManagerInfo: Added broadcast_106_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:37:33 INFO SparkContext: Created broadcast 106 from show at cell82.sc:1\n",
      "24/09/02 20:37:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 20:37:33 INFO SparkContext: Starting job: show at cell82.sc:1\n",
      "24/09/02 20:37:33 INFO DAGScheduler: Got job 64 (show at cell82.sc:1) with 1 output partitions\n",
      "24/09/02 20:37:33 INFO DAGScheduler: Final stage: ResultStage 70 (show at cell82.sc:1)\n",
      "24/09/02 20:37:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:37:33 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:37:33 INFO DAGScheduler: Submitting ResultStage 70 (MapPartitionsRDD[209] at show at cell82.sc:1), which has no missing parents\n",
      "24/09/02 20:37:33 INFO MemoryStore: Block broadcast_107 stored as values in memory (estimated size 14.6 KiB, free 4.5 GiB)\n",
      "24/09/02 20:37:33 INFO MemoryStore: Block broadcast_107_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 4.5 GiB)\n",
      "24/09/02 20:37:33 INFO BlockManagerInfo: Added broadcast_107_piece0 in memory on ubuntu:39321 (size: 7.1 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:37:33 INFO SparkContext: Created broadcast 107 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:37:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 70 (MapPartitionsRDD[209] at show at cell82.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:37:33 INFO TaskSchedulerImpl: Adding task set 70.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:37:33 INFO TaskSetManager: Starting task 0.0 in stage 70.0 (TID 111) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/02 20:37:33 INFO Executor: Running task 0.0 in stage 70.0 (TID 111)\n",
      "24/09/02 20:37:33 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 20:37:33 INFO Executor: Finished task 0.0 in stage 70.0 (TID 111). 2153 bytes result sent to driver\n",
      "24/09/02 20:37:33 INFO TaskSetManager: Finished task 0.0 in stage 70.0 (TID 111) in 16 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:37:33 INFO TaskSchedulerImpl: Removed TaskSet 70.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:37:33 INFO DAGScheduler: ResultStage 70 (show at cell82.sc:1) finished in 0,028 s\n",
      "24/09/02 20:37:33 INFO DAGScheduler: Job 64 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:37:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 70: Stage finished\n",
      "24/09/02 20:37:33 INFO DAGScheduler: Job 64 finished: show at cell82.sc:1, took 0,040012 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+\n",
      "|          Birthdate|       Country|\n",
      "+-------------------+--------------+\n",
      "|1994-02-20 00:46:27|United Kingdom|\n",
      "|1988-06-21 00:15:34|       Iceland|\n",
      "|1974-11-26 15:30:20|       Finland|\n",
      "|1977-05-06 23:57:35|         Italy|\n",
      "|1996-09-13 19:14:27|        Norway|\n",
      "|1969-06-21 03:39:20|        Norway|\n",
      "|1993-02-25 18:37:29|       Bahrain|\n",
      "|1993-03-13 12:37:34|         Spain|\n",
      "|1972-11-10 12:01:08|       Bahrain|\n",
      "|1973-01-13 17:17:26|      Portugal|\n",
      "|1989-11-24 17:12:54|   Switzerland|\n",
      "|1977-06-19 22:35:52|       Austria|\n",
      "|1983-09-21 05:22:18|        Cyprus|\n",
      "|1980-10-28 17:25:59|       Austria|\n",
      "|1982-09-01 09:12:57|       Belgium|\n",
      "|1979-02-03 03:42:47|       Belgium|\n",
      "|1974-12-21 13:27:20|   Unspecified|\n",
      "|1990-07-17 15:47:12|       Belgium|\n",
      "|1981-07-10 00:35:00|        Cyprus|\n",
      "|1989-12-26 00:58:01|       Denmark|\n",
      "+-------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.select(\"Birthdate\", \"Country\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1822538d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 22:24:11 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/01 22:24:11 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/01 22:24:11 INFO CodeGenerator: Code generated in 7.029218 ms\n",
      "24/09/01 22:24:11 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/01 22:24:11 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/01 22:24:11 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 22:24:11 INFO SparkContext: Created broadcast 22 from show at cell25.sc:1\n",
      "24/09/01 22:24:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/01 22:24:11 INFO SparkContext: Starting job: show at cell25.sc:1\n",
      "24/09/01 22:24:11 INFO DAGScheduler: Got job 17 (show at cell25.sc:1) with 1 output partitions\n",
      "24/09/01 22:24:11 INFO DAGScheduler: Final stage: ResultStage 17 (show at cell25.sc:1)\n",
      "24/09/01 22:24:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 22:24:11 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 22:24:11 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[36] at show at cell25.sc:1), which has no missing parents\n",
      "24/09/01 22:24:11 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 14.2 KiB, free 4.5 GiB)\n",
      "24/09/01 22:24:11 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 4.5 GiB)\n",
      "24/09/01 22:24:11 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on ubuntu:39321 (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 22:24:11 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 22:24:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[36] at show at cell25.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 22:24:11 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0\n",
      "24/09/01 22:24:11 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 37) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/01 22:24:11 INFO Executor: Running task 0.0 in stage 17.0 (TID 37)\n",
      "24/09/01 22:24:11 INFO CodeGenerator: Code generated in 14.494197 ms\n",
      "24/09/01 22:24:11 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/01 22:24:11 INFO Executor: Finished task 0.0 in stage 17.0 (TID 37). 1753 bytes result sent to driver\n",
      "24/09/01 22:24:11 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 37) in 53 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 22:24:11 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "24/09/01 22:24:11 INFO DAGScheduler: ResultStage 17 (show at cell25.sc:1) finished in 0,073 s\n",
      "24/09/01 22:24:11 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 22:24:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished\n",
      "24/09/01 22:24:11 INFO DAGScheduler: Job 17 finished: show at cell25.sc:1, took 0,081774 s\n",
      "24/09/01 22:24:11 INFO CodeGenerator: Code generated in 15.82483 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|       Country|\n",
      "+--------------+\n",
      "|United Kingdom|\n",
      "|       Iceland|\n",
      "|       Finland|\n",
      "|         Italy|\n",
      "|        Norway|\n",
      "|        Norway|\n",
      "|       Bahrain|\n",
      "|         Spain|\n",
      "|       Bahrain|\n",
      "|      Portugal|\n",
      "|   Switzerland|\n",
      "|       Austria|\n",
      "|        Cyprus|\n",
      "|       Austria|\n",
      "|       Belgium|\n",
      "|       Belgium|\n",
      "|   Unspecified|\n",
      "|       Belgium|\n",
      "|        Cyprus|\n",
      "|       Denmark|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.select(col(\"Country\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ee0f2ed4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:37:57 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 20:37:57 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 20:37:57 INFO MemoryStore: Block broadcast_110 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:37:57 INFO MemoryStore: Block broadcast_110_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:37:57 INFO BlockManagerInfo: Added broadcast_110_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:37:57 INFO SparkContext: Created broadcast 110 from show at cell84.sc:1\n",
      "24/09/02 20:37:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 20:37:57 INFO SparkContext: Starting job: show at cell84.sc:1\n",
      "24/09/02 20:37:57 INFO DAGScheduler: Got job 66 (show at cell84.sc:1) with 1 output partitions\n",
      "24/09/02 20:37:57 INFO DAGScheduler: Final stage: ResultStage 72 (show at cell84.sc:1)\n",
      "24/09/02 20:37:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:37:57 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:37:57 INFO DAGScheduler: Submitting ResultStage 72 (MapPartitionsRDD[217] at show at cell84.sc:1), which has no missing parents\n",
      "24/09/02 20:37:57 INFO MemoryStore: Block broadcast_111 stored as values in memory (estimated size 14.2 KiB, free 4.5 GiB)\n",
      "24/09/02 20:37:57 INFO MemoryStore: Block broadcast_111_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:37:57 INFO BlockManagerInfo: Added broadcast_111_piece0 in memory on ubuntu:39321 (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:37:57 INFO SparkContext: Created broadcast 111 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:37:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 72 (MapPartitionsRDD[217] at show at cell84.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:37:57 INFO TaskSchedulerImpl: Adding task set 72.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:37:57 INFO TaskSetManager: Starting task 0.0 in stage 72.0 (TID 113) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/02 20:37:57 INFO Executor: Running task 0.0 in stage 72.0 (TID 113)\n",
      "24/09/02 20:37:57 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 20:37:57 INFO Executor: Finished task 0.0 in stage 72.0 (TID 113). 1899 bytes result sent to driver\n",
      "24/09/02 20:37:57 INFO TaskSetManager: Finished task 0.0 in stage 72.0 (TID 113) in 10 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:37:57 INFO TaskSchedulerImpl: Removed TaskSet 72.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:37:57 INFO DAGScheduler: ResultStage 72 (show at cell84.sc:1) finished in 0,017 s\n",
      "24/09/02 20:37:57 INFO DAGScheduler: Job 66 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:37:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 72: Stage finished\n",
      "24/09/02 20:37:57 INFO DAGScheduler: Job 66 finished: show at cell84.sc:1, took 0,022087 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               Date|\n",
      "+-------------------+\n",
      "|1994-02-20 00:46:27|\n",
      "|1988-06-21 00:15:34|\n",
      "|1974-11-26 15:30:20|\n",
      "|1977-05-06 23:57:35|\n",
      "|1996-09-13 19:14:27|\n",
      "|1969-06-21 03:39:20|\n",
      "|1993-02-25 18:37:29|\n",
      "|1993-03-13 12:37:34|\n",
      "|1972-11-10 12:01:08|\n",
      "|1973-01-13 17:17:26|\n",
      "|1989-11-24 17:12:54|\n",
      "|1977-06-19 22:35:52|\n",
      "|1983-09-21 05:22:18|\n",
      "|1980-10-28 17:25:59|\n",
      "|1982-09-01 09:12:57|\n",
      "|1979-02-03 03:42:47|\n",
      "|1974-12-21 13:27:20|\n",
      "|1990-07-17 15:47:12|\n",
      "|1981-07-10 00:35:00|\n",
      "|1989-12-26 00:58:01|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.selectExpr(\"Birthdate as Date\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "250879d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:38:10 INFO BlockManagerInfo: Removed broadcast_110_piece0 on ubuntu:39321 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:38:10 INFO BlockManagerInfo: Removed broadcast_111_piece0 on ubuntu:39321 in memory (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:38:10 INFO BlockManagerInfo: Removed broadcast_109_piece0 on ubuntu:39321 in memory (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:38:10 INFO BlockManagerInfo: Removed broadcast_108_piece0 on ubuntu:39321 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:38:10 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 20:38:10 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 20:38:10 INFO MemoryStore: Block broadcast_112 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:38:10 INFO MemoryStore: Block broadcast_112_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:38:10 INFO BlockManagerInfo: Added broadcast_112_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:38:10 INFO SparkContext: Created broadcast 112 from show at cell85.sc:1\n",
      "24/09/02 20:38:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 20:38:10 INFO SparkContext: Starting job: show at cell85.sc:1\n",
      "24/09/02 20:38:10 INFO DAGScheduler: Got job 67 (show at cell85.sc:1) with 1 output partitions\n",
      "24/09/02 20:38:10 INFO DAGScheduler: Final stage: ResultStage 73 (show at cell85.sc:1)\n",
      "24/09/02 20:38:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:38:10 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:38:10 INFO DAGScheduler: Submitting ResultStage 73 (MapPartitionsRDD[221] at show at cell85.sc:1), which has no missing parents\n",
      "24/09/02 20:38:10 INFO MemoryStore: Block broadcast_113 stored as values in memory (estimated size 17.1 KiB, free 4.5 GiB)\n",
      "24/09/02 20:38:10 INFO MemoryStore: Block broadcast_113_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 4.5 GiB)\n",
      "24/09/02 20:38:10 INFO BlockManagerInfo: Added broadcast_113_piece0 in memory on ubuntu:39321 (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:38:10 INFO SparkContext: Created broadcast 113 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:38:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 73 (MapPartitionsRDD[221] at show at cell85.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:38:10 INFO TaskSchedulerImpl: Adding task set 73.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:38:10 INFO TaskSetManager: Starting task 0.0 in stage 73.0 (TID 114) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/02 20:38:10 INFO Executor: Running task 0.0 in stage 73.0 (TID 114)\n",
      "24/09/02 20:38:10 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 20:38:10 INFO Executor: Finished task 0.0 in stage 73.0 (TID 114). 4695 bytes result sent to driver\n",
      "24/09/02 20:38:10 INFO TaskSetManager: Finished task 0.0 in stage 73.0 (TID 114) in 14 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:38:10 INFO TaskSchedulerImpl: Removed TaskSet 73.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:38:10 INFO DAGScheduler: ResultStage 73 (show at cell85.sc:1) finished in 0,037 s\n",
      "24/09/02 20:38:10 INFO DAGScheduler: Job 67 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:38:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 73: Stage finished\n",
      "24/09/02 20:38:10 INFO DAGScheduler: Job 67 finished: show at cell85.sc:1, took 0,046011 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+----+\n",
      "|             Address|          Birthdate|       Country|CustomerID|               Email|             Name|        Username|Flag|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+----+\n",
      "|Unit 1047 Box 408...|1994-02-20 00:46:27|United Kingdom|     12346|cooperalexis@hotm...|    Lindsay Cowan|valenciajennifer|true|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|  Katherine David|      hillrachel|true|\n",
      "|Unit 2676 Box 935...|1974-11-26 15:30:20|       Finland|     12348| tcrawford@gmail.com|  Leslie Martinez|    serranobrian|true|\n",
      "|2765 Powers Meado...|1977-05-06 23:57:35|         Italy|     12349|  dustin37@yahoo.com|    Brad Cardenas|   charleshudson|true|\n",
      "|17677 Mark Crest\\...|1996-09-13 19:14:27|        Norway|     12350|amyholland@yahoo.com|     Natalie Ford| gregoryharrison|true|\n",
      "|50047 Smith Point...|1969-06-21 03:39:20|        Norway|     12352| vcarter@hotmail.com|      Dana Clarke|          hmyers|true|\n",
      "|633 Miller Turnpi...|1993-02-25 18:37:29|       Bahrain|     12353|   laura34@yahoo.com|     Gary Nichols|  andrewhamilton|true|\n",
      "|38456 Rachael Cau...|1993-03-13 12:37:34|         Spain|     12354|   zmelton@gmail.com|       John Parks|      matthewray|true|\n",
      "|4140 Pamela Hollo...|1972-11-10 12:01:08|       Bahrain|     12355|   scott50@yahoo.com|Jennifer Lawrence|          glopez|true|\n",
      "|8681 Karen Roads ...|1973-01-13 17:17:26|      Portugal|     12356|josephmacias@hotm...|    James Sanchez|        wesley20|true|\n",
      "|18637 Jessica Rid...|1989-11-24 17:12:54|   Switzerland|     12357|michael16@hotmail...|     Ashley Lopez|     thomasdavid|true|\n",
      "|2129 Joel Rapids\\...|1977-06-19 22:35:52|       Austria|     12358|michaelespinoza@g...| Dr. Angela Brown|      patricia44|true|\n",
      "|86636 Maria Viadu...|1983-09-21 05:22:18|        Cyprus|     12359|  ryanpena@yahoo.com|        John Vega|     nelsonmaria|true|\n",
      "|1579 Young Trail\\...|1980-10-28 17:25:59|       Austria|     12360|briannafrost@yaho...|     Lauren Clark|   portermichael|true|\n",
      "|USNS Howard\\nFPO ...|1982-09-01 09:12:57|       Belgium|     12361|virginia36@hotmai...|Jacqueline Haynes|   johnsonshelly|true|\n",
      "|70092 Adams Prair...|1979-02-03 03:42:47|       Belgium|     12362|   april04@gmail.com|     Brian Flores|    hunterdaniel|true|\n",
      "|7322 Owens Inlet ...|1974-12-21 13:27:20|   Unspecified|     12363|   omolina@gmail.com|Christopher Gomez|         james75|true|\n",
      "|86176 Katherine C...|1990-07-17 15:47:12|       Belgium|     12364|barbaraduncan@gma...|     Robert Burns|          eric10|true|\n",
      "|932 Jeremy Spring...|1981-07-10 00:35:00|        Cyprus|     12365|nicoleanderson@ho...|    Joshua Parker|     millerrenee|true|\n",
      "|USNV Chavez\\nFPO ...|1989-12-26 00:58:01|       Denmark|     12367|   aaron99@yahoo.com|Christine Douglas|       michael58|true|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.withColumn(\"Flag\", lit(true)).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7df0d8f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:38:47 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 20:38:47 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 20:38:47 INFO MemoryStore: Block broadcast_114 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:38:47 INFO MemoryStore: Block broadcast_114_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:38:47 INFO BlockManagerInfo: Added broadcast_114_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:38:47 INFO SparkContext: Created broadcast 114 from show at cell86.sc:2\n",
      "24/09/02 20:38:47 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 20:38:47 INFO SparkContext: Starting job: show at cell86.sc:2\n",
      "24/09/02 20:38:47 INFO DAGScheduler: Got job 68 (show at cell86.sc:2) with 1 output partitions\n",
      "24/09/02 20:38:47 INFO DAGScheduler: Final stage: ResultStage 74 (show at cell86.sc:2)\n",
      "24/09/02 20:38:47 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:38:47 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:38:47 INFO DAGScheduler: Submitting ResultStage 74 (MapPartitionsRDD[225] at show at cell86.sc:2), which has no missing parents\n",
      "24/09/02 20:38:47 INFO MemoryStore: Block broadcast_115 stored as values in memory (estimated size 17.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:38:47 INFO MemoryStore: Block broadcast_115_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 4.5 GiB)\n",
      "24/09/02 20:38:47 INFO BlockManagerInfo: Added broadcast_115_piece0 in memory on ubuntu:39321 (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:38:47 INFO SparkContext: Created broadcast 115 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:38:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 74 (MapPartitionsRDD[225] at show at cell86.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:38:47 INFO TaskSchedulerImpl: Adding task set 74.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:38:47 INFO TaskSetManager: Starting task 0.0 in stage 74.0 (TID 115) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/02 20:38:47 INFO Executor: Running task 0.0 in stage 74.0 (TID 115)\n",
      "24/09/02 20:38:47 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 20:38:47 INFO Executor: Finished task 0.0 in stage 74.0 (TID 115). 4581 bytes result sent to driver\n",
      "24/09/02 20:38:47 INFO TaskSetManager: Finished task 0.0 in stage 74.0 (TID 115) in 14 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:38:47 INFO TaskSchedulerImpl: Removed TaskSet 74.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:38:47 INFO DAGScheduler: ResultStage 74 (show at cell86.sc:2) finished in 0,021 s\n",
      "24/09/02 20:38:47 INFO DAGScheduler: Job 68 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:38:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 74: Stage finished\n",
      "24/09/02 20:38:47 INFO DAGScheduler: Job 68 finished: show at cell86.sc:2, took 0,024807 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "|             Address|               Date|       Country|CustomerID|               Email|             Name|        Username|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "|Unit 1047 Box 408...|1994-02-20 00:46:27|United Kingdom|     12346|cooperalexis@hotm...|    Lindsay Cowan|valenciajennifer|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|  Katherine David|      hillrachel|\n",
      "|Unit 2676 Box 935...|1974-11-26 15:30:20|       Finland|     12348| tcrawford@gmail.com|  Leslie Martinez|    serranobrian|\n",
      "|2765 Powers Meado...|1977-05-06 23:57:35|         Italy|     12349|  dustin37@yahoo.com|    Brad Cardenas|   charleshudson|\n",
      "|17677 Mark Crest\\...|1996-09-13 19:14:27|        Norway|     12350|amyholland@yahoo.com|     Natalie Ford| gregoryharrison|\n",
      "|50047 Smith Point...|1969-06-21 03:39:20|        Norway|     12352| vcarter@hotmail.com|      Dana Clarke|          hmyers|\n",
      "|633 Miller Turnpi...|1993-02-25 18:37:29|       Bahrain|     12353|   laura34@yahoo.com|     Gary Nichols|  andrewhamilton|\n",
      "|38456 Rachael Cau...|1993-03-13 12:37:34|         Spain|     12354|   zmelton@gmail.com|       John Parks|      matthewray|\n",
      "|4140 Pamela Hollo...|1972-11-10 12:01:08|       Bahrain|     12355|   scott50@yahoo.com|Jennifer Lawrence|          glopez|\n",
      "|8681 Karen Roads ...|1973-01-13 17:17:26|      Portugal|     12356|josephmacias@hotm...|    James Sanchez|        wesley20|\n",
      "|18637 Jessica Rid...|1989-11-24 17:12:54|   Switzerland|     12357|michael16@hotmail...|     Ashley Lopez|     thomasdavid|\n",
      "|2129 Joel Rapids\\...|1977-06-19 22:35:52|       Austria|     12358|michaelespinoza@g...| Dr. Angela Brown|      patricia44|\n",
      "|86636 Maria Viadu...|1983-09-21 05:22:18|        Cyprus|     12359|  ryanpena@yahoo.com|        John Vega|     nelsonmaria|\n",
      "|1579 Young Trail\\...|1980-10-28 17:25:59|       Austria|     12360|briannafrost@yaho...|     Lauren Clark|   portermichael|\n",
      "|USNS Howard\\nFPO ...|1982-09-01 09:12:57|       Belgium|     12361|virginia36@hotmai...|Jacqueline Haynes|   johnsonshelly|\n",
      "|70092 Adams Prair...|1979-02-03 03:42:47|       Belgium|     12362|   april04@gmail.com|     Brian Flores|    hunterdaniel|\n",
      "|7322 Owens Inlet ...|1974-12-21 13:27:20|   Unspecified|     12363|   omolina@gmail.com|Christopher Gomez|         james75|\n",
      "|86176 Katherine C...|1990-07-17 15:47:12|       Belgium|     12364|barbaraduncan@gma...|     Robert Burns|          eric10|\n",
      "|932 Jeremy Spring...|1981-07-10 00:35:00|        Cyprus|     12365|nicoleanderson@ho...|    Joshua Parker|     millerrenee|\n",
      "|USNV Chavez\\nFPO ...|1989-12-26 00:58:01|       Denmark|     12367|   aaron99@yahoo.com|Christine Douglas|       michael58|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdfRenamed\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Address: string, Date: string ... 5 more fields]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfRenamed = customerDf.withColumnRenamed(\"Birthdate\", \"Date\")\n",
    "dfRenamed.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4086588b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres87_0\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Address\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Birthdate\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Country\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CustomerID\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Email\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Name\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Username\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {})\n",
       ")\n",
       "\u001b[36mres87_1\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Address\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Date\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Country\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CustomerID\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Email\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Name\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Username\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {})\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerDf.schema\n",
    "dfRenamed.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39bab1b",
   "metadata": {},
   "source": [
    "#### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "30bc0966",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:39:37 INFO FileSourceStrategy: Pushed Filters: IsNotNull(Country),EqualTo(Country,Norway)\n",
      "24/09/02 20:39:37 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(Country#1175),(Country#1175 = Norway)\n",
      "24/09/02 20:39:37 INFO MemoryStore: Block broadcast_116 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:39:37 INFO MemoryStore: Block broadcast_116_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:39:37 INFO BlockManagerInfo: Added broadcast_116_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:39:37 INFO SparkContext: Created broadcast 116 from show at cell88.sc:1\n",
      "24/09/02 20:39:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 20:39:37 INFO SparkContext: Starting job: show at cell88.sc:1\n",
      "24/09/02 20:39:37 INFO DAGScheduler: Got job 69 (show at cell88.sc:1) with 1 output partitions\n",
      "24/09/02 20:39:37 INFO DAGScheduler: Final stage: ResultStage 75 (show at cell88.sc:1)\n",
      "24/09/02 20:39:37 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:39:37 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:39:37 INFO DAGScheduler: Submitting ResultStage 75 (MapPartitionsRDD[229] at show at cell88.sc:1), which has no missing parents\n",
      "24/09/02 20:39:37 INFO MemoryStore: Block broadcast_117 stored as values in memory (estimated size 17.7 KiB, free 4.5 GiB)\n",
      "24/09/02 20:39:37 INFO MemoryStore: Block broadcast_117_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 4.5 GiB)\n",
      "24/09/02 20:39:37 INFO BlockManagerInfo: Added broadcast_117_piece0 in memory on ubuntu:39321 (size: 7.9 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:39:37 INFO SparkContext: Created broadcast 117 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:39:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 75 (MapPartitionsRDD[229] at show at cell88.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:39:37 INFO TaskSchedulerImpl: Adding task set 75.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:39:37 INFO TaskSetManager: Starting task 0.0 in stage 75.0 (TID 116) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/02 20:39:37 INFO Executor: Running task 0.0 in stage 75.0 (TID 116)\n",
      "24/09/02 20:39:37 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 20:39:37 INFO Executor: Finished task 0.0 in stage 75.0 (TID 116). 3059 bytes result sent to driver\n",
      "24/09/02 20:39:37 INFO TaskSetManager: Finished task 0.0 in stage 75.0 (TID 116) in 23 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:39:37 INFO TaskSchedulerImpl: Removed TaskSet 75.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:39:37 INFO DAGScheduler: ResultStage 75 (show at cell88.sc:1) finished in 0,042 s\n",
      "24/09/02 20:39:37 INFO DAGScheduler: Job 69 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:39:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 75: Stage finished\n",
      "24/09/02 20:39:37 INFO DAGScheduler: Job 69 finished: show at cell88.sc:1, took 0,047180 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------+----------+--------------------+----------------+---------------+\n",
      "|             Address|          Birthdate|Country|CustomerID|               Email|            Name|       Username|\n",
      "+--------------------+-------------------+-------+----------+--------------------+----------------+---------------+\n",
      "|17677 Mark Crest\\...|1996-09-13 19:14:27| Norway|     12350|amyholland@yahoo.com|    Natalie Ford|gregoryharrison|\n",
      "|50047 Smith Point...|1969-06-21 03:39:20| Norway|     12352| vcarter@hotmail.com|     Dana Clarke|         hmyers|\n",
      "|0297 Jacob Ranch ...|1990-06-01 14:49:52| Norway|     12381|douglaschavez@hot...|   Matthew Jones|    stephanie68|\n",
      "|3102 Hopkins Walk...|1976-06-19 08:10:24| Norway|     12430|crystalromero@hot...|       Lisa Tate|       pgilbert|\n",
      "|637 Philip Lock S...|1984-06-06 09:36:14| Norway|     12432|jessica87@hotmail...|  Cheryl Herring|mathewsnicholas|\n",
      "|546 Tyler Prairie...|1985-05-27 10:39:47| Norway|     12433|mariahmcpherson@g...|  Kaitlin Miller|         lyoung|\n",
      "|6270 Jennifer Pra...|1977-06-01 20:40:04| Norway|     12436|lynncynthia@hotma...|    Rodney Giles|       swiggins|\n",
      "|415 Megan Parkway...|1971-08-29 06:21:22| Norway|     12438|  jeff42@hotmail.com| Thomas Figueroa|  matthewharris|\n",
      "|PSC 4266, Box 099...|1971-09-03 05:42:49| Norway|     12444| richard20@gmail.com|     Meghan Wood|   salazarbilly|\n",
      "|1333 Michael Vill...|1995-03-09 03:25:02| Norway|     12752|seanrobles@gmail.com|Lauren Hernandez|    morrowhenry|\n",
      "+--------------------+-------------------+-------+----------+--------------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.filter(\"Country = 'Norway'\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cb032124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:40:01 INFO FileSourceStrategy: Pushed Filters: IsNotNull(Country),Not(EqualTo(Country,Iceland))\n",
      "24/09/02 20:40:01 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(Country#1175),NOT (Country#1175 = Iceland)\n",
      "24/09/02 20:40:01 INFO MemoryStore: Block broadcast_118 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:40:01 INFO MemoryStore: Block broadcast_118_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:40:01 INFO BlockManagerInfo: Added broadcast_118_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:40:01 INFO SparkContext: Created broadcast 118 from show at cell89.sc:1\n",
      "24/09/02 20:40:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 20:40:01 INFO SparkContext: Starting job: show at cell89.sc:1\n",
      "24/09/02 20:40:01 INFO DAGScheduler: Got job 70 (show at cell89.sc:1) with 1 output partitions\n",
      "24/09/02 20:40:01 INFO DAGScheduler: Final stage: ResultStage 76 (show at cell89.sc:1)\n",
      "24/09/02 20:40:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:40:01 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:40:01 INFO DAGScheduler: Submitting ResultStage 76 (MapPartitionsRDD[233] at show at cell89.sc:1), which has no missing parents\n",
      "24/09/02 20:40:01 INFO BlockManagerInfo: Removed broadcast_117_piece0 on ubuntu:39321 in memory (size: 7.9 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:40:01 INFO MemoryStore: Block broadcast_119 stored as values in memory (estimated size 17.8 KiB, free 4.5 GiB)\n",
      "24/09/02 20:40:01 INFO BlockManagerInfo: Removed broadcast_116_piece0 on ubuntu:39321 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:40:01 INFO MemoryStore: Block broadcast_119_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:40:01 INFO BlockManagerInfo: Added broadcast_119_piece0 in memory on ubuntu:39321 (size: 8.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:40:01 INFO SparkContext: Created broadcast 119 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:40:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 76 (MapPartitionsRDD[233] at show at cell89.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:40:01 INFO TaskSchedulerImpl: Adding task set 76.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:40:01 INFO TaskSetManager: Starting task 0.0 in stage 76.0 (TID 117) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/02 20:40:01 INFO Executor: Running task 0.0 in stage 76.0 (TID 117)\n",
      "24/09/02 20:40:01 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 20:40:01 INFO Executor: Finished task 0.0 in stage 76.0 (TID 117). 4489 bytes result sent to driver\n",
      "24/09/02 20:40:01 INFO TaskSetManager: Finished task 0.0 in stage 76.0 (TID 117) in 18 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:40:01 INFO TaskSchedulerImpl: Removed TaskSet 76.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:40:01 INFO DAGScheduler: ResultStage 76 (show at cell89.sc:1) finished in 0,031 s\n",
      "24/09/02 20:40:01 INFO DAGScheduler: Job 70 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:40:01 INFO TaskSchedulerImpl: Killing all running tasks in stage 76: Stage finished\n",
      "24/09/02 20:40:01 INFO DAGScheduler: Job 70 finished: show at cell89.sc:1, took 0,057574 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+-------------------+--------------+----------+--------------------------+-----------------+----------------+\n",
      "|Address                                              |Birthdate          |Country       |CustomerID|Email                     |Name             |Username        |\n",
      "+-----------------------------------------------------+-------------------+--------------+----------+--------------------------+-----------------+----------------+\n",
      "|Unit 1047 Box 4089\\nDPO AA 57348                     |1994-02-20 00:46:27|United Kingdom|12346     |cooperalexis@hotmail.com  |Lindsay Cowan    |valenciajennifer|\n",
      "|Unit 2676 Box 9352\\nDPO AA 38560                     |1974-11-26 15:30:20|Finland       |12348     |tcrawford@gmail.com       |Leslie Martinez  |serranobrian    |\n",
      "|2765 Powers Meadow\\nHeatherfurt, CT 53165            |1977-05-06 23:57:35|Italy         |12349     |dustin37@yahoo.com        |Brad Cardenas    |charleshudson   |\n",
      "|17677 Mark Crest\\nWalterberg, IA 39017               |1996-09-13 19:14:27|Norway        |12350     |amyholland@yahoo.com      |Natalie Ford     |gregoryharrison |\n",
      "|50047 Smith Point Suite 162\\nWilkinsstad, PA 04106   |1969-06-21 03:39:20|Norway        |12352     |vcarter@hotmail.com       |Dana Clarke      |hmyers          |\n",
      "|633 Miller Turnpike\\nJonathanland, OR 62874          |1993-02-25 18:37:29|Bahrain       |12353     |laura34@yahoo.com         |Gary Nichols     |andrewhamilton  |\n",
      "|38456 Rachael Causeway Apt. 735\\nEvanfort, AR 33893  |1993-03-13 12:37:34|Spain         |12354     |zmelton@gmail.com         |John Parks       |matthewray      |\n",
      "|4140 Pamela Hollow Apt. 849\\nEast Elizabeth, TN 29566|1972-11-10 12:01:08|Bahrain       |12355     |scott50@yahoo.com         |Jennifer Lawrence|glopez          |\n",
      "|8681 Karen Roads Apt. 096\\nLowehaven, IA 19798       |1973-01-13 17:17:26|Portugal      |12356     |josephmacias@hotmail.com  |James Sanchez    |wesley20        |\n",
      "|18637 Jessica Ridge Apt. 157\\nGrossberg, ME 84127    |1989-11-24 17:12:54|Switzerland   |12357     |michael16@hotmail.com     |Ashley Lopez     |thomasdavid     |\n",
      "|2129 Joel Rapids\\nLisahaven, NE 08609                |1977-06-19 22:35:52|Austria       |12358     |michaelespinoza@gmail.com |Dr. Angela Brown |patricia44      |\n",
      "|86636 Maria Viaduct\\nKennethhaven, SD 21876          |1983-09-21 05:22:18|Cyprus        |12359     |ryanpena@yahoo.com        |John Vega        |nelsonmaria     |\n",
      "|1579 Young Trail\\nJessechester, OH 88328             |1980-10-28 17:25:59|Austria       |12360     |briannafrost@yahoo.com    |Lauren Clark     |portermichael   |\n",
      "|USNS Howard\\nFPO AP 30863                            |1982-09-01 09:12:57|Belgium       |12361     |virginia36@hotmail.com    |Jacqueline Haynes|johnsonshelly   |\n",
      "|70092 Adams Prairie\\nTurnerborough, TX 38603         |1979-02-03 03:42:47|Belgium       |12362     |april04@gmail.com         |Brian Flores     |hunterdaniel    |\n",
      "|7322 Owens Inlet Apt. 688\\nPort Leslie, OR 81893     |1974-12-21 13:27:20|Unspecified   |12363     |omolina@gmail.com         |Christopher Gomez|james75         |\n",
      "|86176 Katherine Common\\nWebbhaven, WA 51980          |1990-07-17 15:47:12|Belgium       |12364     |barbaraduncan@gmail.com   |Robert Burns     |eric10          |\n",
      "|932 Jeremy Springs Suite 144\\nJohnmouth, NM 02561    |1981-07-10 00:35:00|Cyprus        |12365     |nicoleanderson@hotmail.com|Joshua Parker    |millerrenee     |\n",
      "|USNV Chavez\\nFPO AP 78727                            |1989-12-26 00:58:01|Denmark       |12367     |aaron99@yahoo.com         |Christine Douglas|michael58       |\n",
      "|565 Hodge Motorway Suite 101\\nWendyberg, FL 57099    |1973-12-21 03:33:47|Cyprus        |12370     |qgibson@hotmail.com       |Derek Curtis     |zsanders        |\n",
      "+-----------------------------------------------------+-------------------+--------------+----------+--------------------------+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.where($\"Country\" =!= \"Iceland\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5044e9ab",
   "metadata": {},
   "source": [
    "#### Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c5c70ef1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:40:20 INFO BlockManagerInfo: Removed broadcast_119_piece0 on ubuntu:39321 in memory (size: 8.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:40:20 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 20:40:20 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 20:40:20 INFO MemoryStore: Block broadcast_120 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:40:20 INFO MemoryStore: Block broadcast_120_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:40:20 INFO BlockManagerInfo: Added broadcast_120_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:40:20 INFO SparkContext: Created broadcast 120 from show at cell90.sc:1\n",
      "24/09/02 20:40:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 20:40:20 INFO SparkContext: Starting job: show at cell90.sc:1\n",
      "24/09/02 20:40:20 INFO DAGScheduler: Got job 71 (show at cell90.sc:1) with 1 output partitions\n",
      "24/09/02 20:40:20 INFO DAGScheduler: Final stage: ResultStage 77 (show at cell90.sc:1)\n",
      "24/09/02 20:40:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:40:20 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:40:20 INFO DAGScheduler: Submitting ResultStage 77 (MapPartitionsRDD[237] at show at cell90.sc:1), which has no missing parents\n",
      "24/09/02 20:40:20 INFO MemoryStore: Block broadcast_121 stored as values in memory (estimated size 14.3 KiB, free 4.5 GiB)\n",
      "24/09/02 20:40:20 INFO MemoryStore: Block broadcast_121_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:40:20 INFO BlockManagerInfo: Added broadcast_121_piece0 in memory on ubuntu:39321 (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:40:20 INFO SparkContext: Created broadcast 121 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:40:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 77 (MapPartitionsRDD[237] at show at cell90.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:40:20 INFO TaskSchedulerImpl: Adding task set 77.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:40:20 INFO TaskSetManager: Starting task 0.0 in stage 77.0 (TID 118) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/02 20:40:20 INFO Executor: Running task 0.0 in stage 77.0 (TID 118)\n",
      "24/09/02 20:40:20 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 20:40:20 INFO Executor: Finished task 0.0 in stage 77.0 (TID 118). 6279 bytes result sent to driver\n",
      "24/09/02 20:40:20 INFO TaskSetManager: Finished task 0.0 in stage 77.0 (TID 118) in 29 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:40:20 INFO TaskSchedulerImpl: Removed TaskSet 77.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:40:20 INFO DAGScheduler: ResultStage 77 (show at cell90.sc:1) finished in 0,043 s\n",
      "24/09/02 20:40:20 INFO DAGScheduler: Job 71 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:40:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 77: Stage finished\n",
      "24/09/02 20:40:20 INFO DAGScheduler: Job 71 finished: show at cell90.sc:1, took 0,049481 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------+----------+--------------------+--------------------+-----------------+\n",
      "|             Address|          Birthdate|       Country|CustomerID|               Email|                Name|         Username|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+--------------------+-----------------+\n",
      "|6942 Connie Skywa...|1973-10-24 00:52:10|United Kingdom|     12989| amber97@hotmail.com|   Brandon Contreras|           ecasey|\n",
      "|79375 David Neck\\...|1971-05-04 22:20:10|United Kingdom|     12988|   erica98@gmail.com|      Gabriel Romero|          qknight|\n",
      "|00881 West Flat\\n...|1997-03-05 19:20:57|United Kingdom|     12987|    vkeith@yahoo.com|Christopher Lawrence|        smcintyre|\n",
      "|499 Jonathan Stre...|1987-10-24 20:05:15|United Kingdom|     12985| fredsmith@yahoo.com|        Xavier Myers|stricklandjeffery|\n",
      "|9505 Melissa Stre...|1975-09-22 15:21:58|United Kingdom|     12984|scottjonathan@yah...|        Brandy Huang|   amandawilliams|\n",
      "|399 Fuentes Roads...|1986-09-30 18:12:45|United Kingdom|     12982|cynthia31@hotmail...|      Linda Stephens|     davidsonomar|\n",
      "|1573 Jessica Glen...|1984-07-23 03:09:18|United Kingdom|     12981|  esharp@hotmail.com|          Dawn Woods|         steven67|\n",
      "|153 Tara Ridges S...|1974-03-03 07:52:15|United Kingdom|     12980| jessica87@gmail.com|         Sean Brooks|        kristen26|\n",
      "|62134 Chen Valley...|1990-10-09 01:29:02|United Kingdom|     12977| fmatthews@gmail.com|          Kyle Simon|          emily28|\n",
      "|7521 Christopher ...|1973-10-10 23:57:51|United Kingdom|     12976|williamsheidi@yah...|         Hannah Rose|         eugene04|\n",
      "|00679 Lucero Moun...|1987-10-13 12:41:52|United Kingdom|     12974|thomasreyes@yahoo...|     Daniel Fletcher|  velazquezangela|\n",
      "|885 Zamora Hills\\...|1986-11-14 14:18:47|United Kingdom|     12971|   cwilcox@yahoo.com|       Caitlin Walls|         ashley11|\n",
      "|4539 Powers Orcha...|1990-09-11 06:01:18|United Kingdom|     12970|edwardspeter@yaho...|      Jonathan Hines|          mmiller|\n",
      "|335 Lewis Land\\nL...|1994-04-25 16:59:48|United Kingdom|     12968| mmurray@hotmail.com|         Susan Davis|   jacksoncolleen|\n",
      "|Unit 3978 Box 615...|1969-05-28 22:09:26|United Kingdom|     12967|thomasjames@gmail...|    Amber Williamson|    justinjohnson|\n",
      "|PSC 6600, Box 447...|1975-10-14 17:46:59|United Kingdom|     12966|colinward@hotmail...|        Amy Robinson|     sarathompson|\n",
      "|0240 Ernest Under...|1968-06-14 23:10:47|United Kingdom|     12965|jeremy10@hotmail.com|      Raymond Patton|      meganbrewer|\n",
      "|2433 Amy Shoals\\n...|1975-11-05 04:34:04|United Kingdom|     12963|  tjohnson@yahoo.com|        Sheila Parks|      dominique55|\n",
      "|833 Wilson Street...|1978-12-25 10:12:45|United Kingdom|     12962|kathyphillips@yah...|        Cheryl Burns|       diazsharon|\n",
      "|809 Robert Plain ...|1978-07-26 19:48:26|United Kingdom|     12957|  steven78@gmail.com|     Reginald Wright|         nathan71|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.sort(col(\"CustomerID\").desc).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "99396b99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:40:31 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 20:40:31 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 20:40:31 INFO MemoryStore: Block broadcast_122 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:40:31 INFO MemoryStore: Block broadcast_122_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:40:31 INFO BlockManagerInfo: Added broadcast_122_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:40:31 INFO SparkContext: Created broadcast 122 from show at cell91.sc:1\n",
      "24/09/02 20:40:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 20:40:31 INFO SparkContext: Starting job: show at cell91.sc:1\n",
      "24/09/02 20:40:31 INFO DAGScheduler: Got job 72 (show at cell91.sc:1) with 1 output partitions\n",
      "24/09/02 20:40:31 INFO DAGScheduler: Final stage: ResultStage 78 (show at cell91.sc:1)\n",
      "24/09/02 20:40:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:40:31 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:40:31 INFO DAGScheduler: Submitting ResultStage 78 (MapPartitionsRDD[241] at show at cell91.sc:1), which has no missing parents\n",
      "24/09/02 20:40:31 INFO MemoryStore: Block broadcast_123 stored as values in memory (estimated size 14.3 KiB, free 4.5 GiB)\n",
      "24/09/02 20:40:31 INFO MemoryStore: Block broadcast_123_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:40:31 INFO BlockManagerInfo: Added broadcast_123_piece0 in memory on ubuntu:39321 (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:40:31 INFO SparkContext: Created broadcast 123 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:40:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 78 (MapPartitionsRDD[241] at show at cell91.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:40:31 INFO TaskSchedulerImpl: Adding task set 78.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:40:31 INFO TaskSetManager: Starting task 0.0 in stage 78.0 (TID 119) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/02 20:40:31 INFO Executor: Running task 0.0 in stage 78.0 (TID 119)\n",
      "24/09/02 20:40:31 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 20:40:31 INFO Executor: Finished task 0.0 in stage 78.0 (TID 119). 6119 bytes result sent to driver\n",
      "24/09/02 20:40:31 INFO TaskSetManager: Finished task 0.0 in stage 78.0 (TID 119) in 16 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:40:31 INFO TaskSchedulerImpl: Removed TaskSet 78.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:40:31 INFO DAGScheduler: ResultStage 78 (show at cell91.sc:1) finished in 0,023 s\n",
      "24/09/02 20:40:31 INFO DAGScheduler: Job 72 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:40:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 78: Stage finished\n",
      "24/09/02 20:40:31 INFO DAGScheduler: Job 72 finished: show at cell91.sc:1, took 0,034450 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "|             Address|          Birthdate|       Country|CustomerID|               Email|             Name|        Username|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "|Unit 1047 Box 408...|1994-02-20 00:46:27|United Kingdom|     12346|cooperalexis@hotm...|    Lindsay Cowan|valenciajennifer|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|  Katherine David|      hillrachel|\n",
      "|Unit 2676 Box 935...|1974-11-26 15:30:20|       Finland|     12348| tcrawford@gmail.com|  Leslie Martinez|    serranobrian|\n",
      "|2765 Powers Meado...|1977-05-06 23:57:35|         Italy|     12349|  dustin37@yahoo.com|    Brad Cardenas|   charleshudson|\n",
      "|17677 Mark Crest\\...|1996-09-13 19:14:27|        Norway|     12350|amyholland@yahoo.com|     Natalie Ford| gregoryharrison|\n",
      "|50047 Smith Point...|1969-06-21 03:39:20|        Norway|     12352| vcarter@hotmail.com|      Dana Clarke|          hmyers|\n",
      "|633 Miller Turnpi...|1993-02-25 18:37:29|       Bahrain|     12353|   laura34@yahoo.com|     Gary Nichols|  andrewhamilton|\n",
      "|38456 Rachael Cau...|1993-03-13 12:37:34|         Spain|     12354|   zmelton@gmail.com|       John Parks|      matthewray|\n",
      "|4140 Pamela Hollo...|1972-11-10 12:01:08|       Bahrain|     12355|   scott50@yahoo.com|Jennifer Lawrence|          glopez|\n",
      "|8681 Karen Roads ...|1973-01-13 17:17:26|      Portugal|     12356|josephmacias@hotm...|    James Sanchez|        wesley20|\n",
      "|18637 Jessica Rid...|1989-11-24 17:12:54|   Switzerland|     12357|michael16@hotmail...|     Ashley Lopez|     thomasdavid|\n",
      "|2129 Joel Rapids\\...|1977-06-19 22:35:52|       Austria|     12358|michaelespinoza@g...| Dr. Angela Brown|      patricia44|\n",
      "|86636 Maria Viadu...|1983-09-21 05:22:18|        Cyprus|     12359|  ryanpena@yahoo.com|        John Vega|     nelsonmaria|\n",
      "|1579 Young Trail\\...|1980-10-28 17:25:59|       Austria|     12360|briannafrost@yaho...|     Lauren Clark|   portermichael|\n",
      "|USNS Howard\\nFPO ...|1982-09-01 09:12:57|       Belgium|     12361|virginia36@hotmai...|Jacqueline Haynes|   johnsonshelly|\n",
      "|70092 Adams Prair...|1979-02-03 03:42:47|       Belgium|     12362|   april04@gmail.com|     Brian Flores|    hunterdaniel|\n",
      "|7322 Owens Inlet ...|1974-12-21 13:27:20|   Unspecified|     12363|   omolina@gmail.com|Christopher Gomez|         james75|\n",
      "|86176 Katherine C...|1990-07-17 15:47:12|       Belgium|     12364|barbaraduncan@gma...|     Robert Burns|          eric10|\n",
      "|932 Jeremy Spring...|1981-07-10 00:35:00|        Cyprus|     12365|nicoleanderson@ho...|    Joshua Parker|     millerrenee|\n",
      "|USNV Chavez\\nFPO ...|1989-12-26 00:58:01|       Denmark|     12367|   aaron99@yahoo.com|Christine Douglas|       michael58|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.orderBy(\"CustomerID\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdc9ec6",
   "metadata": {},
   "source": [
    "#### Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1910596b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:41:38 INFO BlockManagerInfo: Removed broadcast_121_piece0 on ubuntu:39321 in memory (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:41:38 INFO BlockManagerInfo: Removed broadcast_122_piece0 on ubuntu:39321 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:41:38 INFO BlockManagerInfo: Removed broadcast_120_piece0 on ubuntu:39321 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:41:38 INFO BlockManagerInfo: Removed broadcast_123_piece0 on ubuntu:39321 in memory (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:41:38 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 20:41:38 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 20:41:38 INFO MemoryStore: Block broadcast_124 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:41:38 INFO MemoryStore: Block broadcast_124_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:41:38 INFO BlockManagerInfo: Added broadcast_124_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:41:38 INFO SparkContext: Created broadcast 124 from rdd at cell92.sc:2\n",
      "24/09/02 20:41:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old num partitions: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:41:38 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 20:41:38 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 20:41:38 INFO MemoryStore: Block broadcast_125 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:41:38 INFO MemoryStore: Block broadcast_125_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:41:38 INFO BlockManagerInfo: Added broadcast_125_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:41:38 INFO SparkContext: Created broadcast 125 from rdd at cell92.sc:3\n",
      "24/09/02 20:41:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 20:41:38 INFO DAGScheduler: Registering RDD 249 (rdd at cell92.sc:3) as input to shuffle 8\n",
      "24/09/02 20:41:38 INFO DAGScheduler: Got map stage job 73 (rdd at cell92.sc:3) with 1 output partitions\n",
      "24/09/02 20:41:38 INFO DAGScheduler: Final stage: ShuffleMapStage 79 (rdd at cell92.sc:3)\n",
      "24/09/02 20:41:38 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:41:38 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:41:38 INFO DAGScheduler: Submitting ShuffleMapStage 79 (MapPartitionsRDD[249] at rdd at cell92.sc:3), which has no missing parents\n",
      "24/09/02 20:41:38 INFO MemoryStore: Block broadcast_126 stored as values in memory (estimated size 14.5 KiB, free 4.5 GiB)\n",
      "24/09/02 20:41:38 INFO MemoryStore: Block broadcast_126_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 4.5 GiB)\n",
      "24/09/02 20:41:38 INFO BlockManagerInfo: Added broadcast_126_piece0 in memory on ubuntu:39321 (size: 7.3 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:41:38 INFO SparkContext: Created broadcast 126 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:41:38 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 79 (MapPartitionsRDD[249] at rdd at cell92.sc:3) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:41:38 INFO TaskSchedulerImpl: Adding task set 79.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:41:38 INFO TaskSetManager: Starting task 0.0 in stage 79.0 (TID 120) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8235 bytes) \n",
      "24/09/02 20:41:38 INFO Executor: Running task 0.0 in stage 79.0 (TID 120)\n",
      "24/09/02 20:41:38 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 20:41:38 INFO Executor: Finished task 0.0 in stage 79.0 (TID 120). 1713 bytes result sent to driver\n",
      "24/09/02 20:41:38 INFO TaskSetManager: Finished task 0.0 in stage 79.0 (TID 120) in 29 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:41:38 INFO TaskSchedulerImpl: Removed TaskSet 79.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:41:38 INFO DAGScheduler: ShuffleMapStage 79 (rdd at cell92.sc:3) finished in 0,055 s\n",
      "24/09/02 20:41:38 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/02 20:41:38 INFO DAGScheduler: running: Set()\n",
      "24/09/02 20:41:38 INFO DAGScheduler: waiting: Set()\n",
      "24/09/02 20:41:38 INFO DAGScheduler: failed: Set()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New num partitions: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrepartitionedDf\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [Address: string, Birthdate: string ... 5 more fields]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val repartitionedDf = customerDf.repartition(5, col(\"Country\"))\n",
    "println(s\"Old num partitions: ${customerDf.rdd.getNumPartitions}\")\n",
    "println(s\"New num partitions: ${repartitionedDf.rdd.getNumPartitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "44b583d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:42:07 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 20:42:07 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 20:42:07 INFO MemoryStore: Block broadcast_127 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:42:07 INFO MemoryStore: Block broadcast_127_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:42:07 INFO BlockManagerInfo: Added broadcast_127_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:42:07 INFO SparkContext: Created broadcast 127 from rdd at cell93.sc:1\n",
      "24/09/02 20:42:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 20:42:07 INFO DAGScheduler: Registering RDD 256 (rdd at cell93.sc:1) as input to shuffle 9\n",
      "24/09/02 20:42:07 INFO DAGScheduler: Got map stage job 74 (rdd at cell93.sc:1) with 1 output partitions\n",
      "24/09/02 20:42:07 INFO DAGScheduler: Final stage: ShuffleMapStage 80 (rdd at cell93.sc:1)\n",
      "24/09/02 20:42:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:42:07 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:42:07 INFO DAGScheduler: Submitting ShuffleMapStage 80 (MapPartitionsRDD[256] at rdd at cell93.sc:1), which has no missing parents\n",
      "24/09/02 20:42:07 INFO MemoryStore: Block broadcast_128 stored as values in memory (estimated size 14.5 KiB, free 4.5 GiB)\n",
      "24/09/02 20:42:07 INFO MemoryStore: Block broadcast_128_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 4.5 GiB)\n",
      "24/09/02 20:42:07 INFO BlockManagerInfo: Added broadcast_128_piece0 in memory on ubuntu:39321 (size: 7.3 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:42:07 INFO SparkContext: Created broadcast 128 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:42:07 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 80 (MapPartitionsRDD[256] at rdd at cell93.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:42:07 INFO TaskSchedulerImpl: Adding task set 80.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:42:07 INFO TaskSetManager: Starting task 0.0 in stage 80.0 (TID 121) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8235 bytes) \n",
      "24/09/02 20:42:07 INFO Executor: Running task 0.0 in stage 80.0 (TID 121)\n",
      "24/09/02 20:42:07 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 20:42:07 INFO Executor: Finished task 0.0 in stage 80.0 (TID 121). 1713 bytes result sent to driver\n",
      "24/09/02 20:42:07 INFO TaskSetManager: Finished task 0.0 in stage 80.0 (TID 121) in 76 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:42:07 INFO TaskSchedulerImpl: Removed TaskSet 80.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:42:07 INFO DAGScheduler: ShuffleMapStage 80 (rdd at cell93.sc:1) finished in 0,089 s\n",
      "24/09/02 20:42:07 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/02 20:42:07 INFO DAGScheduler: running: Set()\n",
      "24/09/02 20:42:07 INFO DAGScheduler: waiting: Set()\n",
      "24/09/02 20:42:07 INFO DAGScheduler: failed: Set()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num partitions after coalesce: 1\n"
     ]
    }
   ],
   "source": [
    "println(s\"Num partitions after coalesce: ${repartitionedDf.coalesce(1).rdd.getNumPartitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97975bbd",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e35c3a43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:42:50 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 20:42:50 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 20:42:50 INFO MemoryStore: Block broadcast_129 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:42:50 INFO MemoryStore: Block broadcast_129_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:42:50 INFO BlockManagerInfo: Added broadcast_129_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:42:50 INFO SparkContext: Created broadcast 129 from show at cell94.sc:2\n",
      "24/09/02 20:42:50 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 20:42:50 INFO SparkContext: Starting job: show at cell94.sc:2\n",
      "24/09/02 20:42:50 INFO DAGScheduler: Got job 75 (show at cell94.sc:2) with 1 output partitions\n",
      "24/09/02 20:42:50 INFO DAGScheduler: Final stage: ResultStage 81 (show at cell94.sc:2)\n",
      "24/09/02 20:42:50 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:42:50 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:42:50 INFO DAGScheduler: Submitting ResultStage 81 (MapPartitionsRDD[265] at show at cell94.sc:2), which has no missing parents\n",
      "24/09/02 20:42:50 INFO MemoryStore: Block broadcast_130 stored as values in memory (estimated size 16.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:42:50 INFO MemoryStore: Block broadcast_130_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 4.5 GiB)\n",
      "24/09/02 20:42:50 INFO BlockManagerInfo: Added broadcast_130_piece0 in memory on ubuntu:39321 (size: 7.7 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:42:50 INFO SparkContext: Created broadcast 130 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:42:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 81 (MapPartitionsRDD[265] at show at cell94.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:42:50 INFO TaskSchedulerImpl: Adding task set 81.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:42:50 INFO TaskSetManager: Starting task 0.0 in stage 81.0 (TID 122) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/02 20:42:50 INFO Executor: Running task 0.0 in stage 81.0 (TID 122)\n",
      "24/09/02 20:42:50 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 20:42:50 INFO Executor: Finished task 0.0 in stage 81.0 (TID 122). 1705 bytes result sent to driver\n",
      "24/09/02 20:42:50 INFO TaskSetManager: Finished task 0.0 in stage 81.0 (TID 122) in 14 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:42:50 INFO TaskSchedulerImpl: Removed TaskSet 81.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:42:50 INFO DAGScheduler: ResultStage 81 (show at cell94.sc:2) finished in 0,022 s\n",
      "24/09/02 20:42:50 INFO DAGScheduler: Job 75 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:42:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 81: Stage finished\n",
      "24/09/02 20:42:50 INFO DAGScheduler: Job 75 finished: show at cell94.sc:2, took 0,033236 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|          Birthdate|        bd|\n",
      "+-------------------+----------+\n",
      "|1994-02-20 00:46:27|1994-02-20|\n",
      "|1988-06-21 00:15:34|1988-06-21|\n",
      "|1974-11-26 15:30:20|1974-11-26|\n",
      "|1977-05-06 23:57:35|1977-05-06|\n",
      "|1996-09-13 19:14:27|1996-09-13|\n",
      "+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdatDf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Birthdate: string, bd: string]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val datDf = customerDf.select($\"Birthdate\", date_format(col(\"Birthdate\"), \"yyyy-MM-dd\").alias(\"bd\"))\n",
    "datDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "74467040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Birthdate: string (nullable = true)\n",
      " |-- bd: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datDf.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c1d84b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Birthdate: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Username: string (nullable = true)\n",
      " |-- Identity: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.withColumn(\"Identity\", array($\"Name\", $\"Username\")).printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5d4294e1-47ff-4300-8514-b2930b8dd9ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:44:39 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 20:44:39 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 20:44:39 INFO MemoryStore: Block broadcast_131 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:44:39 INFO MemoryStore: Block broadcast_131_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:44:39 INFO BlockManagerInfo: Added broadcast_131_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:44:39 INFO SparkContext: Created broadcast 131 from show at cell97.sc:4\n",
      "24/09/02 20:44:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 20:44:39 INFO SparkContext: Starting job: show at cell97.sc:4\n",
      "24/09/02 20:44:39 INFO DAGScheduler: Got job 76 (show at cell97.sc:4) with 1 output partitions\n",
      "24/09/02 20:44:39 INFO DAGScheduler: Final stage: ResultStage 82 (show at cell97.sc:4)\n",
      "24/09/02 20:44:39 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:44:39 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:44:39 INFO DAGScheduler: Submitting ResultStage 82 (MapPartitionsRDD[269] at show at cell97.sc:4), which has no missing parents\n",
      "24/09/02 20:44:39 INFO MemoryStore: Block broadcast_132 stored as values in memory (estimated size 16.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:44:39 INFO MemoryStore: Block broadcast_132_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 4.5 GiB)\n",
      "24/09/02 20:44:39 INFO BlockManagerInfo: Added broadcast_132_piece0 in memory on ubuntu:39321 (size: 7.7 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:44:39 INFO SparkContext: Created broadcast 132 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:44:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 82 (MapPartitionsRDD[269] at show at cell97.sc:4) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:44:39 INFO TaskSchedulerImpl: Adding task set 82.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:44:39 INFO TaskSetManager: Starting task 0.0 in stage 82.0 (TID 123) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/02 20:44:39 INFO Executor: Running task 0.0 in stage 82.0 (TID 123)\n",
      "24/09/02 20:44:39 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 20:44:39 INFO Executor: Finished task 0.0 in stage 82.0 (TID 123). 1875 bytes result sent to driver\n",
      "24/09/02 20:44:39 INFO TaskSetManager: Finished task 0.0 in stage 82.0 (TID 123) in 14 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:44:39 INFO TaskSchedulerImpl: Removed TaskSet 82.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:44:39 INFO DAGScheduler: ResultStage 82 (show at cell97.sc:4) finished in 0,027 s\n",
      "24/09/02 20:44:39 INFO DAGScheduler: Job 76 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:44:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 82: Stage finished\n",
      "24/09/02 20:44:39 INFO DAGScheduler: Job 76 finished: show at cell97.sc:4, took 0,036598 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+---------------------------------+\n",
      "|Name           |Username        |Identity                         |\n",
      "+---------------+----------------+---------------------------------+\n",
      "|Lindsay Cowan  |valenciajennifer|[Lindsay Cowan, valenciajennifer]|\n",
      "|Katherine David|hillrachel      |[Katherine David, hillrachel]    |\n",
      "|Leslie Martinez|serranobrian    |[Leslie Martinez, serranobrian]  |\n",
      "|Brad Cardenas  |charleshudson   |[Brad Cardenas, charleshudson]   |\n",
      "|Natalie Ford   |gregoryharrison |[Natalie Ford, gregoryharrison]  |\n",
      "+---------------+----------------+---------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf\n",
    "    .withColumn(\"Identity\", array($\"Name\", $\"Username\"))\n",
    "    .select(\"Name\", \"Username\", \"Identity\")\n",
    "    .show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b17c1f-3197-43bd-b043-a96f256bff3c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3 Агрегирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "97bfc928-6e23-4c15-b3d3-e9b314fc23e2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:45:34 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 20:45:34 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 20:45:34 INFO MemoryStore: Block broadcast_136 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:45:34 INFO MemoryStore: Block broadcast_136_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:45:34 INFO BlockManagerInfo: Added broadcast_136_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:45:34 INFO SparkContext: Created broadcast 136 from show at cell99.sc:1\n",
      "24/09/02 20:45:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 20:45:34 INFO DAGScheduler: Registering RDD 280 (show at cell99.sc:1) as input to shuffle 11\n",
      "24/09/02 20:45:34 INFO DAGScheduler: Got map stage job 79 (show at cell99.sc:1) with 1 output partitions\n",
      "24/09/02 20:45:34 INFO DAGScheduler: Final stage: ShuffleMapStage 86 (show at cell99.sc:1)\n",
      "24/09/02 20:45:34 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:45:34 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:45:34 INFO DAGScheduler: Submitting ShuffleMapStage 86 (MapPartitionsRDD[280] at show at cell99.sc:1), which has no missing parents\n",
      "24/09/02 20:45:34 INFO MemoryStore: Block broadcast_137 stored as values in memory (estimated size 38.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:45:34 INFO MemoryStore: Block broadcast_137_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 4.5 GiB)\n",
      "24/09/02 20:45:34 INFO BlockManagerInfo: Added broadcast_137_piece0 in memory on ubuntu:39321 (size: 17.9 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:45:34 INFO SparkContext: Created broadcast 137 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:45:34 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 86 (MapPartitionsRDD[280] at show at cell99.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:45:34 INFO TaskSchedulerImpl: Adding task set 86.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:45:34 INFO TaskSetManager: Starting task 0.0 in stage 86.0 (TID 126) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8235 bytes) \n",
      "24/09/02 20:45:34 INFO Executor: Running task 0.0 in stage 86.0 (TID 126)\n",
      "24/09/02 20:45:34 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 20:45:34 INFO Executor: Finished task 0.0 in stage 86.0 (TID 126). 2716 bytes result sent to driver\n",
      "24/09/02 20:45:34 INFO TaskSetManager: Finished task 0.0 in stage 86.0 (TID 126) in 29 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:45:34 INFO DAGScheduler: ShuffleMapStage 86 (show at cell99.sc:1) finished in 0,038 s\n",
      "24/09/02 20:45:34 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/02 20:45:34 INFO DAGScheduler: running: Set()\n",
      "24/09/02 20:45:34 INFO DAGScheduler: waiting: Set()\n",
      "24/09/02 20:45:34 INFO TaskSchedulerImpl: Removed TaskSet 86.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:45:34 INFO DAGScheduler: failed: Set()\n",
      "24/09/02 20:45:34 INFO ShufflePartitionsUtil: For shuffle(11), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/09/02 20:45:34 INFO CodeGenerator: Code generated in 18.802446 ms\n",
      "24/09/02 20:45:34 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/09/02 20:45:34 INFO CodeGenerator: Code generated in 27.706083 ms\n",
      "24/09/02 20:45:34 INFO SparkContext: Starting job: show at cell99.sc:1\n",
      "24/09/02 20:45:34 INFO DAGScheduler: Got job 80 (show at cell99.sc:1) with 1 output partitions\n",
      "24/09/02 20:45:34 INFO DAGScheduler: Final stage: ResultStage 88 (show at cell99.sc:1)\n",
      "24/09/02 20:45:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 87)\n",
      "24/09/02 20:45:34 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:45:34 INFO DAGScheduler: Submitting ResultStage 88 (MapPartitionsRDD[284] at show at cell99.sc:1), which has no missing parents\n",
      "24/09/02 20:45:34 INFO MemoryStore: Block broadcast_138 stored as values in memory (estimated size 40.9 KiB, free 4.5 GiB)\n",
      "24/09/02 20:45:34 INFO MemoryStore: Block broadcast_138_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:45:34 INFO BlockManagerInfo: Added broadcast_138_piece0 in memory on ubuntu:39321 (size: 19.4 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:45:34 INFO SparkContext: Created broadcast 138 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:45:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 88 (MapPartitionsRDD[284] at show at cell99.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:45:34 INFO TaskSchedulerImpl: Adding task set 88.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:45:34 INFO TaskSetManager: Starting task 0.0 in stage 88.0 (TID 127) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/02 20:45:34 INFO Executor: Running task 0.0 in stage 88.0 (TID 127)\n",
      "24/09/02 20:45:34 INFO CodeGenerator: Code generated in 6.112604 ms\n",
      "24/09/02 20:45:34 INFO ShuffleBlockFetcherIterator: Getting 1 (2.2 KiB) non-empty blocks including 1 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/02 20:45:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/09/02 20:45:34 INFO CodeGenerator: Code generated in 9.996044 ms\n",
      "24/09/02 20:45:34 INFO Executor: Finished task 0.0 in stage 88.0 (TID 127). 6167 bytes result sent to driver\n",
      "24/09/02 20:45:34 INFO TaskSetManager: Finished task 0.0 in stage 88.0 (TID 127) in 41 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:45:34 INFO TaskSchedulerImpl: Removed TaskSet 88.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:45:34 INFO DAGScheduler: ResultStage 88 (show at cell99.sc:1) finished in 0,061 s\n",
      "24/09/02 20:45:34 INFO DAGScheduler: Job 80 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:45:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 88: Stage finished\n",
      "24/09/02 20:45:34 INFO DAGScheduler: Job 80 finished: show at cell99.sc:1, took 0,069688 s\n",
      "24/09/02 20:45:34 INFO CodeGenerator: Code generated in 12.757441 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             Country|count|\n",
      "+--------------------+-----+\n",
      "|           Singapore|    1|\n",
      "|                 RSA|    1|\n",
      "|             Iceland|    1|\n",
      "|        Saudi Arabia|    1|\n",
      "|United Arab Emirates|    1|\n",
      "|      Czech Republic|    1|\n",
      "|              Brazil|    1|\n",
      "|             Lebanon|    1|\n",
      "|              Greece|    2|\n",
      "|         Unspecified|    2|\n",
      "|             Bahrain|    2|\n",
      "|              Israel|    4|\n",
      "|                 USA|    4|\n",
      "|              Poland|    6|\n",
      "|              Sweden|    7|\n",
      "|              Cyprus|    7|\n",
      "|             Denmark|    8|\n",
      "|               Japan|    8|\n",
      "|           Australia|    8|\n",
      "|         Netherlands|    8|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.groupBy(\"Country\").count().orderBy(\"count\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8e740a2d-6fcc-4a8c-b2bf-3854475701c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:48:04 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 20:48:04 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 20:48:04 INFO MemoryStore: Block broadcast_145 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:48:04 INFO MemoryStore: Block broadcast_145_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:48:04 INFO BlockManagerInfo: Added broadcast_145_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:48:04 INFO SparkContext: Created broadcast 145 from show at cell102.sc:4\n",
      "24/09/02 20:48:04 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 20:48:04 INFO DAGScheduler: Registering RDD 309 (show at cell102.sc:4) as input to shuffle 14\n",
      "24/09/02 20:48:04 INFO DAGScheduler: Got map stage job 85 (show at cell102.sc:4) with 1 output partitions\n",
      "24/09/02 20:48:04 INFO DAGScheduler: Final stage: ShuffleMapStage 95 (show at cell102.sc:4)\n",
      "24/09/02 20:48:04 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:48:04 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:48:04 INFO DAGScheduler: Submitting ShuffleMapStage 95 (MapPartitionsRDD[309] at show at cell102.sc:4), which has no missing parents\n",
      "24/09/02 20:48:04 INFO MemoryStore: Block broadcast_146 stored as values in memory (estimated size 34.6 KiB, free 4.5 GiB)\n",
      "24/09/02 20:48:04 INFO MemoryStore: Block broadcast_146_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:48:04 INFO BlockManagerInfo: Added broadcast_146_piece0 in memory on ubuntu:39321 (size: 16.4 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:48:04 INFO SparkContext: Created broadcast 146 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:48:04 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 95 (MapPartitionsRDD[309] at show at cell102.sc:4) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:48:04 INFO TaskSchedulerImpl: Adding task set 95.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:48:04 INFO TaskSetManager: Starting task 0.0 in stage 95.0 (TID 132) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8235 bytes) \n",
      "24/09/02 20:48:04 INFO Executor: Running task 0.0 in stage 95.0 (TID 132)\n",
      "24/09/02 20:48:04 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 20:48:04 INFO Executor: Finished task 0.0 in stage 95.0 (TID 132). 2523 bytes result sent to driver\n",
      "24/09/02 20:48:04 INFO TaskSetManager: Finished task 0.0 in stage 95.0 (TID 132) in 65 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:48:04 INFO TaskSchedulerImpl: Removed TaskSet 95.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:48:04 INFO DAGScheduler: ShuffleMapStage 95 (show at cell102.sc:4) finished in 0,086 s\n",
      "24/09/02 20:48:04 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/02 20:48:04 INFO DAGScheduler: running: Set()\n",
      "24/09/02 20:48:04 INFO DAGScheduler: waiting: Set()\n",
      "24/09/02 20:48:04 INFO DAGScheduler: failed: Set()\n",
      "24/09/02 20:48:04 INFO ShufflePartitionsUtil: For shuffle(14), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/09/02 20:48:04 INFO SparkContext: Starting job: show at cell102.sc:4\n",
      "24/09/02 20:48:04 INFO DAGScheduler: Got job 86 (show at cell102.sc:4) with 1 output partitions\n",
      "24/09/02 20:48:04 INFO DAGScheduler: Final stage: ResultStage 97 (show at cell102.sc:4)\n",
      "24/09/02 20:48:04 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 96)\n",
      "24/09/02 20:48:04 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:48:04 INFO DAGScheduler: Submitting ResultStage 97 (MapPartitionsRDD[314] at show at cell102.sc:4), which has no missing parents\n",
      "24/09/02 20:48:04 INFO MemoryStore: Block broadcast_147 stored as values in memory (estimated size 43.6 KiB, free 4.5 GiB)\n",
      "24/09/02 20:48:04 INFO MemoryStore: Block broadcast_147_piece0 stored as bytes in memory (estimated size 20.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:48:04 INFO BlockManagerInfo: Added broadcast_147_piece0 in memory on ubuntu:39321 (size: 20.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:48:04 INFO SparkContext: Created broadcast 147 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:48:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 97 (MapPartitionsRDD[314] at show at cell102.sc:4) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:48:04 INFO TaskSchedulerImpl: Adding task set 97.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:48:04 INFO TaskSetManager: Starting task 0.0 in stage 97.0 (TID 133) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/02 20:48:04 INFO Executor: Running task 0.0 in stage 97.0 (TID 133)\n",
      "24/09/02 20:48:04 INFO ShuffleBlockFetcherIterator: Getting 1 (36.1 KiB) non-empty blocks including 1 (36.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/02 20:48:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\n",
      "24/09/02 20:48:04 INFO Executor: Finished task 0.0 in stage 97.0 (TID 133). 7995 bytes result sent to driver\n",
      "24/09/02 20:48:04 INFO TaskSetManager: Finished task 0.0 in stage 97.0 (TID 133) in 30 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:48:04 INFO TaskSchedulerImpl: Removed TaskSet 97.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:48:04 INFO DAGScheduler: ResultStage 97 (show at cell102.sc:4) finished in 0,037 s\n",
      "24/09/02 20:48:04 INFO DAGScheduler: Job 86 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:48:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 97: Stage finished\n",
      "24/09/02 20:48:04 INFO DAGScheduler: Job 86 finished: show at cell102.sc:4, took 0,056116 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------------+---------------+\n",
      "|  Country|        bd|min(CustomerID)|max(CustomerID)|\n",
      "+---------+----------+---------------+---------------+\n",
      "|Australia|1966-09-17|          12422|          12422|\n",
      "|Australia|1967-11-29|          12424|          12424|\n",
      "|Australia|1985-12-30|          12386|          12386|\n",
      "|Australia|1986-08-28|          12434|          12434|\n",
      "|Australia|1987-02-26|          12393|          12393|\n",
      "|Australia|1989-08-28|          12415|          12415|\n",
      "|Australia|1993-01-31|          12431|          12431|\n",
      "|Australia|1993-04-02|          12388|          12388|\n",
      "|  Austria|1970-08-06|          12865|          12865|\n",
      "|  Austria|1973-07-16|          12818|          12818|\n",
      "|  Austria|1973-12-21|          12370|          12370|\n",
      "|  Austria|1976-06-13|          12374|          12374|\n",
      "|  Austria|1976-09-03|          12373|          12373|\n",
      "|  Austria|1977-06-19|          12358|          12358|\n",
      "|  Austria|1977-11-30|          12453|          12453|\n",
      "|  Austria|1980-10-28|          12360|          12360|\n",
      "|  Austria|1982-07-01|          12414|          12414|\n",
      "|  Austria|1983-02-24|          12429|          12429|\n",
      "|  Austria|1996-03-28|          12817|          12817|\n",
      "|  Bahrain|1972-11-10|          12355|          12355|\n",
      "+---------+----------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf\n",
    "    .groupBy($\"Country\", date_format(col(\"Birthdate\"), \"yyyy-MM-dd\").alias(\"bd\"))\n",
    "    .agg(min(\"CustomerID\"), max(\"CustomerID\"))\n",
    "    .orderBy(\"Country\")\n",
    "    .show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd0b5c-0c8e-4d5f-8b32-27c6ef1793b9",
   "metadata": {},
   "source": [
    "## 4 Операции над несколькими датафреймами"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9d6226",
   "metadata": {},
   "source": [
    "### Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "803b493e-9fdf-4e4a-b5ae-ed4f3d5f242c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:58:36 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 20:58:36 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 20:58:36 INFO MemoryStore: Block broadcast_148 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:58:36 INFO MemoryStore: Block broadcast_148_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:58:36 INFO BlockManagerInfo: Added broadcast_148_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:58:36 INFO SparkContext: Created broadcast 148 from count at cell103.sc:1\n",
      "24/09/02 20:58:36 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 20:58:36 INFO DAGScheduler: Registering RDD 318 (count at cell103.sc:1) as input to shuffle 15\n",
      "24/09/02 20:58:36 INFO DAGScheduler: Got map stage job 87 (count at cell103.sc:1) with 1 output partitions\n",
      "24/09/02 20:58:36 INFO DAGScheduler: Final stage: ShuffleMapStage 98 (count at cell103.sc:1)\n",
      "24/09/02 20:58:36 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:58:36 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:58:36 INFO DAGScheduler: Submitting ShuffleMapStage 98 (MapPartitionsRDD[318] at count at cell103.sc:1), which has no missing parents\n",
      "24/09/02 20:58:36 INFO MemoryStore: Block broadcast_149 stored as values in memory (estimated size 17.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:58:36 INFO MemoryStore: Block broadcast_149_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 4.5 GiB)\n",
      "24/09/02 20:58:36 INFO BlockManagerInfo: Added broadcast_149_piece0 in memory on ubuntu:39321 (size: 8.3 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:58:36 INFO SparkContext: Created broadcast 149 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:58:36 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 98 (MapPartitionsRDD[318] at count at cell103.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:58:36 INFO TaskSchedulerImpl: Adding task set 98.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:58:36 INFO TaskSetManager: Starting task 0.0 in stage 98.0 (TID 134) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8235 bytes) \n",
      "24/09/02 20:58:36 INFO Executor: Running task 0.0 in stage 98.0 (TID 134)\n",
      "24/09/02 20:58:36 INFO CodeGenerator: Code generated in 12.225391 ms\n",
      "24/09/02 20:58:36 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 20:58:36 INFO Executor: Finished task 0.0 in stage 98.0 (TID 134). 1922 bytes result sent to driver\n",
      "24/09/02 20:58:36 INFO TaskSetManager: Finished task 0.0 in stage 98.0 (TID 134) in 35 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:58:36 INFO TaskSchedulerImpl: Removed TaskSet 98.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:58:36 INFO DAGScheduler: ShuffleMapStage 98 (count at cell103.sc:1) finished in 0,048 s\n",
      "24/09/02 20:58:36 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/02 20:58:36 INFO DAGScheduler: running: Set()\n",
      "24/09/02 20:58:36 INFO DAGScheduler: waiting: Set()\n",
      "24/09/02 20:58:36 INFO DAGScheduler: failed: Set()\n",
      "24/09/02 20:58:37 INFO SparkContext: Starting job: count at cell103.sc:1\n",
      "24/09/02 20:58:37 INFO DAGScheduler: Got job 88 (count at cell103.sc:1) with 1 output partitions\n",
      "24/09/02 20:58:37 INFO DAGScheduler: Final stage: ResultStage 100 (count at cell103.sc:1)\n",
      "24/09/02 20:58:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 99)\n",
      "24/09/02 20:58:37 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:58:37 INFO DAGScheduler: Submitting ResultStage 100 (MapPartitionsRDD[321] at count at cell103.sc:1), which has no missing parents\n",
      "24/09/02 20:58:37 INFO MemoryStore: Block broadcast_150 stored as values in memory (estimated size 12.5 KiB, free 4.5 GiB)\n",
      "24/09/02 20:58:37 INFO MemoryStore: Block broadcast_150_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:58:37 INFO BlockManagerInfo: Added broadcast_150_piece0 in memory on ubuntu:39321 (size: 6.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:58:37 INFO SparkContext: Created broadcast 150 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:58:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 100 (MapPartitionsRDD[321] at count at cell103.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:58:37 INFO TaskSchedulerImpl: Adding task set 100.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:58:37 INFO TaskSetManager: Starting task 0.0 in stage 100.0 (TID 135) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/02 20:58:37 INFO Executor: Running task 0.0 in stage 100.0 (TID 135)\n",
      "24/09/02 20:58:37 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/02 20:58:37 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/09/02 20:58:37 INFO CodeGenerator: Code generated in 8.133467 ms\n",
      "24/09/02 20:58:37 INFO Executor: Finished task 0.0 in stage 100.0 (TID 135). 3995 bytes result sent to driver\n",
      "24/09/02 20:58:37 INFO TaskSetManager: Finished task 0.0 in stage 100.0 (TID 135) in 30 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:58:37 INFO TaskSchedulerImpl: Removed TaskSet 100.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:58:37 INFO DAGScheduler: ResultStage 100 (count at cell103.sc:1) finished in 0,042 s\n",
      "24/09/02 20:58:37 INFO DAGScheduler: Job 88 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:58:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 100: Stage finished\n",
      "24/09/02 20:58:37 INFO DAGScheduler: Job 88 finished: count at cell103.sc:1, took 0,056672 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres103\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m507L\u001b[39m"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerDf.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "37cd41a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 21:00:48 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 21:00:48 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 21:00:48 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 21:00:48 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 21:00:48 INFO MemoryStore: Block broadcast_163 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 21:00:48 INFO BlockManagerInfo: Removed broadcast_162_piece0 on ubuntu:39321 in memory (size: 5.9 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:00:48 INFO BlockManagerInfo: Removed broadcast_158_piece0 on ubuntu:39321 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:00:48 INFO MemoryStore: Block broadcast_163_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 21:00:48 INFO BlockManagerInfo: Added broadcast_163_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:00:48 INFO SparkContext: Created broadcast 163 from count at cell107.sc:1\n",
      "24/09/02 21:00:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 21:00:48 INFO BlockManagerInfo: Removed broadcast_159_piece0 on ubuntu:39321 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:00:48 INFO MemoryStore: Block broadcast_164 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 21:00:48 INFO BlockManagerInfo: Removed broadcast_161_piece0 on ubuntu:39321 in memory (size: 9.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:00:48 INFO MemoryStore: Block broadcast_164_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 21:00:48 INFO BlockManagerInfo: Added broadcast_164_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:00:48 INFO SparkContext: Created broadcast 164 from count at cell107.sc:1\n",
      "24/09/02 21:00:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 21:00:48 INFO BlockManagerInfo: Removed broadcast_160_piece0 on ubuntu:39321 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:00:48 INFO DAGScheduler: Registering RDD 357 (count at cell107.sc:1) as input to shuffle 19\n",
      "24/09/02 21:00:48 INFO DAGScheduler: Got map stage job 95 (count at cell107.sc:1) with 2 output partitions\n",
      "24/09/02 21:00:48 INFO DAGScheduler: Final stage: ShuffleMapStage 110 (count at cell107.sc:1)\n",
      "24/09/02 21:00:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 21:00:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 21:00:48 INFO DAGScheduler: Submitting ShuffleMapStage 110 (MapPartitionsRDD[357] at count at cell107.sc:1), which has no missing parents\n",
      "24/09/02 21:00:48 INFO MemoryStore: Block broadcast_165 stored as values in memory (estimated size 18.1 KiB, free 4.5 GiB)\n",
      "24/09/02 21:00:48 INFO MemoryStore: Block broadcast_165_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 4.5 GiB)\n",
      "24/09/02 21:00:48 INFO BlockManagerInfo: Added broadcast_165_piece0 in memory on ubuntu:39321 (size: 8.9 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:00:48 INFO SparkContext: Created broadcast 165 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 21:00:48 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 110 (MapPartitionsRDD[357] at count at cell107.sc:1) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/09/02 21:00:48 INFO TaskSchedulerImpl: Adding task set 110.0 with 2 tasks resource profile 0\n",
      "24/09/02 21:00:48 INFO TaskSetManager: Starting task 0.0 in stage 110.0 (TID 145) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8344 bytes) \n",
      "24/09/02 21:00:48 INFO TaskSetManager: Starting task 1.0 in stage 110.0 (TID 146) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 8344 bytes) \n",
      "24/09/02 21:00:48 INFO Executor: Running task 0.0 in stage 110.0 (TID 145)\n",
      "24/09/02 21:00:48 INFO Executor: Running task 1.0 in stage 110.0 (TID 146)\n",
      "24/09/02 21:00:48 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 21:00:48 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 21:00:48 INFO Executor: Finished task 1.0 in stage 110.0 (TID 146). 1978 bytes result sent to driver\n",
      "24/09/02 21:00:48 INFO Executor: Finished task 0.0 in stage 110.0 (TID 145). 1978 bytes result sent to driver\n",
      "24/09/02 21:00:48 INFO TaskSetManager: Finished task 1.0 in stage 110.0 (TID 146) in 17 ms on ubuntu (executor driver) (1/2)\n",
      "24/09/02 21:00:48 INFO TaskSetManager: Finished task 0.0 in stage 110.0 (TID 145) in 18 ms on ubuntu (executor driver) (2/2)\n",
      "24/09/02 21:00:48 INFO TaskSchedulerImpl: Removed TaskSet 110.0, whose tasks have all completed, from pool \n",
      "24/09/02 21:00:48 INFO DAGScheduler: ShuffleMapStage 110 (count at cell107.sc:1) finished in 0,027 s\n",
      "24/09/02 21:00:48 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/02 21:00:48 INFO DAGScheduler: running: Set()\n",
      "24/09/02 21:00:48 INFO DAGScheduler: waiting: Set()\n",
      "24/09/02 21:00:48 INFO DAGScheduler: failed: Set()\n",
      "24/09/02 21:00:48 INFO SparkContext: Starting job: count at cell107.sc:1\n",
      "24/09/02 21:00:48 INFO DAGScheduler: Got job 96 (count at cell107.sc:1) with 1 output partitions\n",
      "24/09/02 21:00:48 INFO DAGScheduler: Final stage: ResultStage 112 (count at cell107.sc:1)\n",
      "24/09/02 21:00:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 111)\n",
      "24/09/02 21:00:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 21:00:48 INFO DAGScheduler: Submitting ResultStage 112 (MapPartitionsRDD[360] at count at cell107.sc:1), which has no missing parents\n",
      "24/09/02 21:00:48 INFO MemoryStore: Block broadcast_166 stored as values in memory (estimated size 12.5 KiB, free 4.5 GiB)\n",
      "24/09/02 21:00:48 INFO MemoryStore: Block broadcast_166_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 4.5 GiB)\n",
      "24/09/02 21:00:48 INFO BlockManagerInfo: Added broadcast_166_piece0 in memory on ubuntu:39321 (size: 5.9 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:00:48 INFO SparkContext: Created broadcast 166 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 21:00:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 112 (MapPartitionsRDD[360] at count at cell107.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 21:00:48 INFO TaskSchedulerImpl: Adding task set 112.0 with 1 tasks resource profile 0\n",
      "24/09/02 21:00:48 INFO TaskSetManager: Starting task 0.0 in stage 112.0 (TID 147) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/02 21:00:48 INFO Executor: Running task 0.0 in stage 112.0 (TID 147)\n",
      "24/09/02 21:00:48 INFO ShuffleBlockFetcherIterator: Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/02 21:00:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "24/09/02 21:00:48 INFO Executor: Finished task 0.0 in stage 112.0 (TID 147). 3995 bytes result sent to driver\n",
      "24/09/02 21:00:48 INFO TaskSetManager: Finished task 0.0 in stage 112.0 (TID 147) in 18 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 21:00:48 INFO TaskSchedulerImpl: Removed TaskSet 112.0, whose tasks have all completed, from pool \n",
      "24/09/02 21:00:48 INFO DAGScheduler: ResultStage 112 (count at cell107.sc:1) finished in 0,027 s\n",
      "24/09/02 21:00:48 INFO DAGScheduler: Job 96 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 21:00:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 112: Stage finished\n",
      "24/09/02 21:00:48 INFO DAGScheduler: Job 96 finished: count at cell107.sc:1, took 0,039849 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres107\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m1014L\u001b[39m"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerDf.union(customerDf).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c2ca4043-76ae-462d-b676-b1e4d2dfebb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 21:00:22 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 21:00:22 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 21:00:22 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 21:00:22 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 21:00:22 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 21:00:22 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 21:00:22 INFO MemoryStore: Block broadcast_158 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 21:00:22 INFO BlockManagerInfo: Removed broadcast_155_piece0 on ubuntu:39321 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:00:22 INFO BlockManagerInfo: Removed broadcast_157_piece0 on ubuntu:39321 in memory (size: 5.9 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:00:22 INFO MemoryStore: Block broadcast_158_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 21:00:22 INFO BlockManagerInfo: Added broadcast_158_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:00:22 INFO SparkContext: Created broadcast 158 from count at cell106.sc:1\n",
      "24/09/02 21:00:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 21:00:22 INFO BlockManagerInfo: Removed broadcast_156_piece0 on ubuntu:39321 in memory (size: 8.3 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:00:22 INFO MemoryStore: Block broadcast_159 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 21:00:22 INFO MemoryStore: Block broadcast_159_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 21:00:22 INFO BlockManagerInfo: Added broadcast_159_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:00:22 INFO SparkContext: Created broadcast 159 from count at cell106.sc:1\n",
      "24/09/02 21:00:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 21:00:22 INFO MemoryStore: Block broadcast_160 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 21:00:22 INFO MemoryStore: Block broadcast_160_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 21:00:22 INFO BlockManagerInfo: Added broadcast_160_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:00:22 INFO SparkContext: Created broadcast 160 from count at cell106.sc:1\n",
      "24/09/02 21:00:22 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 21:00:22 INFO DAGScheduler: Registering RDD 347 (count at cell106.sc:1) as input to shuffle 18\n",
      "24/09/02 21:00:22 INFO DAGScheduler: Got map stage job 93 (count at cell106.sc:1) with 3 output partitions\n",
      "24/09/02 21:00:22 INFO DAGScheduler: Final stage: ShuffleMapStage 107 (count at cell106.sc:1)\n",
      "24/09/02 21:00:22 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 21:00:22 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 21:00:22 INFO DAGScheduler: Submitting ShuffleMapStage 107 (MapPartitionsRDD[347] at count at cell106.sc:1), which has no missing parents\n",
      "24/09/02 21:00:22 INFO MemoryStore: Block broadcast_161 stored as values in memory (estimated size 18.9 KiB, free 4.5 GiB)\n",
      "24/09/02 21:00:22 INFO MemoryStore: Block broadcast_161_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 4.5 GiB)\n",
      "24/09/02 21:00:22 INFO BlockManagerInfo: Added broadcast_161_piece0 in memory on ubuntu:39321 (size: 9.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:00:22 INFO SparkContext: Created broadcast 161 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 21:00:22 INFO DAGScheduler: Submitting 3 missing tasks from ShuffleMapStage 107 (MapPartitionsRDD[347] at count at cell106.sc:1) (first 15 tasks are for partitions Vector(0, 1, 2))\n",
      "24/09/02 21:00:22 INFO TaskSchedulerImpl: Adding task set 107.0 with 3 tasks resource profile 0\n",
      "24/09/02 21:00:22 INFO TaskSetManager: Starting task 0.0 in stage 107.0 (TID 141) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8344 bytes) \n",
      "24/09/02 21:00:22 INFO TaskSetManager: Starting task 1.0 in stage 107.0 (TID 142) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 8344 bytes) \n",
      "24/09/02 21:00:22 INFO TaskSetManager: Starting task 2.0 in stage 107.0 (TID 143) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 8344 bytes) \n",
      "24/09/02 21:00:22 INFO Executor: Running task 0.0 in stage 107.0 (TID 141)\n",
      "24/09/02 21:00:22 INFO Executor: Running task 2.0 in stage 107.0 (TID 143)\n",
      "24/09/02 21:00:22 INFO Executor: Running task 1.0 in stage 107.0 (TID 142)\n",
      "24/09/02 21:00:22 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 21:00:22 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 21:00:22 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 21:00:22 INFO Executor: Finished task 2.0 in stage 107.0 (TID 143). 2034 bytes result sent to driver\n",
      "24/09/02 21:00:22 INFO Executor: Finished task 0.0 in stage 107.0 (TID 141). 2034 bytes result sent to driver\n",
      "24/09/02 21:00:22 INFO Executor: Finished task 1.0 in stage 107.0 (TID 142). 2034 bytes result sent to driver\n",
      "24/09/02 21:00:22 INFO TaskSetManager: Finished task 2.0 in stage 107.0 (TID 143) in 37 ms on ubuntu (executor driver) (1/3)\n",
      "24/09/02 21:00:22 INFO TaskSetManager: Finished task 0.0 in stage 107.0 (TID 141) in 41 ms on ubuntu (executor driver) (2/3)\n",
      "24/09/02 21:00:22 INFO TaskSetManager: Finished task 1.0 in stage 107.0 (TID 142) in 43 ms on ubuntu (executor driver) (3/3)\n",
      "24/09/02 21:00:22 INFO TaskSchedulerImpl: Removed TaskSet 107.0, whose tasks have all completed, from pool \n",
      "24/09/02 21:00:22 INFO DAGScheduler: ShuffleMapStage 107 (count at cell106.sc:1) finished in 0,079 s\n",
      "24/09/02 21:00:22 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/02 21:00:22 INFO DAGScheduler: running: Set()\n",
      "24/09/02 21:00:22 INFO DAGScheduler: waiting: Set()\n",
      "24/09/02 21:00:22 INFO DAGScheduler: failed: Set()\n",
      "24/09/02 21:00:22 INFO SparkContext: Starting job: count at cell106.sc:1\n",
      "24/09/02 21:00:22 INFO DAGScheduler: Got job 94 (count at cell106.sc:1) with 1 output partitions\n",
      "24/09/02 21:00:22 INFO DAGScheduler: Final stage: ResultStage 109 (count at cell106.sc:1)\n",
      "24/09/02 21:00:22 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 108)\n",
      "24/09/02 21:00:22 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 21:00:22 INFO DAGScheduler: Submitting ResultStage 109 (MapPartitionsRDD[350] at count at cell106.sc:1), which has no missing parents\n",
      "24/09/02 21:00:22 INFO MemoryStore: Block broadcast_162 stored as values in memory (estimated size 12.5 KiB, free 4.5 GiB)\n",
      "24/09/02 21:00:22 INFO MemoryStore: Block broadcast_162_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 4.5 GiB)\n",
      "24/09/02 21:00:22 INFO BlockManagerInfo: Added broadcast_162_piece0 in memory on ubuntu:39321 (size: 5.9 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:00:22 INFO SparkContext: Created broadcast 162 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 21:00:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 109 (MapPartitionsRDD[350] at count at cell106.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 21:00:22 INFO TaskSchedulerImpl: Adding task set 109.0 with 1 tasks resource profile 0\n",
      "24/09/02 21:00:22 INFO TaskSetManager: Starting task 0.0 in stage 109.0 (TID 144) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/02 21:00:22 INFO Executor: Running task 0.0 in stage 109.0 (TID 144)\n",
      "24/09/02 21:00:22 INFO ShuffleBlockFetcherIterator: Getting 3 (180.0 B) non-empty blocks including 3 (180.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/02 21:00:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "24/09/02 21:00:22 INFO Executor: Finished task 0.0 in stage 109.0 (TID 144). 3995 bytes result sent to driver\n",
      "24/09/02 21:00:22 INFO TaskSetManager: Finished task 0.0 in stage 109.0 (TID 144) in 11 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 21:00:22 INFO TaskSchedulerImpl: Removed TaskSet 109.0, whose tasks have all completed, from pool \n",
      "24/09/02 21:00:22 INFO DAGScheduler: ResultStage 109 (count at cell106.sc:1) finished in 0,022 s\n",
      "24/09/02 21:00:22 INFO DAGScheduler: Job 94 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 21:00:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 109: Stage finished\n",
      "24/09/02 21:00:22 INFO DAGScheduler: Job 94 finished: count at cell106.sc:1, took 0,034749 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres106\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m1521L\u001b[39m"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerDf.union(customerDf).union(customerDf).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c156a8b9-8a7f-4f1f-a5ed-a41c6155c4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:58:58 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 20:58:58 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 20:58:58 INFO MemoryStore: Block broadcast_155 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 20:58:58 INFO BlockManagerInfo: Removed broadcast_153_piece0 on ubuntu:39321 in memory (size: 8.9 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:58:58 INFO MemoryStore: Block broadcast_155_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:58:58 INFO BlockManagerInfo: Removed broadcast_152_piece0 on ubuntu:39321 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:58:58 INFO BlockManagerInfo: Added broadcast_155_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:58:58 INFO SparkContext: Created broadcast 155 from count at cell105.sc:1\n",
      "24/09/02 20:58:58 INFO BlockManagerInfo: Removed broadcast_151_piece0 on ubuntu:39321 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:58:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 20:58:58 INFO DAGScheduler: Registering RDD 335 (count at cell105.sc:1) as input to shuffle 17\n",
      "24/09/02 20:58:58 INFO DAGScheduler: Got map stage job 91 (count at cell105.sc:1) with 1 output partitions\n",
      "24/09/02 20:58:58 INFO DAGScheduler: Final stage: ShuffleMapStage 104 (count at cell105.sc:1)\n",
      "24/09/02 20:58:58 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 20:58:58 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:58:58 INFO DAGScheduler: Submitting ShuffleMapStage 104 (MapPartitionsRDD[335] at count at cell105.sc:1), which has no missing parents\n",
      "24/09/02 20:58:58 INFO BlockManagerInfo: Removed broadcast_154_piece0 on ubuntu:39321 in memory (size: 5.9 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:58:58 INFO MemoryStore: Block broadcast_156 stored as values in memory (estimated size 17.0 KiB, free 4.5 GiB)\n",
      "24/09/02 20:58:58 INFO MemoryStore: Block broadcast_156_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 4.5 GiB)\n",
      "24/09/02 20:58:58 INFO BlockManagerInfo: Added broadcast_156_piece0 in memory on ubuntu:39321 (size: 8.3 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:58:58 INFO SparkContext: Created broadcast 156 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:58:58 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 104 (MapPartitionsRDD[335] at count at cell105.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:58:58 INFO TaskSchedulerImpl: Adding task set 104.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:58:58 INFO TaskSetManager: Starting task 0.0 in stage 104.0 (TID 139) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8235 bytes) \n",
      "24/09/02 20:58:58 INFO Executor: Running task 0.0 in stage 104.0 (TID 139)\n",
      "24/09/02 20:58:58 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 20:58:58 INFO Executor: Finished task 0.0 in stage 104.0 (TID 139). 1922 bytes result sent to driver\n",
      "24/09/02 20:58:58 INFO TaskSetManager: Finished task 0.0 in stage 104.0 (TID 139) in 18 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:58:58 INFO TaskSchedulerImpl: Removed TaskSet 104.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:58:58 INFO DAGScheduler: ShuffleMapStage 104 (count at cell105.sc:1) finished in 0,038 s\n",
      "24/09/02 20:58:58 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/02 20:58:58 INFO DAGScheduler: running: Set()\n",
      "24/09/02 20:58:58 INFO DAGScheduler: waiting: Set()\n",
      "24/09/02 20:58:58 INFO DAGScheduler: failed: Set()\n",
      "24/09/02 20:58:58 INFO SparkContext: Starting job: count at cell105.sc:1\n",
      "24/09/02 20:58:58 INFO DAGScheduler: Got job 92 (count at cell105.sc:1) with 1 output partitions\n",
      "24/09/02 20:58:58 INFO DAGScheduler: Final stage: ResultStage 106 (count at cell105.sc:1)\n",
      "24/09/02 20:58:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 105)\n",
      "24/09/02 20:58:58 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 20:58:58 INFO DAGScheduler: Submitting ResultStage 106 (MapPartitionsRDD[338] at count at cell105.sc:1), which has no missing parents\n",
      "24/09/02 20:58:58 INFO MemoryStore: Block broadcast_157 stored as values in memory (estimated size 12.5 KiB, free 4.5 GiB)\n",
      "24/09/02 20:58:58 INFO MemoryStore: Block broadcast_157_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 4.5 GiB)\n",
      "24/09/02 20:58:58 INFO BlockManagerInfo: Added broadcast_157_piece0 in memory on ubuntu:39321 (size: 5.9 KiB, free: 4.5 GiB)\n",
      "24/09/02 20:58:58 INFO SparkContext: Created broadcast 157 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 20:58:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 106 (MapPartitionsRDD[338] at count at cell105.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 20:58:58 INFO TaskSchedulerImpl: Adding task set 106.0 with 1 tasks resource profile 0\n",
      "24/09/02 20:58:58 INFO TaskSetManager: Starting task 0.0 in stage 106.0 (TID 140) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/02 20:58:58 INFO Executor: Running task 0.0 in stage 106.0 (TID 140)\n",
      "24/09/02 20:58:58 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/02 20:58:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\n",
      "24/09/02 20:58:58 INFO Executor: Finished task 0.0 in stage 106.0 (TID 140). 3995 bytes result sent to driver\n",
      "24/09/02 20:58:58 INFO TaskSetManager: Finished task 0.0 in stage 106.0 (TID 140) in 20 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 20:58:58 INFO TaskSchedulerImpl: Removed TaskSet 106.0, whose tasks have all completed, from pool \n",
      "24/09/02 20:58:58 INFO DAGScheduler: ResultStage 106 (count at cell105.sc:1) finished in 0,046 s\n",
      "24/09/02 20:58:58 INFO DAGScheduler: Job 92 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 20:58:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 106: Stage finished\n",
      "24/09/02 20:58:58 INFO DAGScheduler: Job 92 finished: count at cell105.sc:1, took 0,061022 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres105\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m2535L\u001b[39m"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerDf.count * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ee6cfc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 21:02:38 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 21:02:38 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 21:02:38 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 21:02:38 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 21:02:38 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 21:02:38 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 21:02:38 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 21:02:38 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 21:02:38 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 21:02:38 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 21:02:38 INFO MemoryStore: Block broadcast_167 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 21:02:38 INFO MemoryStore: Block broadcast_167_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 21:02:38 INFO BlockManagerInfo: Added broadcast_167_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:02:38 INFO SparkContext: Created broadcast 167 from count at cell113.sc:1\n",
      "24/09/02 21:02:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 21:02:38 INFO MemoryStore: Block broadcast_168 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 21:02:38 INFO MemoryStore: Block broadcast_168_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 21:02:38 INFO BlockManagerInfo: Added broadcast_168_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:02:38 INFO SparkContext: Created broadcast 168 from count at cell113.sc:1\n",
      "24/09/02 21:02:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 21:02:38 INFO MemoryStore: Block broadcast_169 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 21:02:38 INFO MemoryStore: Block broadcast_169_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 21:02:38 INFO BlockManagerInfo: Added broadcast_169_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:02:38 INFO SparkContext: Created broadcast 169 from count at cell113.sc:1\n",
      "24/09/02 21:02:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 21:02:38 INFO MemoryStore: Block broadcast_170 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 21:02:38 INFO MemoryStore: Block broadcast_170_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 21:02:38 INFO BlockManagerInfo: Added broadcast_170_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:02:38 INFO SparkContext: Created broadcast 170 from count at cell113.sc:1\n",
      "24/09/02 21:02:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 21:02:38 INFO MemoryStore: Block broadcast_171 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 21:02:38 INFO MemoryStore: Block broadcast_171_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 21:02:38 INFO BlockManagerInfo: Added broadcast_171_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:02:38 INFO SparkContext: Created broadcast 171 from count at cell113.sc:1\n",
      "24/09/02 21:02:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 21:02:38 INFO DAGScheduler: Registering RDD 373 (count at cell113.sc:1) as input to shuffle 20\n",
      "24/09/02 21:02:38 INFO DAGScheduler: Got map stage job 97 (count at cell113.sc:1) with 5 output partitions\n",
      "24/09/02 21:02:38 INFO DAGScheduler: Final stage: ShuffleMapStage 113 (count at cell113.sc:1)\n",
      "24/09/02 21:02:38 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 21:02:38 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 21:02:38 INFO DAGScheduler: Submitting ShuffleMapStage 113 (MapPartitionsRDD[373] at count at cell113.sc:1), which has no missing parents\n",
      "24/09/02 21:02:38 INFO MemoryStore: Block broadcast_172 stored as values in memory (estimated size 20.3 KiB, free 4.5 GiB)\n",
      "24/09/02 21:02:38 INFO MemoryStore: Block broadcast_172_piece0 stored as bytes in memory (estimated size 9.1 KiB, free 4.5 GiB)\n",
      "24/09/02 21:02:38 INFO BlockManagerInfo: Added broadcast_172_piece0 in memory on ubuntu:39321 (size: 9.1 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:02:38 INFO SparkContext: Created broadcast 172 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 21:02:38 INFO DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 113 (MapPartitionsRDD[373] at count at cell113.sc:1) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\n",
      "24/09/02 21:02:38 INFO TaskSchedulerImpl: Adding task set 113.0 with 5 tasks resource profile 0\n",
      "24/09/02 21:02:38 INFO TaskSetManager: Starting task 0.0 in stage 113.0 (TID 148) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8344 bytes) \n",
      "24/09/02 21:02:38 INFO TaskSetManager: Starting task 1.0 in stage 113.0 (TID 149) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 8344 bytes) \n",
      "24/09/02 21:02:38 INFO TaskSetManager: Starting task 2.0 in stage 113.0 (TID 150) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 8344 bytes) \n",
      "24/09/02 21:02:38 INFO TaskSetManager: Starting task 3.0 in stage 113.0 (TID 151) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 8344 bytes) \n",
      "24/09/02 21:02:38 INFO TaskSetManager: Starting task 4.0 in stage 113.0 (TID 152) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 8344 bytes) \n",
      "24/09/02 21:02:38 INFO Executor: Running task 2.0 in stage 113.0 (TID 150)\n",
      "24/09/02 21:02:38 INFO Executor: Running task 1.0 in stage 113.0 (TID 149)\n",
      "24/09/02 21:02:38 INFO Executor: Running task 3.0 in stage 113.0 (TID 151)\n",
      "24/09/02 21:02:38 INFO Executor: Running task 0.0 in stage 113.0 (TID 148)\n",
      "24/09/02 21:02:38 INFO Executor: Running task 4.0 in stage 113.0 (TID 152)\n",
      "24/09/02 21:02:38 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 21:02:38 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 21:02:38 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 21:02:38 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 21:02:38 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 21:02:38 INFO Executor: Finished task 3.0 in stage 113.0 (TID 151). 2146 bytes result sent to driver\n",
      "24/09/02 21:02:38 INFO TaskSetManager: Finished task 3.0 in stage 113.0 (TID 151) in 54 ms on ubuntu (executor driver) (1/5)\n",
      "24/09/02 21:02:38 INFO Executor: Finished task 4.0 in stage 113.0 (TID 152). 2146 bytes result sent to driver\n",
      "24/09/02 21:02:38 INFO Executor: Finished task 0.0 in stage 113.0 (TID 148). 2146 bytes result sent to driver\n",
      "24/09/02 21:02:38 INFO Executor: Finished task 2.0 in stage 113.0 (TID 150). 2146 bytes result sent to driver\n",
      "24/09/02 21:02:38 INFO TaskSetManager: Finished task 4.0 in stage 113.0 (TID 152) in 59 ms on ubuntu (executor driver) (2/5)\n",
      "24/09/02 21:02:38 INFO TaskSetManager: Finished task 2.0 in stage 113.0 (TID 150) in 65 ms on ubuntu (executor driver) (3/5)\n",
      "24/09/02 21:02:38 INFO Executor: Finished task 1.0 in stage 113.0 (TID 149). 2146 bytes result sent to driver\n",
      "24/09/02 21:02:38 INFO TaskSetManager: Finished task 0.0 in stage 113.0 (TID 148) in 71 ms on ubuntu (executor driver) (4/5)\n",
      "24/09/02 21:02:38 INFO TaskSetManager: Finished task 1.0 in stage 113.0 (TID 149) in 73 ms on ubuntu (executor driver) (5/5)\n",
      "24/09/02 21:02:38 INFO TaskSchedulerImpl: Removed TaskSet 113.0, whose tasks have all completed, from pool \n",
      "24/09/02 21:02:38 INFO DAGScheduler: ShuffleMapStage 113 (count at cell113.sc:1) finished in 0,106 s\n",
      "24/09/02 21:02:38 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/02 21:02:38 INFO DAGScheduler: running: Set()\n",
      "24/09/02 21:02:38 INFO DAGScheduler: waiting: Set()\n",
      "24/09/02 21:02:38 INFO DAGScheduler: failed: Set()\n",
      "24/09/02 21:02:38 INFO SparkContext: Starting job: count at cell113.sc:1\n",
      "24/09/02 21:02:38 INFO DAGScheduler: Got job 98 (count at cell113.sc:1) with 1 output partitions\n",
      "24/09/02 21:02:38 INFO DAGScheduler: Final stage: ResultStage 115 (count at cell113.sc:1)\n",
      "24/09/02 21:02:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 114)\n",
      "24/09/02 21:02:38 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 21:02:38 INFO DAGScheduler: Submitting ResultStage 115 (MapPartitionsRDD[376] at count at cell113.sc:1), which has no missing parents\n",
      "24/09/02 21:02:38 INFO MemoryStore: Block broadcast_173 stored as values in memory (estimated size 12.5 KiB, free 4.5 GiB)\n",
      "24/09/02 21:02:38 INFO MemoryStore: Block broadcast_173_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 4.5 GiB)\n",
      "24/09/02 21:02:38 INFO BlockManagerInfo: Added broadcast_173_piece0 in memory on ubuntu:39321 (size: 5.9 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:02:38 INFO SparkContext: Created broadcast 173 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 21:02:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 115 (MapPartitionsRDD[376] at count at cell113.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 21:02:38 INFO TaskSchedulerImpl: Adding task set 115.0 with 1 tasks resource profile 0\n",
      "24/09/02 21:02:38 INFO TaskSetManager: Starting task 0.0 in stage 115.0 (TID 153) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/02 21:02:38 INFO Executor: Running task 0.0 in stage 115.0 (TID 153)\n",
      "24/09/02 21:02:38 INFO ShuffleBlockFetcherIterator: Getting 5 (300.0 B) non-empty blocks including 5 (300.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/02 21:02:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "24/09/02 21:02:38 INFO Executor: Finished task 0.0 in stage 115.0 (TID 153). 3995 bytes result sent to driver\n",
      "24/09/02 21:02:38 INFO TaskSetManager: Finished task 0.0 in stage 115.0 (TID 153) in 14 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 21:02:38 INFO TaskSchedulerImpl: Removed TaskSet 115.0, whose tasks have all completed, from pool \n",
      "24/09/02 21:02:38 INFO DAGScheduler: ResultStage 115 (count at cell113.sc:1) finished in 0,030 s\n",
      "24/09/02 21:02:38 INFO DAGScheduler: Job 98 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 21:02:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 115: Stage finished\n",
      "24/09/02 21:02:38 INFO DAGScheduler: Job 98 finished: count at cell113.sc:1, took 0,046999 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres113\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m2535L\u001b[39m"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 to 5).map(a => customerDf).reduce((x, y) => x.union(y)).count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a00e4b2",
   "metadata": {},
   "source": [
    "### Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "40cf39a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 21:03:41 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "24/09/02 21:03:41 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "24/09/02 21:03:41 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 21:03:41 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 21:03:41 INFO MemoryStore: Block broadcast_174 stored as values in memory (estimated size 198.5 KiB, free 4.5 GiB)\n",
      "24/09/02 21:03:41 INFO BlockManagerInfo: Removed broadcast_169_piece0 on ubuntu:39321 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:03:41 INFO BlockManagerInfo: Removed broadcast_167_piece0 on ubuntu:39321 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:03:41 INFO MemoryStore: Block broadcast_174_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 4.5 GiB)\n",
      "24/09/02 21:03:41 INFO BlockManagerInfo: Added broadcast_174_piece0 in memory on ubuntu:39321 (size: 33.9 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:03:41 INFO BlockManagerInfo: Removed broadcast_168_piece0 on ubuntu:39321 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:03:41 INFO SparkContext: Created broadcast 174 from load at cell114.sc:1\n",
      "24/09/02 21:03:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 21:03:41 INFO SparkContext: Starting job: load at cell114.sc:1\n",
      "24/09/02 21:03:41 INFO BlockManagerInfo: Removed broadcast_171_piece0 on ubuntu:39321 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:03:41 INFO DAGScheduler: Got job 99 (load at cell114.sc:1) with 2 output partitions\n",
      "24/09/02 21:03:41 INFO DAGScheduler: Final stage: ResultStage 116 (load at cell114.sc:1)\n",
      "24/09/02 21:03:41 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 21:03:41 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 21:03:41 INFO BlockManagerInfo: Removed broadcast_170_piece0 on ubuntu:39321 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:03:41 INFO DAGScheduler: Submitting ResultStage 116 (MapPartitionsRDD[380] at load at cell114.sc:1), which has no missing parents\n",
      "24/09/02 21:03:41 INFO BlockManagerInfo: Removed broadcast_172_piece0 on ubuntu:39321 in memory (size: 9.1 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:03:41 INFO MemoryStore: Block broadcast_175 stored as values in memory (estimated size 15.8 KiB, free 4.5 GiB)\n",
      "24/09/02 21:03:41 INFO MemoryStore: Block broadcast_175_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 4.5 GiB)\n",
      "24/09/02 21:03:41 INFO BlockManagerInfo: Added broadcast_175_piece0 in memory on ubuntu:39321 (size: 7.4 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:03:41 INFO SparkContext: Created broadcast 175 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 21:03:41 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 116 (MapPartitionsRDD[380] at load at cell114.sc:1) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/09/02 21:03:41 INFO TaskSchedulerImpl: Adding task set 116.0 with 2 tasks resource profile 0\n",
      "24/09/02 21:03:41 INFO BlockManagerInfo: Removed broadcast_173_piece0 on ubuntu:39321 in memory (size: 5.9 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:03:41 INFO TaskSetManager: Starting task 0.0 in stage 116.0 (TID 154) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8244 bytes) \n",
      "24/09/02 21:03:41 INFO TaskSetManager: Starting task 1.0 in stage 116.0 (TID 155) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 8244 bytes) \n",
      "24/09/02 21:03:41 INFO Executor: Running task 0.0 in stage 116.0 (TID 154)\n",
      "24/09/02 21:03:41 INFO Executor: Running task 1.0 in stage 116.0 (TID 155)\n",
      "24/09/02 21:03:41 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/retail_data.json, range: 0-4194304, partition values: [empty row]\n",
      "24/09/02 21:03:41 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/retail_data.json, range: 4194304-7997578, partition values: [empty row]\n",
      "24/09/02 21:03:41 INFO Executor: Finished task 0.0 in stage 116.0 (TID 154). 2093 bytes result sent to driver\n",
      "24/09/02 21:03:41 INFO TaskSetManager: Finished task 0.0 in stage 116.0 (TID 154) in 136 ms on ubuntu (executor driver) (1/2)\n",
      "24/09/02 21:03:41 INFO Executor: Finished task 1.0 in stage 116.0 (TID 155). 2093 bytes result sent to driver\n",
      "24/09/02 21:03:41 INFO TaskSetManager: Finished task 1.0 in stage 116.0 (TID 155) in 160 ms on ubuntu (executor driver) (2/2)\n",
      "24/09/02 21:03:41 INFO TaskSchedulerImpl: Removed TaskSet 116.0, whose tasks have all completed, from pool \n",
      "24/09/02 21:03:41 INFO DAGScheduler: ResultStage 116 (load at cell114.sc:1) finished in 0,172 s\n",
      "24/09/02 21:03:41 INFO DAGScheduler: Job 99 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 21:03:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 116: Stage finished\n",
      "24/09/02 21:03:41 INFO DAGScheduler: Job 99 finished: load at cell114.sc:1, took 0,195401 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mretailDf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CustomerID: string, Description: string ... 5 more fields]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val retailDf = spark.read.format(\"json\").load(\"data/retail_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "916d34e1-e562-468d-bf51-47370b85fa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Birthdate: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "20889378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- UnitPrice: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "retailDf.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "fe8b62e3-0793-4cf4-86eb-03be86191551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres122\u001b[39m: \u001b[32mSet\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mSet\u001b[39m(\u001b[32m\"CustomerID\"\u001b[39m)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(customerDf.dtypes.map(_._1)).toSet.intersect((retailDf.dtypes.map(_._1)).toSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "12be704d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 21:07:39 INFO BlockManagerInfo: Removed broadcast_178_piece0 on ubuntu:39321 in memory (size: 65.5 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:07:39 INFO BlockManagerInfo: Removed broadcast_176_piece0 on ubuntu:39321 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:07:39 INFO BlockManagerInfo: Removed broadcast_177_piece0 on ubuntu:39321 in memory (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:07:39 INFO BlockManagerInfo: Removed broadcast_179_piece0 on ubuntu:39321 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:07:39 INFO BlockManagerInfo: Removed broadcast_180_piece0 on ubuntu:39321 in memory (size: 8.9 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:07:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/09/02 21:07:40 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#1176)\n",
      "24/09/02 21:07:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/09/02 21:07:40 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#2078)\n",
      "24/09/02 21:07:40 INFO MemoryStore: Block broadcast_181 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 21:07:40 INFO MemoryStore: Block broadcast_181_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 21:07:40 INFO BlockManagerInfo: Added broadcast_181_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:07:40 INFO SparkContext: Created broadcast 181 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/02 21:07:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 21:07:40 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/02 21:07:40 INFO DAGScheduler: Got job 102 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "24/09/02 21:07:40 INFO DAGScheduler: Final stage: ResultStage 119 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "24/09/02 21:07:40 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 21:07:40 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 21:07:40 INFO DAGScheduler: Submitting ResultStage 119 (MapPartitionsRDD[392] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "24/09/02 21:07:40 INFO MemoryStore: Block broadcast_182 stored as values in memory (estimated size 16.8 KiB, free 4.5 GiB)\n",
      "24/09/02 21:07:40 INFO MemoryStore: Block broadcast_182_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 4.5 GiB)\n",
      "24/09/02 21:07:40 INFO BlockManagerInfo: Added broadcast_182_piece0 in memory on ubuntu:39321 (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:07:40 INFO SparkContext: Created broadcast 182 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 21:07:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 119 (MapPartitionsRDD[392] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 21:07:40 INFO TaskSchedulerImpl: Adding task set 119.0 with 1 tasks resource profile 0\n",
      "24/09/02 21:07:40 INFO TaskSetManager: Starting task 0.0 in stage 119.0 (TID 158) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/02 21:07:40 INFO Executor: Running task 0.0 in stage 119.0 (TID 158)\n",
      "24/09/02 21:07:40 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 21:07:40 INFO Executor: Finished task 0.0 in stage 119.0 (TID 158). 63135 bytes result sent to driver\n",
      "24/09/02 21:07:40 INFO TaskSetManager: Finished task 0.0 in stage 119.0 (TID 158) in 22 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 21:07:40 INFO TaskSchedulerImpl: Removed TaskSet 119.0, whose tasks have all completed, from pool \n",
      "24/09/02 21:07:40 INFO DAGScheduler: ResultStage 119 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0,048 s\n",
      "24/09/02 21:07:40 INFO DAGScheduler: Job 102 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 21:07:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 119: Stage finished\n",
      "24/09/02 21:07:40 INFO DAGScheduler: Job 102 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0,050527 s\n",
      "24/09/02 21:07:40 INFO MemoryStore: Block broadcast_183 stored as values in memory (estimated size 32.0 MiB, free 4.5 GiB)\n",
      "24/09/02 21:07:40 INFO MemoryStore: Block broadcast_183_piece0 stored as bytes in memory (estimated size 65.5 KiB, free 4.5 GiB)\n",
      "24/09/02 21:07:40 INFO BlockManagerInfo: Added broadcast_183_piece0 in memory on ubuntu:39321 (size: 65.5 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:07:40 INFO SparkContext: Created broadcast 183 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/02 21:07:40 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/09/02 21:07:40 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#2078)\n",
      "24/09/02 21:07:40 INFO MemoryStore: Block broadcast_184 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 21:07:40 INFO MemoryStore: Block broadcast_184_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 21:07:40 INFO BlockManagerInfo: Added broadcast_184_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:07:40 INFO SparkContext: Created broadcast 184 from show at cell124.sc:1\n",
      "24/09/02 21:07:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 21:07:40 INFO SparkContext: Starting job: show at cell124.sc:1\n",
      "24/09/02 21:07:40 INFO DAGScheduler: Got job 103 (show at cell124.sc:1) with 1 output partitions\n",
      "24/09/02 21:07:40 INFO DAGScheduler: Final stage: ResultStage 120 (show at cell124.sc:1)\n",
      "24/09/02 21:07:40 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 21:07:40 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 21:07:40 INFO DAGScheduler: Submitting ResultStage 120 (MapPartitionsRDD[396] at show at cell124.sc:1), which has no missing parents\n",
      "24/09/02 21:07:40 INFO MemoryStore: Block broadcast_185 stored as values in memory (estimated size 20.9 KiB, free 4.5 GiB)\n",
      "24/09/02 21:07:40 INFO MemoryStore: Block broadcast_185_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 4.5 GiB)\n",
      "24/09/02 21:07:40 INFO BlockManagerInfo: Added broadcast_185_piece0 in memory on ubuntu:39321 (size: 8.9 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:07:40 INFO SparkContext: Created broadcast 185 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 21:07:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 120 (MapPartitionsRDD[396] at show at cell124.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 21:07:40 INFO TaskSchedulerImpl: Adding task set 120.0 with 1 tasks resource profile 0\n",
      "24/09/02 21:07:40 INFO TaskSetManager: Starting task 0.0 in stage 120.0 (TID 159) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8244 bytes) \n",
      "24/09/02 21:07:40 INFO Executor: Running task 0.0 in stage 120.0 (TID 159)\n",
      "24/09/02 21:07:40 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/retail_data.json, range: 0-4194304, partition values: [empty row]\n",
      "24/09/02 21:07:40 INFO Executor: Finished task 0.0 in stage 120.0 (TID 159). 2730 bytes result sent to driver\n",
      "24/09/02 21:07:40 INFO TaskSetManager: Finished task 0.0 in stage 120.0 (TID 159) in 28 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 21:07:40 INFO TaskSchedulerImpl: Removed TaskSet 120.0, whose tasks have all completed, from pool \n",
      "24/09/02 21:07:40 INFO DAGScheduler: ResultStage 120 (show at cell124.sc:1) finished in 0,048 s\n",
      "24/09/02 21:07:40 INFO DAGScheduler: Job 103 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 21:07:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 120: Stage finished\n",
      "24/09/02 21:07:40 INFO DAGScheduler: Job 103 finished: show at cell124.sc:1, took 0,052115 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+--------------+--------------------+---------------+----------------+--------------------+---------------+---------+--------+---------+---------+\n",
      "|CustomerID|             Address|          Birthdate|       Country|               Email|           Name|        Username|         Description|    InvoiceDate|InvoiceNo|Quantity|StockCode|UnitPrice|\n",
      "+----------+--------------------+-------------------+--------------+--------------------+---------------+----------------+--------------------+---------------+---------+--------+---------+---------+\n",
      "|     12346|Unit 1047 Box 408...|1994-02-20 00:46:27|United Kingdom|cooperalexis@hotm...|  Lindsay Cowan|valenciajennifer|MEDIUM CERAMIC TO...|1/18/2011 10:01|   541431|   74215|    23166|     1.04|\n",
      "|     12346|Unit 1047 Box 408...|1994-02-20 00:46:27|United Kingdom|cooperalexis@hotm...|  Lindsay Cowan|valenciajennifer|MEDIUM CERAMIC TO...|1/18/2011 10:17|  C541433|  -74215|    23166|     1.04|\n",
      "|     12347|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|timothy78@hotmail...|Katherine David|      hillrachel|BLACK CANDELABRA ...|12/7/2010 14:57|   537626|      12|    85116|      2.1|\n",
      "|     12347|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|timothy78@hotmail...|Katherine David|      hillrachel|AIRLINE BAG VINTA...|12/7/2010 14:57|   537626|       4|    22375|     4.25|\n",
      "|     12347|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|timothy78@hotmail...|Katherine David|      hillrachel|COLOUR GLASS. STA...|12/7/2010 14:57|   537626|      12|    71477|     3.25|\n",
      "|     12347|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|timothy78@hotmail...|Katherine David|      hillrachel|MINI PAINT SET VI...|12/7/2010 14:57|   537626|      36|    22492|     0.65|\n",
      "|     12347|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|timothy78@hotmail...|Katherine David|      hillrachel|CLEAR DRAWER KNOB...|12/7/2010 14:57|   537626|      12|    22771|     1.25|\n",
      "|     12347|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|timothy78@hotmail...|Katherine David|      hillrachel|PINK DRAWER KNOB ...|12/7/2010 14:57|   537626|      12|    22772|     1.25|\n",
      "|     12347|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|timothy78@hotmail...|Katherine David|      hillrachel|GREEN DRAWER KNOB...|12/7/2010 14:57|   537626|      12|    22773|     1.25|\n",
      "|     12347|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|timothy78@hotmail...|Katherine David|      hillrachel|RED DRAWER KNOB A...|12/7/2010 14:57|   537626|      12|    22774|     1.25|\n",
      "+----------+--------------------+-------------------+--------------+--------------------+---------------+----------------+--------------------+---------------+---------+--------+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.join(retailDf, Seq(\"CustomerID\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "90400c03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 21:12:32 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 21:12:32 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 21:12:32 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/09/02 21:12:32 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#2078)\n",
      "24/09/02 21:12:32 INFO CodeGenerator: Code generated in 6.185747 ms\n",
      "24/09/02 21:12:32 INFO MemoryStore: Block broadcast_191 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 21:12:32 INFO MemoryStore: Block broadcast_191_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 21:12:32 INFO BlockManagerInfo: Added broadcast_191_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:12:32 INFO SparkContext: Created broadcast 191 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/02 21:12:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 21:12:32 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/02 21:12:32 INFO DAGScheduler: Got job 106 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions\n",
      "24/09/02 21:12:32 INFO DAGScheduler: Final stage: ResultStage 123 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "24/09/02 21:12:32 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 21:12:32 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 21:12:32 INFO DAGScheduler: Submitting ResultStage 123 (MapPartitionsRDD[408] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "24/09/02 21:12:32 INFO MemoryStore: Block broadcast_192 stored as values in memory (estimated size 17.0 KiB, free 4.5 GiB)\n",
      "24/09/02 21:12:32 INFO MemoryStore: Block broadcast_192_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 4.5 GiB)\n",
      "24/09/02 21:12:32 INFO BlockManagerInfo: Added broadcast_192_piece0 in memory on ubuntu:39321 (size: 7.7 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:12:32 INFO SparkContext: Created broadcast 192 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 21:12:32 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 123 (MapPartitionsRDD[408] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/09/02 21:12:32 INFO TaskSchedulerImpl: Adding task set 123.0 with 2 tasks resource profile 0\n",
      "24/09/02 21:12:32 INFO TaskSetManager: Starting task 0.0 in stage 123.0 (TID 163) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8244 bytes) \n",
      "24/09/02 21:12:32 INFO TaskSetManager: Starting task 1.0 in stage 123.0 (TID 164) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 8244 bytes) \n",
      "24/09/02 21:12:32 INFO Executor: Running task 0.0 in stage 123.0 (TID 163)\n",
      "24/09/02 21:12:32 INFO Executor: Running task 1.0 in stage 123.0 (TID 164)\n",
      "24/09/02 21:12:32 INFO CodeGenerator: Code generated in 16.380406 ms\n",
      "24/09/02 21:12:32 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/retail_data.json, range: 0-4194304, partition values: [empty row]\n",
      "24/09/02 21:12:32 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/retail_data.json, range: 4194304-7997578, partition values: [empty row]\n",
      "24/09/02 21:12:33 INFO Executor: Finished task 1.0 in stage 123.0 (TID 164). 980502 bytes result sent to driver\n",
      "24/09/02 21:12:33 INFO TaskSetManager: Finished task 1.0 in stage 123.0 (TID 164) in 287 ms on ubuntu (executor driver) (1/2)\n",
      "24/09/02 21:12:33 INFO MemoryStore: Block taskresult_163 stored as bytes in memory (estimated size 1080.2 KiB, free 4.5 GiB)\n",
      "24/09/02 21:12:33 INFO BlockManagerInfo: Added taskresult_163 in memory on ubuntu:39321 (size: 1080.2 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:12:33 INFO Executor: Finished task 0.0 in stage 123.0 (TID 163). 1106112 bytes result sent via BlockManager)\n",
      "24/09/02 21:12:33 INFO TaskSetManager: Finished task 0.0 in stage 123.0 (TID 163) in 315 ms on ubuntu (executor driver) (2/2)\n",
      "24/09/02 21:12:33 INFO DAGScheduler: ResultStage 123 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0,334 s\n",
      "24/09/02 21:12:33 INFO BlockManagerInfo: Removed taskresult_163 on ubuntu:39321 in memory (size: 1080.2 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:12:33 INFO TaskSchedulerImpl: Removed TaskSet 123.0, whose tasks have all completed, from pool \n",
      "24/09/02 21:12:33 INFO DAGScheduler: Job 106 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 21:12:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 123: Stage finished\n",
      "24/09/02 21:12:33 INFO DAGScheduler: Job 106 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0,346769 s\n",
      "24/09/02 21:12:33 INFO CodeGenerator: Code generated in 5.21309 ms\n",
      "24/09/02 21:12:33 INFO MemoryStore: Block broadcast_193 stored as values in memory (estimated size 34.0 MiB, free 4.5 GiB)\n",
      "24/09/02 21:12:33 INFO MemoryStore: Block broadcast_193_piece0 stored as bytes in memory (estimated size 2.2 MiB, free 4.5 GiB)\n",
      "24/09/02 21:12:33 INFO BlockManagerInfo: Added broadcast_193_piece0 in memory on ubuntu:39321 (size: 2.2 MiB, free: 4.5 GiB)\n",
      "24/09/02 21:12:33 INFO SparkContext: Created broadcast 193 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/02 21:12:33 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 21:12:33 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 21:12:33 INFO CodeGenerator: Code generated in 15.682309 ms\n",
      "24/09/02 21:12:33 INFO MemoryStore: Block broadcast_194 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 21:12:33 INFO MemoryStore: Block broadcast_194_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 21:12:33 INFO BlockManagerInfo: Added broadcast_194_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:12:33 INFO SparkContext: Created broadcast 194 from show at cell129.sc:4\n",
      "24/09/02 21:12:33 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 21:12:33 INFO SparkContext: Starting job: show at cell129.sc:4\n",
      "24/09/02 21:12:33 INFO DAGScheduler: Got job 107 (show at cell129.sc:4) with 1 output partitions\n",
      "24/09/02 21:12:33 INFO DAGScheduler: Final stage: ResultStage 124 (show at cell129.sc:4)\n",
      "24/09/02 21:12:33 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 21:12:33 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 21:12:33 INFO DAGScheduler: Submitting ResultStage 124 (MapPartitionsRDD[412] at show at cell129.sc:4), which has no missing parents\n",
      "24/09/02 21:12:33 INFO MemoryStore: Block broadcast_195 stored as values in memory (estimated size 22.5 KiB, free 4.5 GiB)\n",
      "24/09/02 21:12:33 INFO MemoryStore: Block broadcast_195_piece0 stored as bytes in memory (estimated size 9.4 KiB, free 4.5 GiB)\n",
      "24/09/02 21:12:33 INFO BlockManagerInfo: Added broadcast_195_piece0 in memory on ubuntu:39321 (size: 9.4 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:12:33 INFO SparkContext: Created broadcast 195 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 21:12:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 124 (MapPartitionsRDD[412] at show at cell129.sc:4) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 21:12:33 INFO TaskSchedulerImpl: Adding task set 124.0 with 1 tasks resource profile 0\n",
      "24/09/02 21:12:33 INFO TaskSetManager: Starting task 0.0 in stage 124.0 (TID 165) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/02 21:12:33 INFO Executor: Running task 0.0 in stage 124.0 (TID 165)\n",
      "24/09/02 21:12:33 INFO CodeGenerator: Code generated in 14.120297 ms\n",
      "24/09/02 21:12:33 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 21:12:33 INFO Executor: Finished task 0.0 in stage 124.0 (TID 165). 2750 bytes result sent to driver\n",
      "24/09/02 21:12:33 INFO TaskSetManager: Finished task 0.0 in stage 124.0 (TID 165) in 37 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 21:12:33 INFO TaskSchedulerImpl: Removed TaskSet 124.0, whose tasks have all completed, from pool \n",
      "24/09/02 21:12:33 INFO DAGScheduler: ResultStage 124 (show at cell129.sc:4) finished in 0,049 s\n",
      "24/09/02 21:12:33 INFO DAGScheduler: Job 107 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 21:12:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 124: Stage finished\n",
      "24/09/02 21:12:33 INFO DAGScheduler: Job 107 finished: show at cell129.sc:4, took 0,055068 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------+----------+--------------------+---------------+----------------+---------------+--------------------+---------------+---------+--------+---------+---------+\n",
      "|             Address|          Birthdate|       Country|CustomerID|               Email|           Name|        Username|rightCustomerID|         Description|    InvoiceDate|InvoiceNo|Quantity|StockCode|UnitPrice|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+---------------+----------------+---------------+--------------------+---------------+---------+--------+---------+---------+\n",
      "|Unit 1047 Box 408...|1994-02-20 00:46:27|United Kingdom|     12346|cooperalexis@hotm...|  Lindsay Cowan|valenciajennifer|          12346|MEDIUM CERAMIC TO...|1/18/2011 10:17|  C541433|  -74215|    23166|     1.04|\n",
      "|Unit 1047 Box 408...|1994-02-20 00:46:27|United Kingdom|     12346|cooperalexis@hotm...|  Lindsay Cowan|valenciajennifer|          12346|MEDIUM CERAMIC TO...|1/18/2011 10:01|   541431|   74215|    23166|     1.04|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|Katherine David|      hillrachel|          12347|MINI PLAYING CARD...|12/7/2011 15:52|   581180|      20|    23508|     0.42|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|Katherine David|      hillrachel|          12347|MINI PLAYING CARD...|12/7/2011 15:52|   581180|      20|    23506|     0.42|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|Katherine David|      hillrachel|          12347|CHRISTMAS TABLE S...|12/7/2011 15:52|   581180|      16|    23271|     0.83|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|Katherine David|      hillrachel|          12347|PINK GOOSE FEATHE...|12/7/2011 15:52|   581180|      12|    21265|     1.95|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|Katherine David|      hillrachel|          12347|WOODLAND CHARLOTT...|12/7/2011 15:52|   581180|      10|    20719|     0.85|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|Katherine David|      hillrachel|          12347|  RABBIT NIGHT LIGHT|12/7/2011 15:52|   581180|      24|    23084|     1.79|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|Katherine David|      hillrachel|          12347|RED TOADSTOOL LED...|12/7/2011 15:52|   581180|      24|    21731|     1.65|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|Katherine David|      hillrachel|          12347|PINK NEW BAROQUEC...|12/7/2011 15:52|   581180|      24|   84625A|     0.85|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+---------------+----------------+---------------+--------------------+---------------+---------+--------+---------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mnewRetailDf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [rightCustomerID: string, Description: string ... 5 more fields]\n",
       "\u001b[36mcustomerRetailDf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Address: string, Birthdate: string ... 12 more fields]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newRetailDf = retailDf.withColumnRenamed(\"CustomerID\", \"rightCustomerID\")\n",
    "\n",
    "val customerRetailDf = customerDf.join(newRetailDf, customerDf(\"CustomerID\") === newRetailDf(\"rightCustomerID\"), \"left\")\n",
    "customerRetailDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1a512977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 21:12:50 INFO BlockManagerInfo: Removed broadcast_195_piece0 on ubuntu:39321 in memory (size: 9.4 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:12:50 INFO BlockManagerInfo: Removed broadcast_192_piece0 on ubuntu:39321 in memory (size: 7.7 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:12:51 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 21:12:51 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 21:12:51 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/09/02 21:12:51 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#2078)\n",
      "24/09/02 21:12:51 INFO CodeGenerator: Code generated in 5.315077 ms\n",
      "24/09/02 21:12:51 INFO MemoryStore: Block broadcast_196 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 21:12:51 INFO MemoryStore: Block broadcast_196_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 21:12:51 INFO BlockManagerInfo: Added broadcast_196_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:12:51 INFO SparkContext: Created broadcast 196 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/02 21:12:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 21:12:51 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/02 21:12:51 INFO DAGScheduler: Got job 108 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 2 output partitions\n",
      "24/09/02 21:12:51 INFO DAGScheduler: Final stage: ResultStage 125 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "24/09/02 21:12:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 21:12:51 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 21:12:51 INFO DAGScheduler: Submitting ResultStage 125 (MapPartitionsRDD[416] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "24/09/02 21:12:51 INFO MemoryStore: Block broadcast_197 stored as values in memory (estimated size 14.6 KiB, free 4.5 GiB)\n",
      "24/09/02 21:12:51 INFO MemoryStore: Block broadcast_197_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 4.5 GiB)\n",
      "24/09/02 21:12:51 INFO BlockManagerInfo: Added broadcast_197_piece0 in memory on ubuntu:39321 (size: 7.1 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:12:51 INFO SparkContext: Created broadcast 197 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 21:12:51 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 125 (MapPartitionsRDD[416] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/09/02 21:12:51 INFO TaskSchedulerImpl: Adding task set 125.0 with 2 tasks resource profile 0\n",
      "24/09/02 21:12:51 INFO TaskSetManager: Starting task 0.0 in stage 125.0 (TID 166) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8244 bytes) \n",
      "24/09/02 21:12:51 INFO TaskSetManager: Starting task 1.0 in stage 125.0 (TID 167) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 8244 bytes) \n",
      "24/09/02 21:12:51 INFO Executor: Running task 0.0 in stage 125.0 (TID 166)\n",
      "24/09/02 21:12:51 INFO Executor: Running task 1.0 in stage 125.0 (TID 167)\n",
      "24/09/02 21:12:51 INFO CodeGenerator: Code generated in 10.910362 ms\n",
      "24/09/02 21:12:51 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/retail_data.json, range: 0-4194304, partition values: [empty row]\n",
      "24/09/02 21:12:51 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/retail_data.json, range: 4194304-7997578, partition values: [empty row]\n",
      "24/09/02 21:12:51 INFO Executor: Finished task 0.0 in stage 125.0 (TID 166). 7876 bytes result sent to driver\n",
      "24/09/02 21:12:51 INFO TaskSetManager: Finished task 0.0 in stage 125.0 (TID 166) in 205 ms on ubuntu (executor driver) (1/2)\n",
      "24/09/02 21:12:51 INFO Executor: Finished task 1.0 in stage 125.0 (TID 167). 7050 bytes result sent to driver\n",
      "24/09/02 21:12:51 INFO TaskSetManager: Finished task 1.0 in stage 125.0 (TID 167) in 205 ms on ubuntu (executor driver) (2/2)\n",
      "24/09/02 21:12:51 INFO TaskSchedulerImpl: Removed TaskSet 125.0, whose tasks have all completed, from pool \n",
      "24/09/02 21:12:51 INFO DAGScheduler: ResultStage 125 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0,213 s\n",
      "24/09/02 21:12:51 INFO DAGScheduler: Job 108 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 21:12:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 125: Stage finished\n",
      "24/09/02 21:12:51 INFO DAGScheduler: Job 108 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0,220040 s\n",
      "24/09/02 21:12:51 INFO MemoryStore: Block broadcast_198 stored as values in memory (estimated size 34.0 MiB, free 4.5 GiB)\n",
      "24/09/02 21:12:51 INFO MemoryStore: Block broadcast_198_piece0 stored as bytes in memory (estimated size 178.7 KiB, free 4.5 GiB)\n",
      "24/09/02 21:12:51 INFO BlockManagerInfo: Added broadcast_198_piece0 in memory on ubuntu:39321 (size: 178.7 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:12:51 INFO SparkContext: Created broadcast 198 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/02 21:12:51 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/02 21:12:51 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/02 21:12:51 INFO CodeGenerator: Code generated in 9.409939 ms\n",
      "24/09/02 21:12:51 INFO MemoryStore: Block broadcast_199 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/02 21:12:51 INFO MemoryStore: Block broadcast_199_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/02 21:12:51 INFO BlockManagerInfo: Added broadcast_199_piece0 in memory on ubuntu:39321 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:12:51 INFO SparkContext: Created broadcast 199 from show at cell130.sc:1\n",
      "24/09/02 21:12:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/02 21:12:51 INFO SparkContext: Starting job: show at cell130.sc:1\n",
      "24/09/02 21:12:51 INFO DAGScheduler: Got job 109 (show at cell130.sc:1) with 1 output partitions\n",
      "24/09/02 21:12:51 INFO DAGScheduler: Final stage: ResultStage 126 (show at cell130.sc:1)\n",
      "24/09/02 21:12:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/02 21:12:51 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/02 21:12:51 INFO DAGScheduler: Submitting ResultStage 126 (MapPartitionsRDD[420] at show at cell130.sc:1), which has no missing parents\n",
      "24/09/02 21:12:51 INFO MemoryStore: Block broadcast_200 stored as values in memory (estimated size 16.7 KiB, free 4.5 GiB)\n",
      "24/09/02 21:12:51 INFO MemoryStore: Block broadcast_200_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 4.5 GiB)\n",
      "24/09/02 21:12:51 INFO BlockManagerInfo: Added broadcast_200_piece0 in memory on ubuntu:39321 (size: 7.9 KiB, free: 4.5 GiB)\n",
      "24/09/02 21:12:51 INFO SparkContext: Created broadcast 200 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/02 21:12:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 126 (MapPartitionsRDD[420] at show at cell130.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/02 21:12:51 INFO TaskSchedulerImpl: Adding task set 126.0 with 1 tasks resource profile 0\n",
      "24/09/02 21:12:51 INFO TaskSetManager: Starting task 0.0 in stage 126.0 (TID 168) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/02 21:12:51 INFO Executor: Running task 0.0 in stage 126.0 (TID 168)\n",
      "24/09/02 21:12:51 INFO CodeGenerator: Code generated in 17.315635 ms\n",
      "24/09/02 21:12:51 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/02 21:12:51 INFO Executor: Finished task 0.0 in stage 126.0 (TID 168). 1633 bytes result sent to driver\n",
      "24/09/02 21:12:51 INFO TaskSetManager: Finished task 0.0 in stage 126.0 (TID 168) in 37 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/02 21:12:51 INFO DAGScheduler: ResultStage 126 (show at cell130.sc:1) finished in 0,053 s\n",
      "24/09/02 21:12:51 INFO DAGScheduler: Job 109 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/02 21:12:51 INFO TaskSchedulerImpl: Removed TaskSet 126.0, whose tasks have all completed, from pool \n",
      "24/09/02 21:12:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 126: Stage finished\n",
      "24/09/02 21:12:51 INFO DAGScheduler: Job 109 finished: show at cell130.sc:1, took 0,061398 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|CustomerID|\n",
      "+----------+\n",
      "|     12346|\n",
      "|     12346|\n",
      "|     12347|\n",
      "|     12347|\n",
      "|     12347|\n",
      "|     12347|\n",
      "|     12347|\n",
      "|     12347|\n",
      "|     12347|\n",
      "|     12347|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerRetailDf.select(\"CustomerID\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45091db0-fbd1-4567-b5eb-faec32fd5325",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "25e2b3ef-f005-4fa1-b6e5-533bf3fd55e6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 21:30:33 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/09/02 21:30:33 INFO SparkUI: Stopped Spark web UI at http://ubuntu:4040\n",
      "24/09/02 21:30:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/09/02 21:30:33 INFO MemoryStore: MemoryStore cleared\n",
      "24/09/02 21:30:33 INFO BlockManager: BlockManager stopped\n",
      "24/09/02 21:30:33 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/09/02 21:30:33 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/09/02 21:30:33 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2f2362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
