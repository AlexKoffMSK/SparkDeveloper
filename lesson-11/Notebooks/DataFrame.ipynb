{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2515a1c-c4f9-4890-aa6d-641bf419cea6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.5.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc99a5fd-2141-4ffe-afb1-4b0a595a75af",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ab7dc3-9173-4d9e-9a91-97256abb3a30",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49835da8-c4d0-461a-a2c8-3c0959118bb4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7df9de1d-1f63-41b1-999b-69614c723fae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "24/09/01 17:18:20 INFO SparkContext: Running Spark version 3.5.0\n",
      "24/09/01 17:18:21 INFO SparkContext: OS info Linux, 6.8.0-40-generic, amd64\n",
      "24/09/01 17:18:21 INFO SparkContext: Java version 11.0.24\n",
      "24/09/01 17:18:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/01 17:18:21 INFO ResourceUtils: ==============================================================\n",
      "24/09/01 17:18:21 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/09/01 17:18:21 INFO ResourceUtils: ==============================================================\n",
      "24/09/01 17:18:21 INFO SparkContext: Submitted application: Dataframe API\n",
      "24/09/01 17:18:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/09/01 17:18:21 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/09/01 17:18:21 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/09/01 17:18:21 INFO SecurityManager: Changing view acls to: vadim\n",
      "24/09/01 17:18:21 INFO SecurityManager: Changing modify acls to: vadim\n",
      "24/09/01 17:18:21 INFO SecurityManager: Changing view acls groups to: \n",
      "24/09/01 17:18:21 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/09/01 17:18:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: vadim; groups with view permissions: EMPTY; users with modify permissions: vadim; groups with modify permissions: EMPTY\n",
      "24/09/01 17:18:21 INFO Utils: Successfully started service 'sparkDriver' on port 42033.\n",
      "24/09/01 17:18:21 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/09/01 17:18:21 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/09/01 17:18:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/09/01 17:18:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/09/01 17:18:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/09/01 17:18:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8d7d7f72-f3be-4eff-bb82-26558a3e25dc\n",
      "24/09/01 17:18:21 INFO MemoryStore: MemoryStore started with capacity 4.5 GiB\n",
      "24/09/01 17:18:21 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/09/01 17:18:21 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/09/01 17:18:21 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/09/01 17:18:22 INFO Executor: Starting executor ID driver on host ubuntu\n",
      "24/09/01 17:18:22 INFO Executor: OS info Linux, 6.8.0-40-generic, amd64\n",
      "24/09/01 17:18:22 INFO Executor: Java version 11.0.24\n",
      "24/09/01 17:18:22 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "24/09/01 17:18:22 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@4757dcba for default.\n",
      "24/09/01 17:18:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36981.\n",
      "24/09/01 17:18:22 INFO NettyBlockTransferService: Server created on ubuntu:36981\n",
      "24/09/01 17:18:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/09/01 17:18:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ubuntu, 36981, None)\n",
      "24/09/01 17:18:22 INFO BlockManagerMasterEndpoint: Registering block manager ubuntu:36981 with 4.5 GiB RAM, BlockManagerId(driver, ubuntu, 36981, None)\n",
      "24/09/01 17:18:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ubuntu, 36981, None)\n",
      "24/09/01 17:18:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ubuntu, 36981, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@6fff8352\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "                .builder()\n",
    "                .master(\"local[*]\")\n",
    "                .appName(\"Dataframe API\")\n",
    "                .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59c101c3-900c-4a51-8f5f-bfc0ce535ef9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.version == 3.5.0\n"
     ]
    }
   ],
   "source": [
    "println(s\"spark.version == ${spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0928fb8-44ac-4f97-ab64-94faf88d9e6d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1 Создание DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74a8a2d",
   "metadata": {},
   "source": [
    "### createDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd693919",
   "metadata": {},
   "source": [
    "#### From List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc8e4c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdata1\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\u001b[32m\"1\"\u001b[39m, \u001b[32m\"Spark\"\u001b[39m),\n",
       "  (\u001b[32m\"2\"\u001b[39m, \u001b[32m\"Scala\"\u001b[39m),\n",
       "  (\u001b[32m\"3\"\u001b[39m, \u001b[32m\"Java\"\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data1 = Seq((\"1\", \"Spark\"), (\"2\", \"Scala\"), (\"3\", \"Java\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "607ab09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 17:18:33 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/09/01 17:18:33 INFO SharedState: Warehouse path is 'file:/home/vadim/workspace/Spark/DataFrame/spark-warehouse'.\n",
      "24/09/01 17:18:35 INFO CodeGenerator: Code generated in 179.713495 ms\n",
      "24/09/01 17:18:35 INFO CodeGenerator: Code generated in 18.241412 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| _1|   _2|\n",
      "+---+-----+\n",
      "|  1|Spark|\n",
      "|  2|Scala|\n",
      "|  3| Java|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_1: string, _2: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1 = spark.createDataFrame(data1)\n",
    "df1.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffde9fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|StudentID|Course|\n",
      "+---------+------+\n",
      "|        1| Spark|\n",
      "|        2| Scala|\n",
      "|        3|  Java|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf1WithNames\u001b[39m: \u001b[32mDataFrame\u001b[39m = [StudentID: string, Course: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df1WithNames = df1.withColumnsRenamed(Map(\"_1\" -> \"StudentID\", \"_2\" -> \"Course\"))\n",
    "df1WithNames.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b603ce90",
   "metadata": {},
   "source": [
    "##### withSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7b8e03cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mschema1\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"StudentID\"\u001b[39m, StringType, \u001b[32mfalse\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Course\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {})\n",
       ")\n",
       "\u001b[36mschema2\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"StudentID\"\u001b[39m, StringType, \u001b[32mfalse\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Course\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {})\n",
       ")\n",
       "\u001b[36mschema3\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"StudentID\"\u001b[39m, StringType, \u001b[32mfalse\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Course\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {})\n",
       ")\n",
       "\u001b[36mres30_3\u001b[39m: \u001b[32mBoolean\u001b[39m = \u001b[32mtrue\u001b[39m\n",
       "\u001b[36mres30_4\u001b[39m: \u001b[32mBoolean\u001b[39m = \u001b[32mtrue\u001b[39m"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema1 = StructType(Array(StructField(\"StudentID\", StringType, false),\n",
    "                              StructField(\"Course\", StringType, true)))\n",
    "\n",
    "val schema2 = new StructType()\n",
    "                    .add(StructField(\"StudentID\", StringType, false))\n",
    "                    .add(StructField(\"Course\", StringType, true))\n",
    "\n",
    "val schema3 = StructType(\n",
    "                StructField(\"StudentID\", StringType, false) :: \n",
    "                StructField(\"Course\", StringType, true) :: \n",
    "                Nil)\n",
    "\n",
    "schema1.equals(schema2)\n",
    "schema2.equals(schema3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c308052f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|StudentID|Course|\n",
      "+---------+------+\n",
      "|        1| Spark|\n",
      "|        2| Scala|\n",
      "|        3|  Java|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mcollection.JavaConverters._\u001b[39m\n",
       "\u001b[36mdata2\u001b[39m: \u001b[32mjava\u001b[39m.\u001b[32mutil\u001b[39m.\u001b[32mList\u001b[39m[\u001b[32mRow\u001b[39m] = \u001b[33mSeqWrapper\u001b[39m(\u001b[33mList\u001b[39m([1,Spark], [2,Scala], [3,Java]))\n",
       "\u001b[36mdf2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [StudentID: string, Course: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collection.JavaConverters._\n",
    "\n",
    "val data2 = data1.map(s => Row(s._1, s._2)).asJava\n",
    "val df2 = spark.createDataFrame(data2, schema2)\n",
    "df2.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd336b",
   "metadata": {},
   "source": [
    "#### fromRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb8852af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 17:22:51 INFO SparkContext: Starting job: show at cell17.sc:3\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Got job 3 (show at cell17.sc:3) with 1 output partitions\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Final stage: ResultStage 3 (show at cell17.sc:3)\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[7] at show at cell17.sc:3), which has no missing parents\n",
      "24/09/01 17:22:51 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:22:51 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 4.5 GiB)\n",
      "24/09/01 17:22:51 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ubuntu:36981 (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:22:51 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at show at cell17.sc:3) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 17:22:51 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "24/09/01 17:22:51 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 8) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/01 17:22:51 INFO Executor: Running task 0.0 in stage 3.0 (TID 8)\n",
      "24/09/01 17:22:51 INFO Executor: Finished task 0.0 in stage 3.0 (TID 8). 1409 bytes result sent to driver\n",
      "24/09/01 17:22:51 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 8) in 16 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 17:22:51 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:22:51 INFO DAGScheduler: ResultStage 3 (show at cell17.sc:3) finished in 0,029 s\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:22:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Job 3 finished: show at cell17.sc:3, took 0,040015 s\n",
      "24/09/01 17:22:51 INFO SparkContext: Starting job: show at cell17.sc:3\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Got job 4 (show at cell17.sc:3) with 4 output partitions\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Final stage: ResultStage 4 (show at cell17.sc:3)\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[7] at show at cell17.sc:3), which has no missing parents\n",
      "24/09/01 17:22:51 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:22:51 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 4.5 GiB)\n",
      "24/09/01 17:22:51 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ubuntu:36981 (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:22:51 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 4 (MapPartitionsRDD[7] at show at cell17.sc:3) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\n",
      "24/09/01 17:22:51 INFO TaskSchedulerImpl: Adding task set 4.0 with 4 tasks resource profile 0\n",
      "24/09/01 17:22:51 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 9) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/01 17:22:51 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 10) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/01 17:22:51 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 11) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/01 17:22:51 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 12) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/01 17:22:51 INFO Executor: Running task 0.0 in stage 4.0 (TID 9)\n",
      "24/09/01 17:22:51 INFO Executor: Running task 1.0 in stage 4.0 (TID 10)\n",
      "24/09/01 17:22:51 INFO Executor: Running task 2.0 in stage 4.0 (TID 11)\n",
      "24/09/01 17:22:51 INFO Executor: Running task 3.0 in stage 4.0 (TID 12)\n",
      "24/09/01 17:22:51 INFO Executor: Finished task 2.0 in stage 4.0 (TID 11). 1409 bytes result sent to driver\n",
      "24/09/01 17:22:51 INFO Executor: Finished task 3.0 in stage 4.0 (TID 12). 1409 bytes result sent to driver\n",
      "24/09/01 17:22:51 INFO Executor: Finished task 1.0 in stage 4.0 (TID 10). 1447 bytes result sent to driver\n",
      "24/09/01 17:22:51 INFO Executor: Finished task 0.0 in stage 4.0 (TID 9). 1409 bytes result sent to driver\n",
      "24/09/01 17:22:51 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 11) in 23 ms on ubuntu (executor driver) (1/4)\n",
      "24/09/01 17:22:51 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 9) in 29 ms on ubuntu (executor driver) (2/4)\n",
      "24/09/01 17:22:51 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 12) in 26 ms on ubuntu (executor driver) (3/4)\n",
      "24/09/01 17:22:51 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 10) in 31 ms on ubuntu (executor driver) (4/4)\n",
      "24/09/01 17:22:51 INFO DAGScheduler: ResultStage 4 (show at cell17.sc:3) finished in 0,051 s\n",
      "24/09/01 17:22:51 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:22:51 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:22:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Job 4 finished: show at cell17.sc:3, took 0,062959 s\n",
      "24/09/01 17:22:51 INFO SparkContext: Starting job: show at cell17.sc:3\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Got job 5 (show at cell17.sc:3) with 3 output partitions\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Final stage: ResultStage 5 (show at cell17.sc:3)\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[7] at show at cell17.sc:3), which has no missing parents\n",
      "24/09/01 17:22:51 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:22:51 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 4.5 GiB)\n",
      "24/09/01 17:22:51 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on ubuntu:36981 (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:22:51 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 5 (MapPartitionsRDD[7] at show at cell17.sc:3) (first 15 tasks are for partitions Vector(5, 6, 7))\n",
      "24/09/01 17:22:51 INFO TaskSchedulerImpl: Adding task set 5.0 with 3 tasks resource profile 0\n",
      "24/09/01 17:22:51 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 13) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/01 17:22:51 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 14) (ubuntu, executor driver, partition 6, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/01 17:22:51 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 15) (ubuntu, executor driver, partition 7, PROCESS_LOCAL, 7667 bytes) \n",
      "24/09/01 17:22:51 INFO Executor: Running task 0.0 in stage 5.0 (TID 13)\n",
      "24/09/01 17:22:51 INFO Executor: Running task 1.0 in stage 5.0 (TID 14)\n",
      "24/09/01 17:22:51 INFO Executor: Running task 2.0 in stage 5.0 (TID 15)\n",
      "24/09/01 17:22:51 INFO Executor: Finished task 1.0 in stage 5.0 (TID 14). 1409 bytes result sent to driver\n",
      "24/09/01 17:22:51 INFO Executor: Finished task 2.0 in stage 5.0 (TID 15). 1447 bytes result sent to driver\n",
      "24/09/01 17:22:51 INFO Executor: Finished task 0.0 in stage 5.0 (TID 13). 1447 bytes result sent to driver\n",
      "24/09/01 17:22:51 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 15) in 12 ms on ubuntu (executor driver) (1/3)\n",
      "24/09/01 17:22:51 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 14) in 13 ms on ubuntu (executor driver) (2/3)\n",
      "24/09/01 17:22:51 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 13) in 22 ms on ubuntu (executor driver) (3/3)\n",
      "24/09/01 17:22:51 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:22:51 INFO DAGScheduler: ResultStage 5 (show at cell17.sc:3) finished in 0,067 s\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:22:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "24/09/01 17:22:51 INFO DAGScheduler: Job 5 finished: show at cell17.sc:3, took 0,094685 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| _1|   _2|\n",
      "+---+-----+\n",
      "|  1|Spark|\n",
      "|  2|Scala|\n",
      "|  3| Java|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd1\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = ParallelCollectionRDD[4] at parallelize at cell17.sc:1\n",
       "\u001b[36mfromRDD1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_1: string, _2: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = spark.sparkContext.parallelize(data1)\n",
    "val fromRDD1 = spark.createDataFrame(rdd1)\n",
    "fromRDD1.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e85415",
   "metadata": {},
   "source": [
    "##### withSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4d1e502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 17:23:48 INFO CodeGenerator: Code generated in 9.94827 ms\n",
      "24/09/01 17:23:48 INFO SparkContext: Starting job: show at cell19.sc:3\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Got job 6 (show at cell19.sc:3) with 1 output partitions\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Final stage: ResultStage 6 (show at cell19.sc:3)\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[12] at show at cell19.sc:3), which has no missing parents\n",
      "24/09/01 17:23:48 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 21.4 KiB, free 4.5 GiB)\n",
      "24/09/01 17:23:48 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 4.5 GiB)\n",
      "24/09/01 17:23:48 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on ubuntu:36981 (size: 9.8 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:23:48 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[12] at show at cell19.sc:3) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 17:23:48 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "24/09/01 17:23:48 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 16) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/01 17:23:48 INFO Executor: Running task 0.0 in stage 6.0 (TID 16)\n",
      "24/09/01 17:23:48 INFO CodeGenerator: Code generated in 12.853195 ms\n",
      "24/09/01 17:23:48 INFO Executor: Finished task 0.0 in stage 6.0 (TID 16). 1409 bytes result sent to driver\n",
      "24/09/01 17:23:48 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 16) in 49 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 17:23:48 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:23:48 INFO DAGScheduler: ResultStage 6 (show at cell19.sc:3) finished in 0,130 s\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:23:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Job 6 finished: show at cell19.sc:3, took 0,138783 s\n",
      "24/09/01 17:23:48 INFO SparkContext: Starting job: show at cell19.sc:3\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Got job 7 (show at cell19.sc:3) with 4 output partitions\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Final stage: ResultStage 7 (show at cell19.sc:3)\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[12] at show at cell19.sc:3), which has no missing parents\n",
      "24/09/01 17:23:48 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 21.4 KiB, free 4.5 GiB)\n",
      "24/09/01 17:23:48 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 4.5 GiB)\n",
      "24/09/01 17:23:48 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on ubuntu:36981 (size: 9.8 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:23:48 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 7 (MapPartitionsRDD[12] at show at cell19.sc:3) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\n",
      "24/09/01 17:23:48 INFO TaskSchedulerImpl: Adding task set 7.0 with 4 tasks resource profile 0\n",
      "24/09/01 17:23:48 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 17) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/01 17:23:48 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 18) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/01 17:23:48 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 19) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/01 17:23:48 INFO TaskSetManager: Starting task 3.0 in stage 7.0 (TID 20) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/01 17:23:48 INFO Executor: Running task 0.0 in stage 7.0 (TID 17)\n",
      "24/09/01 17:23:48 INFO Executor: Running task 3.0 in stage 7.0 (TID 20)\n",
      "24/09/01 17:23:48 INFO Executor: Running task 2.0 in stage 7.0 (TID 19)\n",
      "24/09/01 17:23:48 INFO Executor: Running task 1.0 in stage 7.0 (TID 18)\n",
      "24/09/01 17:23:48 INFO Executor: Finished task 2.0 in stage 7.0 (TID 19). 1409 bytes result sent to driver\n",
      "24/09/01 17:23:48 INFO Executor: Finished task 0.0 in stage 7.0 (TID 17). 1409 bytes result sent to driver\n",
      "24/09/01 17:23:48 INFO Executor: Finished task 3.0 in stage 7.0 (TID 20). 1409 bytes result sent to driver\n",
      "24/09/01 17:23:48 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 17) in 38 ms on ubuntu (executor driver) (1/4)\n",
      "24/09/01 17:23:48 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 19) in 44 ms on ubuntu (executor driver) (2/4)\n",
      "24/09/01 17:23:48 INFO TaskSetManager: Finished task 3.0 in stage 7.0 (TID 20) in 48 ms on ubuntu (executor driver) (3/4)\n",
      "24/09/01 17:23:48 INFO CodeGenerator: Code generated in 58.48726 ms\n",
      "24/09/01 17:23:48 INFO Executor: Finished task 1.0 in stage 7.0 (TID 18). 1447 bytes result sent to driver\n",
      "24/09/01 17:23:48 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 18) in 108 ms on ubuntu (executor driver) (4/4)\n",
      "24/09/01 17:23:48 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:23:48 INFO DAGScheduler: ResultStage 7 (show at cell19.sc:3) finished in 0,138 s\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:23:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Job 7 finished: show at cell19.sc:3, took 0,150081 s\n",
      "24/09/01 17:23:48 INFO SparkContext: Starting job: show at cell19.sc:3\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Got job 8 (show at cell19.sc:3) with 3 output partitions\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Final stage: ResultStage 8 (show at cell19.sc:3)\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[12] at show at cell19.sc:3), which has no missing parents\n",
      "24/09/01 17:23:48 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 21.4 KiB, free 4.5 GiB)\n",
      "24/09/01 17:23:48 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 4.5 GiB)\n",
      "24/09/01 17:23:48 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on ubuntu:36981 (size: 9.8 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:23:48 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 8 (MapPartitionsRDD[12] at show at cell19.sc:3) (first 15 tasks are for partitions Vector(5, 6, 7))\n",
      "24/09/01 17:23:48 INFO TaskSchedulerImpl: Adding task set 8.0 with 3 tasks resource profile 0\n",
      "24/09/01 17:23:48 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 21) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/01 17:23:48 INFO TaskSetManager: Starting task 1.0 in stage 8.0 (TID 22) (ubuntu, executor driver, partition 6, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/01 17:23:48 INFO TaskSetManager: Starting task 2.0 in stage 8.0 (TID 23) (ubuntu, executor driver, partition 7, PROCESS_LOCAL, 7667 bytes) \n",
      "24/09/01 17:23:48 INFO Executor: Running task 2.0 in stage 8.0 (TID 23)\n",
      "24/09/01 17:23:48 INFO Executor: Running task 0.0 in stage 8.0 (TID 21)\n",
      "24/09/01 17:23:48 INFO Executor: Running task 1.0 in stage 8.0 (TID 22)\n",
      "24/09/01 17:23:48 INFO Executor: Finished task 0.0 in stage 8.0 (TID 21). 1447 bytes result sent to driver\n",
      "24/09/01 17:23:48 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 21) in 38 ms on ubuntu (executor driver) (1/3)\n",
      "24/09/01 17:23:48 INFO Executor: Finished task 1.0 in stage 8.0 (TID 22). 1409 bytes result sent to driver\n",
      "24/09/01 17:23:48 INFO TaskSetManager: Finished task 1.0 in stage 8.0 (TID 22) in 38 ms on ubuntu (executor driver) (2/3)\n",
      "24/09/01 17:23:48 INFO Executor: Finished task 2.0 in stage 8.0 (TID 23). 1447 bytes result sent to driver\n",
      "24/09/01 17:23:48 INFO TaskSetManager: Finished task 2.0 in stage 8.0 (TID 23) in 36 ms on ubuntu (executor driver) (3/3)\n",
      "24/09/01 17:23:48 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:23:48 INFO DAGScheduler: ResultStage 8 (show at cell19.sc:3) finished in 0,068 s\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:23:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "24/09/01 17:23:48 INFO DAGScheduler: Job 8 finished: show at cell19.sc:3, took 0,079662 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|StudentID|Course|\n",
      "+---------+------+\n",
      "|        1| Spark|\n",
      "|        2| Scala|\n",
      "|        3|  Java|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd2\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mRow\u001b[39m] = MapPartitionsRDD[9] at map at cell19.sc:1\n",
       "\u001b[36mfromRDD2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [StudentID: string, Course: string]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd2 = rdd1.map(s => Row(s._1, s._2))\n",
    "val fromRDD2 = spark.createDataFrame(rdd2, schema2)\n",
    "fromRDD2.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f7656",
   "metadata": {},
   "source": [
    "### toDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d021abd",
   "metadata": {},
   "source": [
    "#### From List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "010978b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 17:29:36 INFO CodeGenerator: Code generated in 12.90181 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| _1|   _2|\n",
      "+---+-----+\n",
      "|  1|Spark|\n",
      "|  2|Scala|\n",
      "|  3| Java|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf11\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_1: string, _2: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df11 = data1.toDF()\n",
    "df11.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b56c3330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|StudentID|Course|\n",
      "+---------+------+\n",
      "|        1| Spark|\n",
      "|        2| Scala|\n",
      "|        3|  Java|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mcolumns\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m\"StudentID\"\u001b[39m, \u001b[32m\"Course\"\u001b[39m)\n",
       "\u001b[36mdf12\u001b[39m: \u001b[32mDataFrame\u001b[39m = [StudentID: string, Course: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val columns = List(\"StudentID\", \"Course\")\n",
    "val df12 = data1.toDF(columns: _*)\n",
    "df12.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f508c0da",
   "metadata": {},
   "source": [
    "#### fromRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14ca80f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 17:36:44 INFO SparkContext: Starting job: show at cell23.sc:2\n",
      "24/09/01 17:36:44 INFO DAGScheduler: Got job 12 (show at cell23.sc:2) with 1 output partitions\n",
      "24/09/01 17:36:44 INFO DAGScheduler: Final stage: ResultStage 12 (show at cell23.sc:2)\n",
      "24/09/01 17:36:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:36:44 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:36:44 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[18] at show at cell23.sc:2), which has no missing parents\n",
      "24/09/01 17:36:44 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:36:44 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 4.5 GiB)\n",
      "24/09/01 17:36:44 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on ubuntu:36981 (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:36:44 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:36:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[18] at show at cell23.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 17:36:44 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
      "24/09/01 17:36:44 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 32) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/01 17:36:44 INFO Executor: Running task 0.0 in stage 12.0 (TID 32)\n",
      "24/09/01 17:36:44 INFO Executor: Finished task 0.0 in stage 12.0 (TID 32). 1366 bytes result sent to driver\n",
      "24/09/01 17:36:44 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 32) in 13 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 17:36:44 INFO DAGScheduler: ResultStage 12 (show at cell23.sc:2) finished in 0,025 s\n",
      "24/09/01 17:36:44 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:36:44 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:36:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
      "24/09/01 17:36:44 INFO DAGScheduler: Job 12 finished: show at cell23.sc:2, took 0,033085 s\n",
      "24/09/01 17:36:44 INFO SparkContext: Starting job: show at cell23.sc:2\n",
      "24/09/01 17:36:44 INFO DAGScheduler: Got job 13 (show at cell23.sc:2) with 4 output partitions\n",
      "24/09/01 17:36:44 INFO DAGScheduler: Final stage: ResultStage 13 (show at cell23.sc:2)\n",
      "24/09/01 17:36:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:36:44 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:36:44 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[18] at show at cell23.sc:2), which has no missing parents\n",
      "24/09/01 17:36:44 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:36:44 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 4.5 GiB)\n",
      "24/09/01 17:36:44 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on ubuntu:36981 (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:36:44 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:36:44 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 13 (MapPartitionsRDD[18] at show at cell23.sc:2) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\n",
      "24/09/01 17:36:44 INFO TaskSchedulerImpl: Adding task set 13.0 with 4 tasks resource profile 0\n",
      "24/09/01 17:36:44 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 33) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/01 17:36:44 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 34) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/01 17:36:44 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 35) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/01 17:36:44 INFO TaskSetManager: Starting task 3.0 in stage 13.0 (TID 36) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/01 17:36:44 INFO Executor: Running task 0.0 in stage 13.0 (TID 33)\n",
      "24/09/01 17:36:44 INFO Executor: Running task 2.0 in stage 13.0 (TID 35)\n",
      "24/09/01 17:36:44 INFO BlockManagerInfo: Removed broadcast_9_piece0 on ubuntu:36981 in memory (size: 5.8 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:36:44 INFO Executor: Running task 3.0 in stage 13.0 (TID 36)\n",
      "24/09/01 17:36:44 INFO Executor: Running task 1.0 in stage 13.0 (TID 34)\n",
      "24/09/01 17:36:44 INFO Executor: Finished task 0.0 in stage 13.0 (TID 33). 1409 bytes result sent to driver\n",
      "24/09/01 17:36:44 INFO Executor: Finished task 2.0 in stage 13.0 (TID 35). 1409 bytes result sent to driver\n",
      "24/09/01 17:36:44 INFO Executor: Finished task 3.0 in stage 13.0 (TID 36). 1366 bytes result sent to driver\n",
      "24/09/01 17:36:44 INFO Executor: Finished task 1.0 in stage 13.0 (TID 34). 1404 bytes result sent to driver\n",
      "24/09/01 17:36:44 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 33) in 37 ms on ubuntu (executor driver) (1/4)\n",
      "24/09/01 17:36:44 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 35) in 17 ms on ubuntu (executor driver) (2/4)\n",
      "24/09/01 17:36:44 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 34) in 19 ms on ubuntu (executor driver) (3/4)\n",
      "24/09/01 17:36:44 INFO TaskSetManager: Finished task 3.0 in stage 13.0 (TID 36) in 19 ms on ubuntu (executor driver) (4/4)\n",
      "24/09/01 17:36:44 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:36:44 INFO BlockManagerInfo: Removed broadcast_11_piece0 on ubuntu:36981 in memory (size: 5.8 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:36:44 INFO DAGScheduler: ResultStage 13 (show at cell23.sc:2) finished in 0,059 s\n",
      "24/09/01 17:36:44 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:36:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
      "24/09/01 17:36:44 INFO DAGScheduler: Job 13 finished: show at cell23.sc:2, took 0,070235 s\n",
      "24/09/01 17:36:44 INFO BlockManagerInfo: Removed broadcast_12_piece0 on ubuntu:36981 in memory (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:36:44 INFO SparkContext: Starting job: show at cell23.sc:2\n",
      "24/09/01 17:36:44 INFO DAGScheduler: Got job 14 (show at cell23.sc:2) with 3 output partitions\n",
      "24/09/01 17:36:44 INFO DAGScheduler: Final stage: ResultStage 14 (show at cell23.sc:2)\n",
      "24/09/01 17:36:44 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:36:44 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:36:44 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[18] at show at cell23.sc:2), which has no missing parents\n",
      "24/09/01 17:36:44 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:36:44 INFO BlockManagerInfo: Removed broadcast_10_piece0 on ubuntu:36981 in memory (size: 5.8 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:36:44 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 4.5 GiB)\n",
      "24/09/01 17:36:44 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on ubuntu:36981 (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:36:44 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:36:44 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 14 (MapPartitionsRDD[18] at show at cell23.sc:2) (first 15 tasks are for partitions Vector(5, 6, 7))\n",
      "24/09/01 17:36:44 INFO TaskSchedulerImpl: Adding task set 14.0 with 3 tasks resource profile 0\n",
      "24/09/01 17:36:44 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 37) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/01 17:36:44 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 38) (ubuntu, executor driver, partition 6, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/01 17:36:44 INFO TaskSetManager: Starting task 2.0 in stage 14.0 (TID 39) (ubuntu, executor driver, partition 7, PROCESS_LOCAL, 7667 bytes) \n",
      "24/09/01 17:36:44 INFO Executor: Running task 0.0 in stage 14.0 (TID 37)\n",
      "24/09/01 17:36:44 INFO Executor: Running task 1.0 in stage 14.0 (TID 38)\n",
      "24/09/01 17:36:44 INFO Executor: Running task 2.0 in stage 14.0 (TID 39)\n",
      "24/09/01 17:36:44 INFO Executor: Finished task 0.0 in stage 14.0 (TID 37). 1404 bytes result sent to driver\n",
      "24/09/01 17:36:44 INFO Executor: Finished task 1.0 in stage 14.0 (TID 38). 1366 bytes result sent to driver\n",
      "24/09/01 17:36:45 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 38) in 25 ms on ubuntu (executor driver) (1/3)\n",
      "24/09/01 17:36:45 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 37) in 30 ms on ubuntu (executor driver) (2/3)\n",
      "24/09/01 17:36:45 INFO Executor: Finished task 2.0 in stage 14.0 (TID 39). 1447 bytes result sent to driver\n",
      "24/09/01 17:36:45 INFO TaskSetManager: Finished task 2.0 in stage 14.0 (TID 39) in 43 ms on ubuntu (executor driver) (3/3)\n",
      "24/09/01 17:36:45 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:36:45 INFO DAGScheduler: ResultStage 14 (show at cell23.sc:2) finished in 0,064 s\n",
      "24/09/01 17:36:45 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:36:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
      "24/09/01 17:36:45 INFO DAGScheduler: Job 14 finished: show at cell23.sc:2, took 0,076070 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| _1|   _2|\n",
      "+---+-----+\n",
      "|  1|Spark|\n",
      "|  2|Scala|\n",
      "|  3| Java|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfromRDD11\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_1: string, _2: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fromRDD11 = rdd1.toDF()\n",
    "fromRDD11.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02700001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 17:36:56 INFO SparkContext: Starting job: show at cell24.sc:2\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Got job 15 (show at cell24.sc:2) with 1 output partitions\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Final stage: ResultStage 15 (show at cell24.sc:2)\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[21] at show at cell24.sc:2), which has no missing parents\n",
      "24/09/01 17:36:56 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:36:56 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 4.5 GiB)\n",
      "24/09/01 17:36:56 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on ubuntu:36981 (size: 5.8 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:36:56 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[21] at show at cell24.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 17:36:56 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
      "24/09/01 17:36:56 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 40) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/01 17:36:56 INFO Executor: Running task 0.0 in stage 15.0 (TID 40)\n",
      "24/09/01 17:36:56 INFO Executor: Finished task 0.0 in stage 15.0 (TID 40). 1409 bytes result sent to driver\n",
      "24/09/01 17:36:56 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 40) in 14 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 17:36:56 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:36:56 INFO DAGScheduler: ResultStage 15 (show at cell24.sc:2) finished in 0,049 s\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:36:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Job 15 finished: show at cell24.sc:2, took 0,061473 s\n",
      "24/09/01 17:36:56 INFO SparkContext: Starting job: show at cell24.sc:2\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Got job 16 (show at cell24.sc:2) with 4 output partitions\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Final stage: ResultStage 16 (show at cell24.sc:2)\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[21] at show at cell24.sc:2), which has no missing parents\n",
      "24/09/01 17:36:56 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:36:56 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 4.5 GiB)\n",
      "24/09/01 17:36:56 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on ubuntu:36981 (size: 5.8 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:36:56 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 16 (MapPartitionsRDD[21] at show at cell24.sc:2) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\n",
      "24/09/01 17:36:56 INFO TaskSchedulerImpl: Adding task set 16.0 with 4 tasks resource profile 0\n",
      "24/09/01 17:36:56 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 41) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/01 17:36:56 INFO TaskSetManager: Starting task 1.0 in stage 16.0 (TID 42) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/01 17:36:56 INFO TaskSetManager: Starting task 2.0 in stage 16.0 (TID 43) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/01 17:36:56 INFO TaskSetManager: Starting task 3.0 in stage 16.0 (TID 44) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/01 17:36:56 INFO Executor: Running task 2.0 in stage 16.0 (TID 43)\n",
      "24/09/01 17:36:56 INFO Executor: Running task 1.0 in stage 16.0 (TID 42)\n",
      "24/09/01 17:36:56 INFO Executor: Running task 0.0 in stage 16.0 (TID 41)\n",
      "24/09/01 17:36:56 INFO Executor: Running task 3.0 in stage 16.0 (TID 44)\n",
      "24/09/01 17:36:56 INFO Executor: Finished task 2.0 in stage 16.0 (TID 43). 1366 bytes result sent to driver\n",
      "24/09/01 17:36:56 INFO TaskSetManager: Finished task 2.0 in stage 16.0 (TID 43) in 9 ms on ubuntu (executor driver) (1/4)\n",
      "24/09/01 17:36:56 INFO Executor: Finished task 0.0 in stage 16.0 (TID 41). 1409 bytes result sent to driver\n",
      "24/09/01 17:36:56 INFO Executor: Finished task 3.0 in stage 16.0 (TID 44). 1366 bytes result sent to driver\n",
      "24/09/01 17:36:56 INFO Executor: Finished task 1.0 in stage 16.0 (TID 42). 1404 bytes result sent to driver\n",
      "24/09/01 17:36:56 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 41) in 17 ms on ubuntu (executor driver) (2/4)\n",
      "24/09/01 17:36:56 INFO TaskSetManager: Finished task 1.0 in stage 16.0 (TID 42) in 25 ms on ubuntu (executor driver) (3/4)\n",
      "24/09/01 17:36:56 INFO TaskSetManager: Finished task 3.0 in stage 16.0 (TID 44) in 24 ms on ubuntu (executor driver) (4/4)\n",
      "24/09/01 17:36:56 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:36:56 INFO DAGScheduler: ResultStage 16 (show at cell24.sc:2) finished in 0,050 s\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:36:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Job 16 finished: show at cell24.sc:2, took 0,060822 s\n",
      "24/09/01 17:36:56 INFO SparkContext: Starting job: show at cell24.sc:2\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Got job 17 (show at cell24.sc:2) with 3 output partitions\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Final stage: ResultStage 17 (show at cell24.sc:2)\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[21] at show at cell24.sc:2), which has no missing parents\n",
      "24/09/01 17:36:56 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:36:56 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 4.5 GiB)\n",
      "24/09/01 17:36:56 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on ubuntu:36981 (size: 5.8 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:36:56 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 17 (MapPartitionsRDD[21] at show at cell24.sc:2) (first 15 tasks are for partitions Vector(5, 6, 7))\n",
      "24/09/01 17:36:56 INFO TaskSchedulerImpl: Adding task set 17.0 with 3 tasks resource profile 0\n",
      "24/09/01 17:36:56 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 45) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7668 bytes) \n",
      "24/09/01 17:36:56 INFO TaskSetManager: Starting task 1.0 in stage 17.0 (TID 46) (ubuntu, executor driver, partition 6, PROCESS_LOCAL, 7607 bytes) \n",
      "24/09/01 17:36:56 INFO TaskSetManager: Starting task 2.0 in stage 17.0 (TID 47) (ubuntu, executor driver, partition 7, PROCESS_LOCAL, 7667 bytes) \n",
      "24/09/01 17:36:56 INFO Executor: Running task 0.0 in stage 17.0 (TID 45)\n",
      "24/09/01 17:36:56 INFO Executor: Running task 1.0 in stage 17.0 (TID 46)\n",
      "24/09/01 17:36:56 INFO Executor: Finished task 1.0 in stage 17.0 (TID 46). 1366 bytes result sent to driver\n",
      "24/09/01 17:36:56 INFO Executor: Running task 2.0 in stage 17.0 (TID 47)\n",
      "24/09/01 17:36:56 INFO Executor: Finished task 0.0 in stage 17.0 (TID 45). 1404 bytes result sent to driver\n",
      "24/09/01 17:36:56 INFO Executor: Finished task 2.0 in stage 17.0 (TID 47). 1404 bytes result sent to driver\n",
      "24/09/01 17:36:56 INFO TaskSetManager: Finished task 1.0 in stage 17.0 (TID 46) in 22 ms on ubuntu (executor driver) (1/3)\n",
      "24/09/01 17:36:56 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 45) in 30 ms on ubuntu (executor driver) (2/3)\n",
      "24/09/01 17:36:56 INFO TaskSetManager: Finished task 2.0 in stage 17.0 (TID 47) in 26 ms on ubuntu (executor driver) (3/3)\n",
      "24/09/01 17:36:56 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:36:56 INFO DAGScheduler: ResultStage 17 (show at cell24.sc:2) finished in 0,048 s\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:36:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished\n",
      "24/09/01 17:36:56 INFO DAGScheduler: Job 17 finished: show at cell24.sc:2, took 0,061180 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|StudentID|Course|\n",
      "+---------+------+\n",
      "|        1| Spark|\n",
      "|        2| Scala|\n",
      "|        3|  Java|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfromRDD12\u001b[39m: \u001b[32mDataFrame\u001b[39m = [StudentID: string, Course: string]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fromRDD12 = rdd1.toDF(columns: _*)\n",
    "fromRDD12.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8949d910",
   "metadata": {},
   "source": [
    "### fromFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24814ffa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 17:40:02 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "24/09/01 17:40:02 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "24/09/01 17:40:02 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/01 17:40:02 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/01 17:40:02 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 198.5 KiB, free 4.5 GiB)\n",
      "24/09/01 17:40:02 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 4.5 GiB)\n",
      "24/09/01 17:40:02 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on ubuntu:36981 (size: 33.9 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:40:02 INFO SparkContext: Created broadcast 22 from load at cell28.sc:1\n",
      "24/09/01 17:40:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/01 17:40:02 INFO SparkContext: Starting job: load at cell28.sc:1\n",
      "24/09/01 17:40:02 INFO DAGScheduler: Got job 20 (load at cell28.sc:1) with 1 output partitions\n",
      "24/09/01 17:40:02 INFO DAGScheduler: Final stage: ResultStage 20 (load at cell28.sc:1)\n",
      "24/09/01 17:40:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:40:02 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:40:02 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[33] at load at cell28.sc:1), which has no missing parents\n",
      "24/09/01 17:40:02 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 15.8 KiB, free 4.5 GiB)\n",
      "24/09/01 17:40:02 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 4.5 GiB)\n",
      "24/09/01 17:40:02 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on ubuntu:36981 (size: 7.4 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:40:02 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:40:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[33] at load at cell28.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 17:40:02 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0\n",
      "24/09/01 17:40:02 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 50) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/01 17:40:02 INFO Executor: Running task 0.0 in stage 20.0 (TID 50)\n",
      "24/09/01 17:40:02 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/01 17:40:02 INFO Executor: Finished task 0.0 in stage 20.0 (TID 50). 2076 bytes result sent to driver\n",
      "24/09/01 17:40:02 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 50) in 72 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 17:40:02 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:40:02 INFO DAGScheduler: ResultStage 20 (load at cell28.sc:1) finished in 0,093 s\n",
      "24/09/01 17:40:02 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:40:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished\n",
      "24/09/01 17:40:02 INFO DAGScheduler: Job 20 finished: load at cell28.sc:1, took 0,101186 s\n",
      "24/09/01 17:40:02 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/01 17:40:02 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/01 17:40:03 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/01 17:40:03 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:40:03 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on ubuntu:36981 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:40:03 INFO SparkContext: Created broadcast 24 from show at cell28.sc:2\n",
      "24/09/01 17:40:03 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/01 17:40:03 INFO SparkContext: Starting job: show at cell28.sc:2\n",
      "24/09/01 17:40:03 INFO DAGScheduler: Got job 21 (show at cell28.sc:2) with 1 output partitions\n",
      "24/09/01 17:40:03 INFO DAGScheduler: Final stage: ResultStage 21 (show at cell28.sc:2)\n",
      "24/09/01 17:40:03 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:40:03 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:40:03 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[37] at show at cell28.sc:2), which has no missing parents\n",
      "24/09/01 17:40:03 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 17.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:40:03 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 4.5 GiB)\n",
      "24/09/01 17:40:03 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on ubuntu:36981 (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:40:03 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:40:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[37] at show at cell28.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 17:40:03 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0\n",
      "24/09/01 17:40:03 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 51) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/01 17:40:03 INFO Executor: Running task 0.0 in stage 21.0 (TID 51)\n",
      "24/09/01 17:40:03 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/01 17:40:03 INFO Executor: Finished task 0.0 in stage 21.0 (TID 51). 2480 bytes result sent to driver\n",
      "24/09/01 17:40:03 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 51) in 19 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 17:40:03 INFO DAGScheduler: ResultStage 21 (show at cell28.sc:2) finished in 0,035 s\n",
      "24/09/01 17:40:03 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:40:03 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:40:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished\n",
      "24/09/01 17:40:03 INFO DAGScheduler: Job 21 finished: show at cell28.sc:2, took 0,043182 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "|Address                                               |Birthdate          |Country       |CustomerID|Email                   |Name           |Username        |\n",
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "|Unit 1047 Box 4089\\nDPO AA 57348                      |1994-02-20 00:46:27|United Kingdom|12346     |cooperalexis@hotmail.com|Lindsay Cowan  |valenciajennifer|\n",
      "|55711 Janet Plaza Apt. 865\\nChristinachester, CT 62716|1988-06-21 00:15:34|Iceland       |12347     |timothy78@hotmail.com   |Katherine David|hillrachel      |\n",
      "|Unit 2676 Box 9352\\nDPO AA 38560                      |1974-11-26 15:30:20|Finland       |12348     |tcrawford@gmail.com     |Leslie Martinez|serranobrian    |\n",
      "|2765 Powers Meadow\\nHeatherfurt, CT 53165             |1977-05-06 23:57:35|Italy         |12349     |dustin37@yahoo.com      |Brad Cardenas  |charleshudson   |\n",
      "|17677 Mark Crest\\nWalterberg, IA 39017                |1996-09-13 19:14:27|Norway        |12350     |amyholland@yahoo.com    |Natalie Ford   |gregoryharrison |\n",
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfromFile1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Address: string, Birthdate: string ... 5 more fields]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fromFile1 = spark.read.format(\"json\").load(\"data/customer_data.json\")\n",
    "fromFile1.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dbd6f48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mfileSchema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Address\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Birthdate\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Country\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CustomerID\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Email\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Name\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Username\"\u001b[39m, StringType, \u001b[32mtrue\u001b[39m, {})\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fileSchema = StructType(\n",
    "                    StructField(\"Address\", StringType, true) ::\n",
    "                    StructField(\"Birthdate\", StringType, true) ::\n",
    "                    StructField(\"Country\", StringType, true) ::\n",
    "                    StructField(\"CustomerID\", StringType, true) ::\n",
    "                    StructField(\"Email\", StringType, true) ::\n",
    "                    StructField(\"Name\", StringType, true) ::\n",
    "                    StructField(\"Username\", StringType, true) ::\n",
    "                    Nil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d51c6e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 17:43:57 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "24/09/01 17:43:57 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/01 17:43:57 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/01 17:43:57 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/01 17:43:57 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:43:57 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on ubuntu:36981 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:43:57 INFO SparkContext: Created broadcast 26 from show at cell32.sc:2\n",
      "24/09/01 17:43:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/01 17:43:57 INFO SparkContext: Starting job: show at cell32.sc:2\n",
      "24/09/01 17:43:57 INFO DAGScheduler: Got job 22 (show at cell32.sc:2) with 1 output partitions\n",
      "24/09/01 17:43:57 INFO DAGScheduler: Final stage: ResultStage 22 (show at cell32.sc:2)\n",
      "24/09/01 17:43:57 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:43:57 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:43:57 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[41] at show at cell32.sc:2), which has no missing parents\n",
      "24/09/01 17:43:57 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 17.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:43:57 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 4.5 GiB)\n",
      "24/09/01 17:43:57 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on ubuntu:36981 (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:43:57 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:43:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[41] at show at cell32.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 17:43:57 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks resource profile 0\n",
      "24/09/01 17:43:57 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 52) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/01 17:43:57 INFO Executor: Running task 0.0 in stage 22.0 (TID 52)\n",
      "24/09/01 17:43:57 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/01 17:43:57 INFO Executor: Finished task 0.0 in stage 22.0 (TID 52). 2480 bytes result sent to driver\n",
      "24/09/01 17:43:57 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 52) in 23 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 17:43:57 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:43:57 INFO DAGScheduler: ResultStage 22 (show at cell32.sc:2) finished in 0,053 s\n",
      "24/09/01 17:43:57 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:43:57 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished\n",
      "24/09/01 17:43:57 INFO DAGScheduler: Job 22 finished: show at cell32.sc:2, took 0,070150 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "|Address                                               |Birthdate          |Country       |CustomerID|Email                   |Name           |Username        |\n",
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "|Unit 1047 Box 4089\\nDPO AA 57348                      |1994-02-20 00:46:27|United Kingdom|12346     |cooperalexis@hotmail.com|Lindsay Cowan  |valenciajennifer|\n",
      "|55711 Janet Plaza Apt. 865\\nChristinachester, CT 62716|1988-06-21 00:15:34|Iceland       |12347     |timothy78@hotmail.com   |Katherine David|hillrachel      |\n",
      "|Unit 2676 Box 9352\\nDPO AA 38560                      |1974-11-26 15:30:20|Finland       |12348     |tcrawford@gmail.com     |Leslie Martinez|serranobrian    |\n",
      "|2765 Powers Meadow\\nHeatherfurt, CT 53165             |1977-05-06 23:57:35|Italy         |12349     |dustin37@yahoo.com      |Brad Cardenas  |charleshudson   |\n",
      "|17677 Mark Crest\\nWalterberg, IA 39017                |1996-09-13 19:14:27|Norway        |12350     |amyholland@yahoo.com    |Natalie Ford   |gregoryharrison |\n",
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfromFile2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Address: string, Birthdate: string ... 5 more fields]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fromFile2 = spark.read.format(\"json\").schema(fileSchema).load(\"data/customer_data.json\")\n",
    "fromFile2.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab7d210",
   "metadata": {},
   "source": [
    "## 2 Основные операции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "067a8858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcustomerDf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Address: string, Birthdate: string ... 5 more fields]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val customerDf = fromFile1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c371153c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Birthdate: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d7245a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres36\u001b[39m: \u001b[32mArray\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mArray\u001b[39m(\n",
       "  (\u001b[32m\"Address\"\u001b[39m, \u001b[32m\"StringType\"\u001b[39m),\n",
       "  (\u001b[32m\"Birthdate\"\u001b[39m, \u001b[32m\"StringType\"\u001b[39m),\n",
       "  (\u001b[32m\"Country\"\u001b[39m, \u001b[32m\"StringType\"\u001b[39m),\n",
       "  (\u001b[32m\"CustomerID\"\u001b[39m, \u001b[32m\"StringType\"\u001b[39m),\n",
       "  (\u001b[32m\"Email\"\u001b[39m, \u001b[32m\"StringType\"\u001b[39m),\n",
       "  (\u001b[32m\"Name\"\u001b[39m, \u001b[32m\"StringType\"\u001b[39m),\n",
       "  (\u001b[32m\"Username\"\u001b[39m, \u001b[32m\"StringType\"\u001b[39m)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerDf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2cc65050",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 17:51:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/01 17:51:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/01 17:51:12 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/01 17:51:12 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:51:12 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on ubuntu:36981 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:51:12 INFO SparkContext: Created broadcast 28 from head at cell35.sc:1\n",
      "24/09/01 17:51:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/01 17:51:12 INFO SparkContext: Starting job: head at cell35.sc:1\n",
      "24/09/01 17:51:12 INFO DAGScheduler: Got job 23 (head at cell35.sc:1) with 1 output partitions\n",
      "24/09/01 17:51:12 INFO DAGScheduler: Final stage: ResultStage 23 (head at cell35.sc:1)\n",
      "24/09/01 17:51:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:51:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:51:12 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[44] at head at cell35.sc:1), which has no missing parents\n",
      "24/09/01 17:51:12 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 11.6 KiB, free 4.5 GiB)\n",
      "24/09/01 17:51:12 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:51:12 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on ubuntu:36981 (size: 6.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:51:12 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:51:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[44] at head at cell35.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 17:51:12 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks resource profile 0\n",
      "24/09/01 17:51:12 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 53) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/01 17:51:12 INFO Executor: Running task 0.0 in stage 23.0 (TID 53)\n",
      "24/09/01 17:51:12 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/01 17:51:12 INFO Executor: Finished task 0.0 in stage 23.0 (TID 53). 1623 bytes result sent to driver\n",
      "24/09/01 17:51:12 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 53) in 25 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 17:51:12 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:51:12 INFO DAGScheduler: ResultStage 23 (head at cell35.sc:1) finished in 0,038 s\n",
      "24/09/01 17:51:12 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:51:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 23: Stage finished\n",
      "24/09/01 17:51:12 INFO DAGScheduler: Job 23 finished: head at cell35.sc:1, took 0,046464 s\n",
      "24/09/01 17:51:12 INFO CodeGenerator: Code generated in 47.43397 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres35\u001b[39m: \u001b[32mRow\u001b[39m = [Unit 1047 Box 4089\n",
       "DPO AA 57348,1994-02-20 00:46:27,United Kingdom,12346,cooperalexis@hotmail.com,Lindsay Cowan,valenciajennifer]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerDf.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a9c61d",
   "metadata": {},
   "source": [
    "#### Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b78c5d8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 17:51:37 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/01 17:51:37 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/01 17:51:37 INFO CodeGenerator: Code generated in 7.820498 ms\n",
      "24/09/01 17:51:37 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/01 17:51:37 INFO BlockManagerInfo: Removed broadcast_28_piece0 on ubuntu:36981 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:51:37 INFO BlockManagerInfo: Removed broadcast_29_piece0 on ubuntu:36981 in memory (size: 6.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:51:37 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:51:37 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on ubuntu:36981 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:51:37 INFO SparkContext: Created broadcast 30 from show at cell37.sc:1\n",
      "24/09/01 17:51:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/01 17:51:37 INFO SparkContext: Starting job: show at cell37.sc:1\n",
      "24/09/01 17:51:37 INFO DAGScheduler: Got job 24 (show at cell37.sc:1) with 1 output partitions\n",
      "24/09/01 17:51:37 INFO DAGScheduler: Final stage: ResultStage 24 (show at cell37.sc:1)\n",
      "24/09/01 17:51:37 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:51:37 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:51:37 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[48] at show at cell37.sc:1), which has no missing parents\n",
      "24/09/01 17:51:37 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 14.7 KiB, free 4.5 GiB)\n",
      "24/09/01 17:51:37 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 4.5 GiB)\n",
      "24/09/01 17:51:37 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on ubuntu:36981 (size: 7.1 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:51:37 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:51:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[48] at show at cell37.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 17:51:37 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks resource profile 0\n",
      "24/09/01 17:51:37 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 54) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/01 17:51:37 INFO Executor: Running task 0.0 in stage 24.0 (TID 54)\n",
      "24/09/01 17:51:37 INFO CodeGenerator: Code generated in 13.9353 ms\n",
      "24/09/01 17:51:37 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/01 17:51:37 INFO CodeGenerator: Code generated in 10.561443 ms\n",
      "24/09/01 17:51:37 INFO Executor: Finished task 0.0 in stage 24.0 (TID 54). 2153 bytes result sent to driver\n",
      "24/09/01 17:51:37 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 54) in 57 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 17:51:37 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:51:37 INFO DAGScheduler: ResultStage 24 (show at cell37.sc:1) finished in 0,080 s\n",
      "24/09/01 17:51:37 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:51:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 24: Stage finished\n",
      "24/09/01 17:51:37 INFO DAGScheduler: Job 24 finished: show at cell37.sc:1, took 0,090265 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+\n",
      "|          Birthdate|       Country|\n",
      "+-------------------+--------------+\n",
      "|1994-02-20 00:46:27|United Kingdom|\n",
      "|1988-06-21 00:15:34|       Iceland|\n",
      "|1974-11-26 15:30:20|       Finland|\n",
      "|1977-05-06 23:57:35|         Italy|\n",
      "|1996-09-13 19:14:27|        Norway|\n",
      "|1969-06-21 03:39:20|        Norway|\n",
      "|1993-02-25 18:37:29|       Bahrain|\n",
      "|1993-03-13 12:37:34|         Spain|\n",
      "|1972-11-10 12:01:08|       Bahrain|\n",
      "|1973-01-13 17:17:26|      Portugal|\n",
      "|1989-11-24 17:12:54|   Switzerland|\n",
      "|1977-06-19 22:35:52|       Austria|\n",
      "|1983-09-21 05:22:18|        Cyprus|\n",
      "|1980-10-28 17:25:59|       Austria|\n",
      "|1982-09-01 09:12:57|       Belgium|\n",
      "|1979-02-03 03:42:47|       Belgium|\n",
      "|1974-12-21 13:27:20|   Unspecified|\n",
      "|1990-07-17 15:47:12|       Belgium|\n",
      "|1981-07-10 00:35:00|        Cyprus|\n",
      "|1989-12-26 00:58:01|       Denmark|\n",
      "+-------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.select(\"Birthdate\", \"Country\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1822538d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 17:51:45 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/01 17:51:45 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/01 17:51:45 INFO CodeGenerator: Code generated in 6.315174 ms\n",
      "24/09/01 17:51:45 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/01 17:51:45 INFO BlockManagerInfo: Removed broadcast_30_piece0 on ubuntu:36981 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:51:45 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:51:45 INFO BlockManagerInfo: Removed broadcast_31_piece0 on ubuntu:36981 in memory (size: 7.1 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:51:45 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on ubuntu:36981 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:51:45 INFO SparkContext: Created broadcast 32 from show at cell38.sc:1\n",
      "24/09/01 17:51:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/01 17:51:45 INFO SparkContext: Starting job: show at cell38.sc:1\n",
      "24/09/01 17:51:45 INFO DAGScheduler: Got job 25 (show at cell38.sc:1) with 1 output partitions\n",
      "24/09/01 17:51:45 INFO DAGScheduler: Final stage: ResultStage 25 (show at cell38.sc:1)\n",
      "24/09/01 17:51:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:51:45 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:51:45 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[52] at show at cell38.sc:1), which has no missing parents\n",
      "24/09/01 17:51:45 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 14.2 KiB, free 4.5 GiB)\n",
      "24/09/01 17:51:45 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:51:45 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on ubuntu:36981 (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:51:45 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:51:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[52] at show at cell38.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 17:51:45 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0\n",
      "24/09/01 17:51:45 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 55) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/01 17:51:45 INFO Executor: Running task 0.0 in stage 25.0 (TID 55)\n",
      "24/09/01 17:51:45 INFO CodeGenerator: Code generated in 11.726231 ms\n",
      "24/09/01 17:51:45 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/01 17:51:45 INFO Executor: Finished task 0.0 in stage 25.0 (TID 55). 1753 bytes result sent to driver\n",
      "24/09/01 17:51:45 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 55) in 30 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 17:51:45 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:51:45 INFO DAGScheduler: ResultStage 25 (show at cell38.sc:1) finished in 0,042 s\n",
      "24/09/01 17:51:45 INFO DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:51:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished\n",
      "24/09/01 17:51:45 INFO DAGScheduler: Job 25 finished: show at cell38.sc:1, took 0,051646 s\n",
      "24/09/01 17:51:45 INFO CodeGenerator: Code generated in 9.027179 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|       Country|\n",
      "+--------------+\n",
      "|United Kingdom|\n",
      "|       Iceland|\n",
      "|       Finland|\n",
      "|         Italy|\n",
      "|        Norway|\n",
      "|        Norway|\n",
      "|       Bahrain|\n",
      "|         Spain|\n",
      "|       Bahrain|\n",
      "|      Portugal|\n",
      "|   Switzerland|\n",
      "|       Austria|\n",
      "|        Cyprus|\n",
      "|       Austria|\n",
      "|       Belgium|\n",
      "|       Belgium|\n",
      "|   Unspecified|\n",
      "|       Belgium|\n",
      "|        Cyprus|\n",
      "|       Denmark|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.select(col(\"Country\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee0f2ed4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 17:52:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/01 17:52:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/01 17:52:12 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/01 17:52:12 INFO BlockManagerInfo: Removed broadcast_35_piece0 on ubuntu:36981 in memory (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:52:12 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:52:12 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on ubuntu:36981 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:52:12 INFO SparkContext: Created broadcast 36 from show at cell40.sc:1\n",
      "24/09/01 17:52:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/01 17:52:12 INFO BlockManagerInfo: Removed broadcast_34_piece0 on ubuntu:36981 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:52:12 INFO SparkContext: Starting job: show at cell40.sc:1\n",
      "24/09/01 17:52:12 INFO DAGScheduler: Got job 27 (show at cell40.sc:1) with 1 output partitions\n",
      "24/09/01 17:52:12 INFO DAGScheduler: Final stage: ResultStage 27 (show at cell40.sc:1)\n",
      "24/09/01 17:52:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:52:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:52:12 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[60] at show at cell40.sc:1), which has no missing parents\n",
      "24/09/01 17:52:12 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 14.2 KiB, free 4.5 GiB)\n",
      "24/09/01 17:52:12 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:52:12 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on ubuntu:36981 (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:52:12 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:52:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[60] at show at cell40.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 17:52:12 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0\n",
      "24/09/01 17:52:12 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 57) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/01 17:52:12 INFO Executor: Running task 0.0 in stage 27.0 (TID 57)\n",
      "24/09/01 17:52:12 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/01 17:52:13 INFO Executor: Finished task 0.0 in stage 27.0 (TID 57). 1899 bytes result sent to driver\n",
      "24/09/01 17:52:13 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 57) in 21 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 17:52:13 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:52:13 INFO DAGScheduler: ResultStage 27 (show at cell40.sc:1) finished in 0,038 s\n",
      "24/09/01 17:52:13 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:52:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished\n",
      "24/09/01 17:52:13 INFO DAGScheduler: Job 27 finished: show at cell40.sc:1, took 0,048260 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               Date|\n",
      "+-------------------+\n",
      "|1994-02-20 00:46:27|\n",
      "|1988-06-21 00:15:34|\n",
      "|1974-11-26 15:30:20|\n",
      "|1977-05-06 23:57:35|\n",
      "|1996-09-13 19:14:27|\n",
      "|1969-06-21 03:39:20|\n",
      "|1993-02-25 18:37:29|\n",
      "|1993-03-13 12:37:34|\n",
      "|1972-11-10 12:01:08|\n",
      "|1973-01-13 17:17:26|\n",
      "|1989-11-24 17:12:54|\n",
      "|1977-06-19 22:35:52|\n",
      "|1983-09-21 05:22:18|\n",
      "|1980-10-28 17:25:59|\n",
      "|1982-09-01 09:12:57|\n",
      "|1979-02-03 03:42:47|\n",
      "|1974-12-21 13:27:20|\n",
      "|1990-07-17 15:47:12|\n",
      "|1981-07-10 00:35:00|\n",
      "|1989-12-26 00:58:01|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.selectExpr(\"Birthdate as Date\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "250879d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 17:52:19 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/01 17:52:19 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/01 17:52:19 INFO CodeGenerator: Code generated in 11.2263 ms\n",
      "24/09/01 17:52:19 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/01 17:52:19 INFO BlockManagerInfo: Removed broadcast_36_piece0 on ubuntu:36981 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:52:19 INFO BlockManagerInfo: Removed broadcast_37_piece0 on ubuntu:36981 in memory (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:52:19 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:52:19 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on ubuntu:36981 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:52:19 INFO SparkContext: Created broadcast 38 from show at cell41.sc:1\n",
      "24/09/01 17:52:19 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/01 17:52:19 INFO SparkContext: Starting job: show at cell41.sc:1\n",
      "24/09/01 17:52:19 INFO DAGScheduler: Got job 28 (show at cell41.sc:1) with 1 output partitions\n",
      "24/09/01 17:52:19 INFO DAGScheduler: Final stage: ResultStage 28 (show at cell41.sc:1)\n",
      "24/09/01 17:52:19 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:52:19 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:52:19 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[64] at show at cell41.sc:1), which has no missing parents\n",
      "24/09/01 17:52:19 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 17.1 KiB, free 4.5 GiB)\n",
      "24/09/01 17:52:19 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 4.5 GiB)\n",
      "24/09/01 17:52:19 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on ubuntu:36981 (size: 7.7 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:52:19 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:52:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[64] at show at cell41.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 17:52:19 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0\n",
      "24/09/01 17:52:19 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 58) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/01 17:52:19 INFO Executor: Running task 0.0 in stage 28.0 (TID 58)\n",
      "24/09/01 17:52:19 INFO CodeGenerator: Code generated in 18.784689 ms\n",
      "24/09/01 17:52:19 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/01 17:52:19 INFO Executor: Finished task 0.0 in stage 28.0 (TID 58). 4695 bytes result sent to driver\n",
      "24/09/01 17:52:19 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 58) in 60 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 17:52:19 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:52:19 INFO DAGScheduler: ResultStage 28 (show at cell41.sc:1) finished in 0,079 s\n",
      "24/09/01 17:52:19 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:52:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished\n",
      "24/09/01 17:52:19 INFO DAGScheduler: Job 28 finished: show at cell41.sc:1, took 0,098938 s\n",
      "24/09/01 17:52:19 INFO CodeGenerator: Code generated in 30.956004 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+----+\n",
      "|             Address|          Birthdate|       Country|CustomerID|               Email|             Name|        Username|Flag|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+----+\n",
      "|Unit 1047 Box 408...|1994-02-20 00:46:27|United Kingdom|     12346|cooperalexis@hotm...|    Lindsay Cowan|valenciajennifer|true|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|  Katherine David|      hillrachel|true|\n",
      "|Unit 2676 Box 935...|1974-11-26 15:30:20|       Finland|     12348| tcrawford@gmail.com|  Leslie Martinez|    serranobrian|true|\n",
      "|2765 Powers Meado...|1977-05-06 23:57:35|         Italy|     12349|  dustin37@yahoo.com|    Brad Cardenas|   charleshudson|true|\n",
      "|17677 Mark Crest\\...|1996-09-13 19:14:27|        Norway|     12350|amyholland@yahoo.com|     Natalie Ford| gregoryharrison|true|\n",
      "|50047 Smith Point...|1969-06-21 03:39:20|        Norway|     12352| vcarter@hotmail.com|      Dana Clarke|          hmyers|true|\n",
      "|633 Miller Turnpi...|1993-02-25 18:37:29|       Bahrain|     12353|   laura34@yahoo.com|     Gary Nichols|  andrewhamilton|true|\n",
      "|38456 Rachael Cau...|1993-03-13 12:37:34|         Spain|     12354|   zmelton@gmail.com|       John Parks|      matthewray|true|\n",
      "|4140 Pamela Hollo...|1972-11-10 12:01:08|       Bahrain|     12355|   scott50@yahoo.com|Jennifer Lawrence|          glopez|true|\n",
      "|8681 Karen Roads ...|1973-01-13 17:17:26|      Portugal|     12356|josephmacias@hotm...|    James Sanchez|        wesley20|true|\n",
      "|18637 Jessica Rid...|1989-11-24 17:12:54|   Switzerland|     12357|michael16@hotmail...|     Ashley Lopez|     thomasdavid|true|\n",
      "|2129 Joel Rapids\\...|1977-06-19 22:35:52|       Austria|     12358|michaelespinoza@g...| Dr. Angela Brown|      patricia44|true|\n",
      "|86636 Maria Viadu...|1983-09-21 05:22:18|        Cyprus|     12359|  ryanpena@yahoo.com|        John Vega|     nelsonmaria|true|\n",
      "|1579 Young Trail\\...|1980-10-28 17:25:59|       Austria|     12360|briannafrost@yaho...|     Lauren Clark|   portermichael|true|\n",
      "|USNS Howard\\nFPO ...|1982-09-01 09:12:57|       Belgium|     12361|virginia36@hotmai...|Jacqueline Haynes|   johnsonshelly|true|\n",
      "|70092 Adams Prair...|1979-02-03 03:42:47|       Belgium|     12362|   april04@gmail.com|     Brian Flores|    hunterdaniel|true|\n",
      "|7322 Owens Inlet ...|1974-12-21 13:27:20|   Unspecified|     12363|   omolina@gmail.com|Christopher Gomez|         james75|true|\n",
      "|86176 Katherine C...|1990-07-17 15:47:12|       Belgium|     12364|barbaraduncan@gma...|     Robert Burns|          eric10|true|\n",
      "|932 Jeremy Spring...|1981-07-10 00:35:00|        Cyprus|     12365|nicoleanderson@ho...|    Joshua Parker|     millerrenee|true|\n",
      "|USNV Chavez\\nFPO ...|1989-12-26 00:58:01|       Denmark|     12367|   aaron99@yahoo.com|Christine Douglas|       michael58|true|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.withColumn(\"Flag\", lit(true)).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7df0d8f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 17:52:29 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/01 17:52:29 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/01 17:52:29 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/01 17:52:29 INFO BlockManagerInfo: Removed broadcast_39_piece0 on ubuntu:36981 in memory (size: 7.7 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:52:29 INFO BlockManagerInfo: Removed broadcast_38_piece0 on ubuntu:36981 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:52:29 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:52:29 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on ubuntu:36981 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:52:29 INFO SparkContext: Created broadcast 40 from show at cell42.sc:1\n",
      "24/09/01 17:52:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/01 17:52:29 INFO SparkContext: Starting job: show at cell42.sc:1\n",
      "24/09/01 17:52:29 INFO DAGScheduler: Got job 29 (show at cell42.sc:1) with 1 output partitions\n",
      "24/09/01 17:52:29 INFO DAGScheduler: Final stage: ResultStage 29 (show at cell42.sc:1)\n",
      "24/09/01 17:52:29 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:52:29 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:52:29 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[68] at show at cell42.sc:1), which has no missing parents\n",
      "24/09/01 17:52:29 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 17.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:52:29 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 4.5 GiB)\n",
      "24/09/01 17:52:29 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on ubuntu:36981 (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:52:29 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:52:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[68] at show at cell42.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 17:52:29 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0\n",
      "24/09/01 17:52:29 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 59) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/01 17:52:29 INFO Executor: Running task 0.0 in stage 29.0 (TID 59)\n",
      "24/09/01 17:52:29 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/01 17:52:29 INFO Executor: Finished task 0.0 in stage 29.0 (TID 59). 4581 bytes result sent to driver\n",
      "24/09/01 17:52:29 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 59) in 33 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 17:52:29 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:52:29 INFO DAGScheduler: ResultStage 29 (show at cell42.sc:1) finished in 0,050 s\n",
      "24/09/01 17:52:29 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:52:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished\n",
      "24/09/01 17:52:29 INFO DAGScheduler: Job 29 finished: show at cell42.sc:1, took 0,065604 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "|             Address|               Date|       Country|CustomerID|               Email|             Name|        Username|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "|Unit 1047 Box 408...|1994-02-20 00:46:27|United Kingdom|     12346|cooperalexis@hotm...|    Lindsay Cowan|valenciajennifer|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|  Katherine David|      hillrachel|\n",
      "|Unit 2676 Box 935...|1974-11-26 15:30:20|       Finland|     12348| tcrawford@gmail.com|  Leslie Martinez|    serranobrian|\n",
      "|2765 Powers Meado...|1977-05-06 23:57:35|         Italy|     12349|  dustin37@yahoo.com|    Brad Cardenas|   charleshudson|\n",
      "|17677 Mark Crest\\...|1996-09-13 19:14:27|        Norway|     12350|amyholland@yahoo.com|     Natalie Ford| gregoryharrison|\n",
      "|50047 Smith Point...|1969-06-21 03:39:20|        Norway|     12352| vcarter@hotmail.com|      Dana Clarke|          hmyers|\n",
      "|633 Miller Turnpi...|1993-02-25 18:37:29|       Bahrain|     12353|   laura34@yahoo.com|     Gary Nichols|  andrewhamilton|\n",
      "|38456 Rachael Cau...|1993-03-13 12:37:34|         Spain|     12354|   zmelton@gmail.com|       John Parks|      matthewray|\n",
      "|4140 Pamela Hollo...|1972-11-10 12:01:08|       Bahrain|     12355|   scott50@yahoo.com|Jennifer Lawrence|          glopez|\n",
      "|8681 Karen Roads ...|1973-01-13 17:17:26|      Portugal|     12356|josephmacias@hotm...|    James Sanchez|        wesley20|\n",
      "|18637 Jessica Rid...|1989-11-24 17:12:54|   Switzerland|     12357|michael16@hotmail...|     Ashley Lopez|     thomasdavid|\n",
      "|2129 Joel Rapids\\...|1977-06-19 22:35:52|       Austria|     12358|michaelespinoza@g...| Dr. Angela Brown|      patricia44|\n",
      "|86636 Maria Viadu...|1983-09-21 05:22:18|        Cyprus|     12359|  ryanpena@yahoo.com|        John Vega|     nelsonmaria|\n",
      "|1579 Young Trail\\...|1980-10-28 17:25:59|       Austria|     12360|briannafrost@yaho...|     Lauren Clark|   portermichael|\n",
      "|USNS Howard\\nFPO ...|1982-09-01 09:12:57|       Belgium|     12361|virginia36@hotmai...|Jacqueline Haynes|   johnsonshelly|\n",
      "|70092 Adams Prair...|1979-02-03 03:42:47|       Belgium|     12362|   april04@gmail.com|     Brian Flores|    hunterdaniel|\n",
      "|7322 Owens Inlet ...|1974-12-21 13:27:20|   Unspecified|     12363|   omolina@gmail.com|Christopher Gomez|         james75|\n",
      "|86176 Katherine C...|1990-07-17 15:47:12|       Belgium|     12364|barbaraduncan@gma...|     Robert Burns|          eric10|\n",
      "|932 Jeremy Spring...|1981-07-10 00:35:00|        Cyprus|     12365|nicoleanderson@ho...|    Joshua Parker|     millerrenee|\n",
      "|USNV Chavez\\nFPO ...|1989-12-26 00:58:01|       Denmark|     12367|   aaron99@yahoo.com|Christine Douglas|       michael58|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.withColumnRenamed(\"Birthdate\", \"Date\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39bab1b",
   "metadata": {},
   "source": [
    "#### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "30bc0966",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 17:52:37 INFO FileSourceStrategy: Pushed Filters: IsNotNull(Country),EqualTo(Country,Norway)\n",
      "24/09/01 17:52:37 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(Country#254),(Country#254 = Norway)\n",
      "24/09/01 17:52:37 INFO CodeGenerator: Code generated in 11.102797 ms\n",
      "24/09/01 17:52:37 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/01 17:52:37 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:52:37 INFO BlockManagerInfo: Removed broadcast_40_piece0 on ubuntu:36981 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:52:37 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on ubuntu:36981 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:52:37 INFO SparkContext: Created broadcast 42 from show at cell43.sc:1\n",
      "24/09/01 17:52:37 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/01 17:52:37 INFO BlockManagerInfo: Removed broadcast_41_piece0 on ubuntu:36981 in memory (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:52:37 INFO SparkContext: Starting job: show at cell43.sc:1\n",
      "24/09/01 17:52:37 INFO DAGScheduler: Got job 30 (show at cell43.sc:1) with 1 output partitions\n",
      "24/09/01 17:52:37 INFO DAGScheduler: Final stage: ResultStage 30 (show at cell43.sc:1)\n",
      "24/09/01 17:52:37 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:52:37 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:52:37 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[72] at show at cell43.sc:1), which has no missing parents\n",
      "24/09/01 17:52:37 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 17.7 KiB, free 4.5 GiB)\n",
      "24/09/01 17:52:37 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 4.5 GiB)\n",
      "24/09/01 17:52:37 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on ubuntu:36981 (size: 7.9 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:52:37 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:52:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[72] at show at cell43.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 17:52:37 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0\n",
      "24/09/01 17:52:37 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 60) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/01 17:52:37 INFO Executor: Running task 0.0 in stage 30.0 (TID 60)\n",
      "24/09/01 17:52:37 INFO CodeGenerator: Code generated in 26.096941 ms\n",
      "24/09/01 17:52:37 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/01 17:52:37 INFO CodeGenerator: Code generated in 11.441037 ms\n",
      "24/09/01 17:52:37 INFO Executor: Finished task 0.0 in stage 30.0 (TID 60). 3059 bytes result sent to driver\n",
      "24/09/01 17:52:37 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 60) in 102 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 17:52:37 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:52:37 INFO DAGScheduler: ResultStage 30 (show at cell43.sc:1) finished in 0,132 s\n",
      "24/09/01 17:52:37 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:52:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 30: Stage finished\n",
      "24/09/01 17:52:37 INFO DAGScheduler: Job 30 finished: show at cell43.sc:1, took 0,147255 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------+----------+--------------------+----------------+---------------+\n",
      "|             Address|          Birthdate|Country|CustomerID|               Email|            Name|       Username|\n",
      "+--------------------+-------------------+-------+----------+--------------------+----------------+---------------+\n",
      "|17677 Mark Crest\\...|1996-09-13 19:14:27| Norway|     12350|amyholland@yahoo.com|    Natalie Ford|gregoryharrison|\n",
      "|50047 Smith Point...|1969-06-21 03:39:20| Norway|     12352| vcarter@hotmail.com|     Dana Clarke|         hmyers|\n",
      "|0297 Jacob Ranch ...|1990-06-01 14:49:52| Norway|     12381|douglaschavez@hot...|   Matthew Jones|    stephanie68|\n",
      "|3102 Hopkins Walk...|1976-06-19 08:10:24| Norway|     12430|crystalromero@hot...|       Lisa Tate|       pgilbert|\n",
      "|637 Philip Lock S...|1984-06-06 09:36:14| Norway|     12432|jessica87@hotmail...|  Cheryl Herring|mathewsnicholas|\n",
      "|546 Tyler Prairie...|1985-05-27 10:39:47| Norway|     12433|mariahmcpherson@g...|  Kaitlin Miller|         lyoung|\n",
      "|6270 Jennifer Pra...|1977-06-01 20:40:04| Norway|     12436|lynncynthia@hotma...|    Rodney Giles|       swiggins|\n",
      "|415 Megan Parkway...|1971-08-29 06:21:22| Norway|     12438|  jeff42@hotmail.com| Thomas Figueroa|  matthewharris|\n",
      "|PSC 4266, Box 099...|1971-09-03 05:42:49| Norway|     12444| richard20@gmail.com|     Meghan Wood|   salazarbilly|\n",
      "|1333 Michael Vill...|1995-03-09 03:25:02| Norway|     12752|seanrobles@gmail.com|Lauren Hernandez|    morrowhenry|\n",
      "+--------------------+-------------------+-------+----------+--------------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.filter(\"Country = 'Norway'\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cb032124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 21:56:01 INFO FileSourceStrategy: Pushed Filters: IsNotNull(Country),Not(EqualTo(Country,Iceland))\n",
      "24/09/01 21:56:01 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(Country#254),NOT (Country#254 = Iceland)\n",
      "24/09/01 21:56:01 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/01 21:56:01 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/01 21:56:01 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on ubuntu:36981 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 21:56:01 INFO SparkContext: Created broadcast 65 from show at cell56.sc:1\n",
      "24/09/01 21:56:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/01 21:56:01 INFO SparkContext: Starting job: show at cell56.sc:1\n",
      "24/09/01 21:56:01 INFO DAGScheduler: Got job 41 (show at cell56.sc:1) with 1 output partitions\n",
      "24/09/01 21:56:01 INFO DAGScheduler: Final stage: ResultStage 41 (show at cell56.sc:1)\n",
      "24/09/01 21:56:01 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 21:56:01 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 21:56:01 INFO DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[132] at show at cell56.sc:1), which has no missing parents\n",
      "24/09/01 21:56:02 INFO MemoryStore: Block broadcast_66 stored as values in memory (estimated size 17.8 KiB, free 4.5 GiB)\n",
      "24/09/01 21:56:02 INFO MemoryStore: Block broadcast_66_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 4.5 GiB)\n",
      "24/09/01 21:56:02 INFO BlockManagerInfo: Added broadcast_66_piece0 in memory on ubuntu:36981 (size: 8.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 21:56:02 INFO SparkContext: Created broadcast 66 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 21:56:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[132] at show at cell56.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 21:56:02 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks resource profile 0\n",
      "24/09/01 21:56:02 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 71) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/01 21:56:02 INFO Executor: Running task 0.0 in stage 41.0 (TID 71)\n",
      "24/09/01 21:56:02 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/01 21:56:02 INFO Executor: Finished task 0.0 in stage 41.0 (TID 71). 4532 bytes result sent to driver\n",
      "24/09/01 21:56:02 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 71) in 30 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 21:56:02 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool \n",
      "24/09/01 21:56:02 INFO DAGScheduler: ResultStage 41 (show at cell56.sc:1) finished in 0,079 s\n",
      "24/09/01 21:56:02 INFO DAGScheduler: Job 41 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 21:56:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 41: Stage finished\n",
      "24/09/01 21:56:02 INFO DAGScheduler: Job 41 finished: show at cell56.sc:1, took 0,082573 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------+-------------------+--------------+----------+--------------------------+-----------------+----------------+\n",
      "|Address                                              |Birthdate          |Country       |CustomerID|Email                     |Name             |Username        |\n",
      "+-----------------------------------------------------+-------------------+--------------+----------+--------------------------+-----------------+----------------+\n",
      "|Unit 1047 Box 4089\\nDPO AA 57348                     |1994-02-20 00:46:27|United Kingdom|12346     |cooperalexis@hotmail.com  |Lindsay Cowan    |valenciajennifer|\n",
      "|Unit 2676 Box 9352\\nDPO AA 38560                     |1974-11-26 15:30:20|Finland       |12348     |tcrawford@gmail.com       |Leslie Martinez  |serranobrian    |\n",
      "|2765 Powers Meadow\\nHeatherfurt, CT 53165            |1977-05-06 23:57:35|Italy         |12349     |dustin37@yahoo.com        |Brad Cardenas    |charleshudson   |\n",
      "|17677 Mark Crest\\nWalterberg, IA 39017               |1996-09-13 19:14:27|Norway        |12350     |amyholland@yahoo.com      |Natalie Ford     |gregoryharrison |\n",
      "|50047 Smith Point Suite 162\\nWilkinsstad, PA 04106   |1969-06-21 03:39:20|Norway        |12352     |vcarter@hotmail.com       |Dana Clarke      |hmyers          |\n",
      "|633 Miller Turnpike\\nJonathanland, OR 62874          |1993-02-25 18:37:29|Bahrain       |12353     |laura34@yahoo.com         |Gary Nichols     |andrewhamilton  |\n",
      "|38456 Rachael Causeway Apt. 735\\nEvanfort, AR 33893  |1993-03-13 12:37:34|Spain         |12354     |zmelton@gmail.com         |John Parks       |matthewray      |\n",
      "|4140 Pamela Hollow Apt. 849\\nEast Elizabeth, TN 29566|1972-11-10 12:01:08|Bahrain       |12355     |scott50@yahoo.com         |Jennifer Lawrence|glopez          |\n",
      "|8681 Karen Roads Apt. 096\\nLowehaven, IA 19798       |1973-01-13 17:17:26|Portugal      |12356     |josephmacias@hotmail.com  |James Sanchez    |wesley20        |\n",
      "|18637 Jessica Ridge Apt. 157\\nGrossberg, ME 84127    |1989-11-24 17:12:54|Switzerland   |12357     |michael16@hotmail.com     |Ashley Lopez     |thomasdavid     |\n",
      "|2129 Joel Rapids\\nLisahaven, NE 08609                |1977-06-19 22:35:52|Austria       |12358     |michaelespinoza@gmail.com |Dr. Angela Brown |patricia44      |\n",
      "|86636 Maria Viaduct\\nKennethhaven, SD 21876          |1983-09-21 05:22:18|Cyprus        |12359     |ryanpena@yahoo.com        |John Vega        |nelsonmaria     |\n",
      "|1579 Young Trail\\nJessechester, OH 88328             |1980-10-28 17:25:59|Austria       |12360     |briannafrost@yahoo.com    |Lauren Clark     |portermichael   |\n",
      "|USNS Howard\\nFPO AP 30863                            |1982-09-01 09:12:57|Belgium       |12361     |virginia36@hotmail.com    |Jacqueline Haynes|johnsonshelly   |\n",
      "|70092 Adams Prairie\\nTurnerborough, TX 38603         |1979-02-03 03:42:47|Belgium       |12362     |april04@gmail.com         |Brian Flores     |hunterdaniel    |\n",
      "|7322 Owens Inlet Apt. 688\\nPort Leslie, OR 81893     |1974-12-21 13:27:20|Unspecified   |12363     |omolina@gmail.com         |Christopher Gomez|james75         |\n",
      "|86176 Katherine Common\\nWebbhaven, WA 51980          |1990-07-17 15:47:12|Belgium       |12364     |barbaraduncan@gmail.com   |Robert Burns     |eric10          |\n",
      "|932 Jeremy Springs Suite 144\\nJohnmouth, NM 02561    |1981-07-10 00:35:00|Cyprus        |12365     |nicoleanderson@hotmail.com|Joshua Parker    |millerrenee     |\n",
      "|USNV Chavez\\nFPO AP 78727                            |1989-12-26 00:58:01|Denmark       |12367     |aaron99@yahoo.com         |Christine Douglas|michael58       |\n",
      "|565 Hodge Motorway Suite 101\\nWendyberg, FL 57099    |1973-12-21 03:33:47|Cyprus        |12370     |qgibson@hotmail.com       |Derek Curtis     |zsanders        |\n",
      "+-----------------------------------------------------+-------------------+--------------+----------+--------------------------+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.where($\"Country\" =!= \"Iceland\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5044e9ab",
   "metadata": {},
   "source": [
    "#### Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c5c70ef1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 21:56:34 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/01 21:56:34 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/01 21:56:34 INFO MemoryStore: Block broadcast_67 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/01 21:56:34 INFO MemoryStore: Block broadcast_67_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/01 21:56:34 INFO BlockManagerInfo: Added broadcast_67_piece0 in memory on ubuntu:36981 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 21:56:34 INFO SparkContext: Created broadcast 67 from show at cell57.sc:1\n",
      "24/09/01 21:56:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/01 21:56:34 INFO SparkContext: Starting job: show at cell57.sc:1\n",
      "24/09/01 21:56:34 INFO DAGScheduler: Got job 42 (show at cell57.sc:1) with 1 output partitions\n",
      "24/09/01 21:56:34 INFO DAGScheduler: Final stage: ResultStage 42 (show at cell57.sc:1)\n",
      "24/09/01 21:56:34 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 21:56:34 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 21:56:34 INFO DAGScheduler: Submitting ResultStage 42 (MapPartitionsRDD[136] at show at cell57.sc:1), which has no missing parents\n",
      "24/09/01 21:56:34 INFO MemoryStore: Block broadcast_68 stored as values in memory (estimated size 14.3 KiB, free 4.5 GiB)\n",
      "24/09/01 21:56:34 INFO MemoryStore: Block broadcast_68_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 4.5 GiB)\n",
      "24/09/01 21:56:34 INFO BlockManagerInfo: Added broadcast_68_piece0 in memory on ubuntu:36981 (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 21:56:34 INFO SparkContext: Created broadcast 68 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 21:56:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (MapPartitionsRDD[136] at show at cell57.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 21:56:34 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks resource profile 0\n",
      "24/09/01 21:56:34 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 72) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/01 21:56:34 INFO Executor: Running task 0.0 in stage 42.0 (TID 72)\n",
      "24/09/01 21:56:34 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/01 21:56:34 INFO Executor: Finished task 0.0 in stage 42.0 (TID 72). 6279 bytes result sent to driver\n",
      "24/09/01 21:56:34 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 72) in 39 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 21:56:34 INFO DAGScheduler: ResultStage 42 (show at cell57.sc:1) finished in 0,046 s\n",
      "24/09/01 21:56:34 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool \n",
      "24/09/01 21:56:34 INFO DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 21:56:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished\n",
      "24/09/01 21:56:34 INFO DAGScheduler: Job 42 finished: show at cell57.sc:1, took 0,058337 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------+----------+--------------------+--------------------+-----------------+\n",
      "|             Address|          Birthdate|       Country|CustomerID|               Email|                Name|         Username|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+--------------------+-----------------+\n",
      "|6942 Connie Skywa...|1973-10-24 00:52:10|United Kingdom|     12989| amber97@hotmail.com|   Brandon Contreras|           ecasey|\n",
      "|79375 David Neck\\...|1971-05-04 22:20:10|United Kingdom|     12988|   erica98@gmail.com|      Gabriel Romero|          qknight|\n",
      "|00881 West Flat\\n...|1997-03-05 19:20:57|United Kingdom|     12987|    vkeith@yahoo.com|Christopher Lawrence|        smcintyre|\n",
      "|499 Jonathan Stre...|1987-10-24 20:05:15|United Kingdom|     12985| fredsmith@yahoo.com|        Xavier Myers|stricklandjeffery|\n",
      "|9505 Melissa Stre...|1975-09-22 15:21:58|United Kingdom|     12984|scottjonathan@yah...|        Brandy Huang|   amandawilliams|\n",
      "|399 Fuentes Roads...|1986-09-30 18:12:45|United Kingdom|     12982|cynthia31@hotmail...|      Linda Stephens|     davidsonomar|\n",
      "|1573 Jessica Glen...|1984-07-23 03:09:18|United Kingdom|     12981|  esharp@hotmail.com|          Dawn Woods|         steven67|\n",
      "|153 Tara Ridges S...|1974-03-03 07:52:15|United Kingdom|     12980| jessica87@gmail.com|         Sean Brooks|        kristen26|\n",
      "|62134 Chen Valley...|1990-10-09 01:29:02|United Kingdom|     12977| fmatthews@gmail.com|          Kyle Simon|          emily28|\n",
      "|7521 Christopher ...|1973-10-10 23:57:51|United Kingdom|     12976|williamsheidi@yah...|         Hannah Rose|         eugene04|\n",
      "|00679 Lucero Moun...|1987-10-13 12:41:52|United Kingdom|     12974|thomasreyes@yahoo...|     Daniel Fletcher|  velazquezangela|\n",
      "|885 Zamora Hills\\...|1986-11-14 14:18:47|United Kingdom|     12971|   cwilcox@yahoo.com|       Caitlin Walls|         ashley11|\n",
      "|4539 Powers Orcha...|1990-09-11 06:01:18|United Kingdom|     12970|edwardspeter@yaho...|      Jonathan Hines|          mmiller|\n",
      "|335 Lewis Land\\nL...|1994-04-25 16:59:48|United Kingdom|     12968| mmurray@hotmail.com|         Susan Davis|   jacksoncolleen|\n",
      "|Unit 3978 Box 615...|1969-05-28 22:09:26|United Kingdom|     12967|thomasjames@gmail...|    Amber Williamson|    justinjohnson|\n",
      "|PSC 6600, Box 447...|1975-10-14 17:46:59|United Kingdom|     12966|colinward@hotmail...|        Amy Robinson|     sarathompson|\n",
      "|0240 Ernest Under...|1968-06-14 23:10:47|United Kingdom|     12965|jeremy10@hotmail.com|      Raymond Patton|      meganbrewer|\n",
      "|2433 Amy Shoals\\n...|1975-11-05 04:34:04|United Kingdom|     12963|  tjohnson@yahoo.com|        Sheila Parks|      dominique55|\n",
      "|833 Wilson Street...|1978-12-25 10:12:45|United Kingdom|     12962|kathyphillips@yah...|        Cheryl Burns|       diazsharon|\n",
      "|809 Robert Plain ...|1978-07-26 19:48:26|United Kingdom|     12957|  steven78@gmail.com|     Reginald Wright|         nathan71|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.sort(col(\"CustomerID\").desc).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "99396b99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 17:52:53 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/01 17:52:53 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/01 17:52:53 INFO CodeGenerator: Code generated in 5.601076 ms\n",
      "24/09/01 17:52:53 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/01 17:52:53 INFO BlockManagerInfo: Removed broadcast_47_piece0 on ubuntu:36981 in memory (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:52:53 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:52:53 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on ubuntu:36981 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:52:53 INFO SparkContext: Created broadcast 48 from show at cell46.sc:1\n",
      "24/09/01 17:52:53 INFO BlockManagerInfo: Removed broadcast_46_piece0 on ubuntu:36981 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:52:53 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/01 17:52:53 INFO SparkContext: Starting job: show at cell46.sc:1\n",
      "24/09/01 17:52:53 INFO DAGScheduler: Got job 33 (show at cell46.sc:1) with 1 output partitions\n",
      "24/09/01 17:52:53 INFO DAGScheduler: Final stage: ResultStage 33 (show at cell46.sc:1)\n",
      "24/09/01 17:52:53 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:52:53 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:52:53 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[84] at show at cell46.sc:1), which has no missing parents\n",
      "24/09/01 17:52:53 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 14.3 KiB, free 4.5 GiB)\n",
      "24/09/01 17:52:53 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:52:53 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on ubuntu:36981 (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:52:53 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:52:53 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[84] at show at cell46.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 17:52:53 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks resource profile 0\n",
      "24/09/01 17:52:53 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 63) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/01 17:52:53 INFO Executor: Running task 0.0 in stage 33.0 (TID 63)\n",
      "24/09/01 17:52:53 INFO CodeGenerator: Code generated in 11.554012 ms\n",
      "24/09/01 17:52:53 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/01 17:52:53 INFO Executor: Finished task 0.0 in stage 33.0 (TID 63). 6119 bytes result sent to driver\n",
      "24/09/01 17:52:53 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 63) in 61 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 17:52:53 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:52:53 INFO DAGScheduler: ResultStage 33 (show at cell46.sc:1) finished in 0,071 s\n",
      "24/09/01 17:52:53 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:52:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 33: Stage finished\n",
      "24/09/01 17:52:53 INFO DAGScheduler: Job 33 finished: show at cell46.sc:1, took 0,079618 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "|             Address|          Birthdate|       Country|CustomerID|               Email|             Name|        Username|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "|Unit 1047 Box 408...|1994-02-20 00:46:27|United Kingdom|     12346|cooperalexis@hotm...|    Lindsay Cowan|valenciajennifer|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|  Katherine David|      hillrachel|\n",
      "|Unit 2676 Box 935...|1974-11-26 15:30:20|       Finland|     12348| tcrawford@gmail.com|  Leslie Martinez|    serranobrian|\n",
      "|2765 Powers Meado...|1977-05-06 23:57:35|         Italy|     12349|  dustin37@yahoo.com|    Brad Cardenas|   charleshudson|\n",
      "|17677 Mark Crest\\...|1996-09-13 19:14:27|        Norway|     12350|amyholland@yahoo.com|     Natalie Ford| gregoryharrison|\n",
      "|50047 Smith Point...|1969-06-21 03:39:20|        Norway|     12352| vcarter@hotmail.com|      Dana Clarke|          hmyers|\n",
      "|633 Miller Turnpi...|1993-02-25 18:37:29|       Bahrain|     12353|   laura34@yahoo.com|     Gary Nichols|  andrewhamilton|\n",
      "|38456 Rachael Cau...|1993-03-13 12:37:34|         Spain|     12354|   zmelton@gmail.com|       John Parks|      matthewray|\n",
      "|4140 Pamela Hollo...|1972-11-10 12:01:08|       Bahrain|     12355|   scott50@yahoo.com|Jennifer Lawrence|          glopez|\n",
      "|8681 Karen Roads ...|1973-01-13 17:17:26|      Portugal|     12356|josephmacias@hotm...|    James Sanchez|        wesley20|\n",
      "|18637 Jessica Rid...|1989-11-24 17:12:54|   Switzerland|     12357|michael16@hotmail...|     Ashley Lopez|     thomasdavid|\n",
      "|2129 Joel Rapids\\...|1977-06-19 22:35:52|       Austria|     12358|michaelespinoza@g...| Dr. Angela Brown|      patricia44|\n",
      "|86636 Maria Viadu...|1983-09-21 05:22:18|        Cyprus|     12359|  ryanpena@yahoo.com|        John Vega|     nelsonmaria|\n",
      "|1579 Young Trail\\...|1980-10-28 17:25:59|       Austria|     12360|briannafrost@yaho...|     Lauren Clark|   portermichael|\n",
      "|USNS Howard\\nFPO ...|1982-09-01 09:12:57|       Belgium|     12361|virginia36@hotmai...|Jacqueline Haynes|   johnsonshelly|\n",
      "|70092 Adams Prair...|1979-02-03 03:42:47|       Belgium|     12362|   april04@gmail.com|     Brian Flores|    hunterdaniel|\n",
      "|7322 Owens Inlet ...|1974-12-21 13:27:20|   Unspecified|     12363|   omolina@gmail.com|Christopher Gomez|         james75|\n",
      "|86176 Katherine C...|1990-07-17 15:47:12|       Belgium|     12364|barbaraduncan@gma...|     Robert Burns|          eric10|\n",
      "|932 Jeremy Spring...|1981-07-10 00:35:00|        Cyprus|     12365|nicoleanderson@ho...|    Joshua Parker|     millerrenee|\n",
      "|USNV Chavez\\nFPO ...|1989-12-26 00:58:01|       Denmark|     12367|   aaron99@yahoo.com|Christine Douglas|       michael58|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.orderBy(\"CustomerID\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdc9ec6",
   "metadata": {},
   "source": [
    "#### Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1910596b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 17:53:18 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/01 17:53:18 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/01 17:53:18 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/01 17:53:18 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:53:18 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on ubuntu:36981 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:53:18 INFO SparkContext: Created broadcast 53 from rdd at cell49.sc:2\n",
      "24/09/01 17:53:18 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/01 17:53:18 INFO DAGScheduler: Registering RDD 100 (rdd at cell49.sc:2) as input to shuffle 1\n",
      "24/09/01 17:53:18 INFO DAGScheduler: Got map stage job 35 (rdd at cell49.sc:2) with 1 output partitions\n",
      "24/09/01 17:53:18 INFO DAGScheduler: Final stage: ShuffleMapStage 35 (rdd at cell49.sc:2)\n",
      "24/09/01 17:53:18 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:53:18 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:53:18 INFO DAGScheduler: Submitting ShuffleMapStage 35 (MapPartitionsRDD[100] at rdd at cell49.sc:2), which has no missing parents\n",
      "24/09/01 17:53:18 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 14.5 KiB, free 4.5 GiB)\n",
      "24/09/01 17:53:18 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 4.5 GiB)\n",
      "24/09/01 17:53:18 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on ubuntu:36981 (size: 7.3 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:53:18 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:53:18 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 35 (MapPartitionsRDD[100] at rdd at cell49.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 17:53:18 INFO TaskSchedulerImpl: Adding task set 35.0 with 1 tasks resource profile 0\n",
      "24/09/01 17:53:18 INFO TaskSetManager: Starting task 0.0 in stage 35.0 (TID 65) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8235 bytes) \n",
      "24/09/01 17:53:18 INFO Executor: Running task 0.0 in stage 35.0 (TID 65)\n",
      "24/09/01 17:53:19 INFO CodeGenerator: Code generated in 12.896972 ms\n",
      "24/09/01 17:53:19 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/01 17:53:19 INFO Executor: Finished task 0.0 in stage 35.0 (TID 65). 1713 bytes result sent to driver\n",
      "24/09/01 17:53:19 INFO TaskSetManager: Finished task 0.0 in stage 35.0 (TID 65) in 46 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 17:53:19 INFO TaskSchedulerImpl: Removed TaskSet 35.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:53:19 INFO DAGScheduler: ShuffleMapStage 35 (rdd at cell49.sc:2) finished in 0,062 s\n",
      "24/09/01 17:53:19 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/01 17:53:19 INFO DAGScheduler: running: Set()\n",
      "24/09/01 17:53:19 INFO DAGScheduler: waiting: Set()\n",
      "24/09/01 17:53:19 INFO DAGScheduler: failed: Set()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New num partitions: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrepartitionedDf\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [Address: string, Birthdate: string ... 5 more fields]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val repartitionedDf = customerDf.repartition(5, col(\"Country\"))\n",
    "println(s\"Old num partitions: ${customerDf.rdd.getNumPartitions}\")\n",
    "println(s\"New num partitions: ${repartitionedDf.rdd.getNumPartitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "44b583d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 17:54:06 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/01 17:54:06 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/01 17:54:06 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/01 17:54:06 INFO BlockManagerInfo: Removed broadcast_54_piece0 on ubuntu:36981 in memory (size: 7.3 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:54:06 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:54:06 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on ubuntu:36981 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:54:06 INFO SparkContext: Created broadcast 55 from rdd at cell50.sc:1\n",
      "24/09/01 17:54:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/01 17:54:06 INFO DAGScheduler: Registering RDD 107 (rdd at cell50.sc:1) as input to shuffle 2\n",
      "24/09/01 17:54:06 INFO DAGScheduler: Got map stage job 36 (rdd at cell50.sc:1) with 1 output partitions\n",
      "24/09/01 17:54:06 INFO DAGScheduler: Final stage: ShuffleMapStage 36 (rdd at cell50.sc:1)\n",
      "24/09/01 17:54:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:54:06 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:54:06 INFO DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[107] at rdd at cell50.sc:1), which has no missing parents\n",
      "24/09/01 17:54:06 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 14.5 KiB, free 4.5 GiB)\n",
      "24/09/01 17:54:06 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 4.5 GiB)\n",
      "24/09/01 17:54:06 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on ubuntu:36981 (size: 7.3 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:54:06 INFO SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:54:06 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[107] at rdd at cell50.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 17:54:06 INFO TaskSchedulerImpl: Adding task set 36.0 with 1 tasks resource profile 0\n",
      "24/09/01 17:54:06 INFO TaskSetManager: Starting task 0.0 in stage 36.0 (TID 66) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8235 bytes) \n",
      "24/09/01 17:54:06 INFO Executor: Running task 0.0 in stage 36.0 (TID 66)\n",
      "24/09/01 17:54:06 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/01 17:54:06 INFO Executor: Finished task 0.0 in stage 36.0 (TID 66). 1713 bytes result sent to driver\n",
      "24/09/01 17:54:06 INFO TaskSetManager: Finished task 0.0 in stage 36.0 (TID 66) in 32 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 17:54:06 INFO TaskSchedulerImpl: Removed TaskSet 36.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:54:06 INFO DAGScheduler: ShuffleMapStage 36 (rdd at cell50.sc:1) finished in 0,042 s\n",
      "24/09/01 17:54:06 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/01 17:54:06 INFO DAGScheduler: running: Set()\n",
      "24/09/01 17:54:06 INFO DAGScheduler: waiting: Set()\n",
      "24/09/01 17:54:06 INFO DAGScheduler: failed: Set()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num partitions after coalesce: 1\n"
     ]
    }
   ],
   "source": [
    "println(s\"Num partitions after coalesce: ${repartitionedDf.coalesce(1).rdd.getNumPartitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97975bbd",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3d3f8758-3334-43dd-8ada-f85d412dba84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 17:54:13 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/01 17:54:13 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/01 17:54:13 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/01 17:54:13 INFO BlockManagerInfo: Removed broadcast_56_piece0 on ubuntu:36981 in memory (size: 7.3 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:54:13 INFO BlockManagerInfo: Removed broadcast_55_piece0 on ubuntu:36981 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:54:13 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:54:13 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on ubuntu:36981 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:54:13 INFO SparkContext: Created broadcast 57 from show at cell51.sc:1\n",
      "24/09/01 17:54:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/01 17:54:13 INFO SparkContext: Starting job: show at cell51.sc:1\n",
      "24/09/01 17:54:13 INFO DAGScheduler: Got job 37 (show at cell51.sc:1) with 1 output partitions\n",
      "24/09/01 17:54:13 INFO DAGScheduler: Final stage: ResultStage 37 (show at cell51.sc:1)\n",
      "24/09/01 17:54:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:54:13 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:54:13 INFO DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[116] at show at cell51.sc:1), which has no missing parents\n",
      "24/09/01 17:54:13 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 14.2 KiB, free 4.5 GiB)\n",
      "24/09/01 17:54:13 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:54:13 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on ubuntu:36981 (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:54:13 INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:54:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[116] at show at cell51.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 17:54:13 INFO TaskSchedulerImpl: Adding task set 37.0 with 1 tasks resource profile 0\n",
      "24/09/01 17:54:13 INFO TaskSetManager: Starting task 0.0 in stage 37.0 (TID 67) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/01 17:54:13 INFO Executor: Running task 0.0 in stage 37.0 (TID 67)\n",
      "24/09/01 17:54:13 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/01 17:54:13 INFO Executor: Finished task 0.0 in stage 37.0 (TID 67). 1636 bytes result sent to driver\n",
      "24/09/01 17:54:13 INFO TaskSetManager: Finished task 0.0 in stage 37.0 (TID 67) in 41 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 17:54:13 INFO TaskSchedulerImpl: Removed TaskSet 37.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:54:13 INFO DAGScheduler: ResultStage 37 (show at cell51.sc:1) finished in 0,056 s\n",
      "24/09/01 17:54:13 INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:54:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 37: Stage finished\n",
      "24/09/01 17:54:13 INFO DAGScheduler: Job 37 finished: show at cell51.sc:1, took 0,064467 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|          Birthdate|\n",
      "+-------------------+\n",
      "|1994-02-20 00:46:27|\n",
      "|1988-06-21 00:15:34|\n",
      "|1974-11-26 15:30:20|\n",
      "|1977-05-06 23:57:35|\n",
      "|1996-09-13 19:14:27|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.select(\"Birthdate\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e35c3a43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 17:54:26 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/01 17:54:26 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/01 17:54:26 INFO CodeGenerator: Code generated in 14.798035 ms\n",
      "24/09/01 17:54:26 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/01 17:54:26 INFO BlockManagerInfo: Removed broadcast_58_piece0 on ubuntu:36981 in memory (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:54:26 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:54:26 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on ubuntu:36981 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:54:26 INFO SparkContext: Created broadcast 59 from show at cell52.sc:1\n",
      "24/09/01 17:54:26 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/01 17:54:26 INFO BlockManagerInfo: Removed broadcast_57_piece0 on ubuntu:36981 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:54:26 INFO SparkContext: Starting job: show at cell52.sc:1\n",
      "24/09/01 17:54:26 INFO DAGScheduler: Got job 38 (show at cell52.sc:1) with 1 output partitions\n",
      "24/09/01 17:54:26 INFO DAGScheduler: Final stage: ResultStage 38 (show at cell52.sc:1)\n",
      "24/09/01 17:54:26 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:54:26 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:54:26 INFO DAGScheduler: Submitting ResultStage 38 (MapPartitionsRDD[120] at show at cell52.sc:1), which has no missing parents\n",
      "24/09/01 17:54:26 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 15.5 KiB, free 4.5 GiB)\n",
      "24/09/01 17:54:26 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 4.5 GiB)\n",
      "24/09/01 17:54:26 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on ubuntu:36981 (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:54:26 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:54:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 38 (MapPartitionsRDD[120] at show at cell52.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 17:54:26 INFO TaskSchedulerImpl: Adding task set 38.0 with 1 tasks resource profile 0\n",
      "24/09/01 17:54:26 INFO TaskSetManager: Starting task 0.0 in stage 38.0 (TID 68) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/01 17:54:26 INFO Executor: Running task 0.0 in stage 38.0 (TID 68)\n",
      "24/09/01 17:54:26 INFO CodeGenerator: Code generated in 20.701528 ms\n",
      "24/09/01 17:54:26 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/01 17:54:26 INFO Executor: Finished task 0.0 in stage 38.0 (TID 68). 1600 bytes result sent to driver\n",
      "24/09/01 17:54:26 INFO TaskSetManager: Finished task 0.0 in stage 38.0 (TID 68) in 41 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 17:54:26 INFO DAGScheduler: ResultStage 38 (show at cell52.sc:1) finished in 0,056 s\n",
      "24/09/01 17:54:26 INFO DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:54:26 INFO TaskSchedulerImpl: Removed TaskSet 38.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:54:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 38: Stage finished\n",
      "24/09/01 17:54:26 INFO DAGScheduler: Job 38 finished: show at cell52.sc:1, took 0,073299 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|        bd|\n",
      "+----------+\n",
      "|1994-02-20|\n",
      "|1988-06-21|\n",
      "|1974-11-26|\n",
      "|1977-05-06|\n",
      "|1996-09-13|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.select(date_format(col(\"Birthdate\"), \"yyyy-MM-dd\").alias(\"bd\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9d91ec17-53a6-47a3-8b27-9aaba11f390e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 17:54:32 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/01 17:54:32 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/01 17:54:32 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/01 17:54:32 INFO BlockManagerInfo: Removed broadcast_59_piece0 on ubuntu:36981 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:54:32 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:54:32 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on ubuntu:36981 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:54:32 INFO BlockManagerInfo: Removed broadcast_60_piece0 on ubuntu:36981 in memory (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:54:32 INFO SparkContext: Created broadcast 61 from show at cell53.sc:1\n",
      "24/09/01 17:54:32 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/01 17:54:32 INFO SparkContext: Starting job: show at cell53.sc:1\n",
      "24/09/01 17:54:32 INFO DAGScheduler: Got job 39 (show at cell53.sc:1) with 1 output partitions\n",
      "24/09/01 17:54:32 INFO DAGScheduler: Final stage: ResultStage 39 (show at cell53.sc:1)\n",
      "24/09/01 17:54:32 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:54:32 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:54:32 INFO DAGScheduler: Submitting ResultStage 39 (MapPartitionsRDD[124] at show at cell53.sc:1), which has no missing parents\n",
      "24/09/01 17:54:32 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 14.6 KiB, free 4.5 GiB)\n",
      "24/09/01 17:54:32 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 4.5 GiB)\n",
      "24/09/01 17:54:32 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on ubuntu:36981 (size: 7.1 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:54:32 INFO SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:54:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 39 (MapPartitionsRDD[124] at show at cell53.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 17:54:32 INFO TaskSchedulerImpl: Adding task set 39.0 with 1 tasks resource profile 0\n",
      "24/09/01 17:54:32 INFO TaskSetManager: Starting task 0.0 in stage 39.0 (TID 69) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/01 17:54:32 INFO Executor: Running task 0.0 in stage 39.0 (TID 69)\n",
      "24/09/01 17:54:32 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/01 17:54:32 INFO Executor: Finished task 0.0 in stage 39.0 (TID 69). 1763 bytes result sent to driver\n",
      "24/09/01 17:54:32 INFO TaskSetManager: Finished task 0.0 in stage 39.0 (TID 69) in 24 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 17:54:32 INFO TaskSchedulerImpl: Removed TaskSet 39.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:54:32 INFO DAGScheduler: ResultStage 39 (show at cell53.sc:1) finished in 0,061 s\n",
      "24/09/01 17:54:32 INFO DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:54:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 39: Stage finished\n",
      "24/09/01 17:54:32 INFO DAGScheduler: Job 39 finished: show at cell53.sc:1, took 0,087730 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+\n",
      "|           Name|        Username|\n",
      "+---------------+----------------+\n",
      "|  Lindsay Cowan|valenciajennifer|\n",
      "|Katherine David|      hillrachel|\n",
      "|Leslie Martinez|    serranobrian|\n",
      "|  Brad Cardenas|   charleshudson|\n",
      "|   Natalie Ford| gregoryharrison|\n",
      "+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.select(\"Name\", \"Username\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c1d84b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Birthdate: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Username: string (nullable = true)\n",
      " |-- Identity: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.withColumn(\"Identity\", array('Name, 'Username)).printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5d4294e1-47ff-4300-8514-b2930b8dd9ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/01 17:54:55 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/01 17:54:55 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/01 17:54:55 INFO CodeGenerator: Code generated in 9.001022 ms\n",
      "24/09/01 17:54:55 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/01 17:54:55 INFO BlockManagerInfo: Removed broadcast_62_piece0 on ubuntu:36981 in memory (size: 7.1 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:54:55 INFO BlockManagerInfo: Removed broadcast_61_piece0 on ubuntu:36981 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:54:55 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/01 17:54:55 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on ubuntu:36981 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:54:55 INFO SparkContext: Created broadcast 63 from show at cell55.sc:4\n",
      "24/09/01 17:54:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/01 17:54:55 INFO SparkContext: Starting job: show at cell55.sc:4\n",
      "24/09/01 17:54:55 INFO DAGScheduler: Got job 40 (show at cell55.sc:4) with 1 output partitions\n",
      "24/09/01 17:54:55 INFO DAGScheduler: Final stage: ResultStage 40 (show at cell55.sc:4)\n",
      "24/09/01 17:54:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/01 17:54:55 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/01 17:54:55 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[128] at show at cell55.sc:4), which has no missing parents\n",
      "24/09/01 17:54:55 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 16.4 KiB, free 4.5 GiB)\n",
      "24/09/01 17:54:55 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 4.5 GiB)\n",
      "24/09/01 17:54:55 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on ubuntu:36981 (size: 7.7 KiB, free: 4.5 GiB)\n",
      "24/09/01 17:54:55 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/01 17:54:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[128] at show at cell55.sc:4) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/01 17:54:55 INFO TaskSchedulerImpl: Adding task set 40.0 with 1 tasks resource profile 0\n",
      "24/09/01 17:54:55 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 70) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/09/01 17:54:55 INFO Executor: Running task 0.0 in stage 40.0 (TID 70)\n",
      "24/09/01 17:54:55 INFO CodeGenerator: Code generated in 15.732649 ms\n",
      "24/09/01 17:54:55 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/DataFrame/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/01 17:54:55 INFO Executor: Finished task 0.0 in stage 40.0 (TID 70). 1875 bytes result sent to driver\n",
      "24/09/01 17:54:55 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 70) in 30 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/01 17:54:55 INFO DAGScheduler: ResultStage 40 (show at cell55.sc:4) finished in 0,040 s\n",
      "24/09/01 17:54:55 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/01 17:54:55 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool \n",
      "24/09/01 17:54:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished\n",
      "24/09/01 17:54:55 INFO DAGScheduler: Job 40 finished: show at cell55.sc:4, took 0,048670 s\n",
      "24/09/01 17:54:55 INFO CodeGenerator: Code generated in 7.071775 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+---------------------------------+\n",
      "|Name           |Username        |Identity                         |\n",
      "+---------------+----------------+---------------------------------+\n",
      "|Lindsay Cowan  |valenciajennifer|[Lindsay Cowan, valenciajennifer]|\n",
      "|Katherine David|hillrachel      |[Katherine David, hillrachel]    |\n",
      "|Leslie Martinez|serranobrian    |[Leslie Martinez, serranobrian]  |\n",
      "|Brad Cardenas  |charleshudson   |[Brad Cardenas, charleshudson]   |\n",
      "|Natalie Ford   |gregoryharrison |[Natalie Ford, gregoryharrison]  |\n",
      "+---------------+----------------+---------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf\n",
    "    .withColumn(\"Identity\", array('Name, 'Username))\n",
    "    .select(\"Name\", \"Username\", \"Identity\")\n",
    "    .show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b17c1f-3197-43bd-b043-a96f256bff3c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3 Агрегирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bfc928-6e23-4c15-b3d3-e9b314fc23e2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customerDf.groupBy(\"Country\").agg(count(lit(1))).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b73e30-6cd2-4b6c-84f7-04fd5327711a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customerDf.groupBy(\"Country\").count().show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e740a2d-6fcc-4a8c-b2bf-3854475701c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customerDf.groupBy(\"Country\", \"Birthdate\").agg(min(\"CustomerID\"), max(\"CustomerID\")).orderBy(\"Country\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd0b5c-0c8e-4d5f-8b32-27c6ef1793b9",
   "metadata": {},
   "source": [
    "### union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b493e-9fdf-4e4a-b5ae-ed4f3d5f242c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customerDf.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca4043-76ae-462d-b676-b1e4d2dfebb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customerDf.union(customerDf).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c156a8b9-8a7f-4f1f-a5ed-a41c6155c4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDf.count * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7c197a-d99b-458a-9791-2c995266078f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(1 to 5).toList.map(a => customerDf).reduce((x, y) => x.union(y)).count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a00e4b2",
   "metadata": {},
   "source": [
    "### join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cf39a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val retailDf = spark.read.format(\"json\").load(\"data/retail_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d34e1-e562-468d-bf51-47370b85fa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "customerDf.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20889378",
   "metadata": {},
   "outputs": [],
   "source": [
    "retailDf.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8b62e3-0793-4cf4-86eb-03be86191551",
   "metadata": {},
   "outputs": [],
   "source": [
    "(customerDf.dtypes.map(_._1)).toSet.intersect((retailDf.dtypes.map(_._1)).toSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12be704d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customerDf.join(retailDf, \"CustomerID\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90400c03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customerDf\n",
    "    .join(retailDf, customerDf(\"CustomerID\") === retailDf(\"CustomerID\"), \"left\")\n",
    "    //.select(\"CustomerID\")\n",
    "    .show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45091db0-fbd1-4567-b5eb-faec32fd5325",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e2b3ef-f005-4fa1-b6e5-533bf3fd55e6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
