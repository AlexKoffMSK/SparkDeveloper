{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0d22977-ebe9-44f2-be27-e847b10bd2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.5.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c205ccf0-1f60-499b-86bf-b0f1a5352037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e92a7ca3-7597-4737-9d72-a050430d0be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42fc6b17-f21f-48a8-a9ae-249ff2e7d789",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "24/09/06 14:53:01 INFO SparkContext: Running Spark version 3.5.0\n",
      "24/09/06 14:53:01 INFO SparkContext: OS info Linux, 6.8.0-40-generic, amd64\n",
      "24/09/06 14:53:01 INFO SparkContext: Java version 11.0.24\n",
      "24/09/06 14:53:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/09/06 14:53:01 INFO ResourceUtils: ==============================================================\n",
      "24/09/06 14:53:01 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/09/06 14:53:01 INFO ResourceUtils: ==============================================================\n",
      "24/09/06 14:53:01 INFO SparkContext: Submitted application: Dataframe API\n",
      "24/09/06 14:53:01 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/09/06 14:53:01 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/09/06 14:53:01 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/09/06 14:53:01 INFO SecurityManager: Changing view acls to: vadim\n",
      "24/09/06 14:53:01 INFO SecurityManager: Changing modify acls to: vadim\n",
      "24/09/06 14:53:01 INFO SecurityManager: Changing view acls groups to: \n",
      "24/09/06 14:53:01 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/09/06 14:53:01 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: vadim; groups with view permissions: EMPTY; users with modify permissions: vadim; groups with modify permissions: EMPTY\n",
      "24/09/06 14:53:02 INFO Utils: Successfully started service 'sparkDriver' on port 34313.\n",
      "24/09/06 14:53:02 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/09/06 14:53:02 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/09/06 14:53:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/09/06 14:53:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/09/06 14:53:02 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/09/06 14:53:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c66f5c88-67f4-42cb-843e-d14ab0091972\n",
      "24/09/06 14:53:02 INFO MemoryStore: MemoryStore started with capacity 4.5 GiB\n",
      "24/09/06 14:53:02 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/09/06 14:53:02 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/09/06 14:53:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/09/06 14:53:02 INFO Executor: Starting executor ID driver on host ubuntu\n",
      "24/09/06 14:53:02 INFO Executor: OS info Linux, 6.8.0-40-generic, amd64\n",
      "24/09/06 14:53:02 INFO Executor: Java version 11.0.24\n",
      "24/09/06 14:53:02 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "24/09/06 14:53:02 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1da7ac66 for default.\n",
      "24/09/06 14:53:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34403.\n",
      "24/09/06 14:53:02 INFO NettyBlockTransferService: Server created on ubuntu:34403\n",
      "24/09/06 14:53:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/09/06 14:53:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ubuntu, 34403, None)\n",
      "24/09/06 14:53:02 INFO BlockManagerMasterEndpoint: Registering block manager ubuntu:34403 with 4.5 GiB RAM, BlockManagerId(driver, ubuntu, 34403, None)\n",
      "24/09/06 14:53:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ubuntu, 34403, None)\n",
      "24/09/06 14:53:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ubuntu, 34403, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@11c4a98b\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "                .builder()\n",
    "                .master(\"local[*]\")\n",
    "                .appName(\"Dataframe API\")\n",
    "                .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25d77452-a4f3-4795-a898-211247296a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.version == 3.5.0\n"
     ]
    }
   ],
   "source": [
    "println(s\"spark.version == ${spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5567071e-689f-4e21-b0bd-bef018305386",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:53:03 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "24/09/06 14:53:03 INFO SharedState: Warehouse path is 'file:/home/vadim/workspace/Spark/SQL/spark-warehouse'.\n",
      "24/09/06 14:53:04 INFO InMemoryFileIndex: It took 26 ms to list leaf files for 1 paths.\n",
      "24/09/06 14:53:04 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "24/09/06 14:53:05 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:53:05 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:53:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 198.5 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ubuntu:34403 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:06 INFO SparkContext: Created broadcast 0 from load at cell6.sc:1\n",
      "24/09/06 14:53:06 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:53:06 INFO SparkContext: Starting job: load at cell6.sc:1\n",
      "24/09/06 14:53:06 INFO DAGScheduler: Got job 0 (load at cell6.sc:1) with 1 output partitions\n",
      "24/09/06 14:53:06 INFO DAGScheduler: Final stage: ResultStage 0 (load at cell6.sc:1)\n",
      "24/09/06 14:53:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:53:06 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:53:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at load at cell6.sc:1), which has no missing parents\n",
      "24/09/06 14:53:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 15.8 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ubuntu:34403 (size: 7.4 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:06 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:53:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at load at cell6.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:53:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:53:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8240 bytes) \n",
      "24/09/06 14:53:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "24/09/06 14:53:06 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/SQL/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:53:06 INFO CodeGenerator: Code generated in 206.554023 ms\n",
      "24/09/06 14:53:06 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2119 bytes result sent to driver\n",
      "24/09/06 14:53:06 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 481 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:53:06 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:53:06 INFO DAGScheduler: ResultStage 0 (load at cell6.sc:1) finished in 0,639 s\n",
      "24/09/06 14:53:06 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:53:06 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/09/06 14:53:06 INFO DAGScheduler: Job 0 finished: load at cell6.sc:1, took 0,702800 s\n",
      "24/09/06 14:53:07 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:53:07 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:53:07 INFO CodeGenerator: Code generated in 20.25164 ms\n",
      "24/09/06 14:53:07 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:07 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:07 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ubuntu:34403 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:07 INFO SparkContext: Created broadcast 2 from show at cell6.sc:2\n",
      "24/09/06 14:53:07 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:53:07 INFO SparkContext: Starting job: show at cell6.sc:2\n",
      "24/09/06 14:53:07 INFO DAGScheduler: Got job 1 (show at cell6.sc:2) with 1 output partitions\n",
      "24/09/06 14:53:07 INFO DAGScheduler: Final stage: ResultStage 1 (show at cell6.sc:2)\n",
      "24/09/06 14:53:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:53:07 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:53:07 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at show at cell6.sc:2), which has no missing parents\n",
      "24/09/06 14:53:07 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 17.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:07 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:07 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ubuntu:34403 (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:07 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:53:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at show at cell6.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:53:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:53:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8240 bytes) \n",
      "24/09/06 14:53:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "24/09/06 14:53:07 INFO CodeGenerator: Code generated in 23.848947 ms\n",
      "24/09/06 14:53:07 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/SQL/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:53:07 INFO CodeGenerator: Code generated in 14.66833 ms\n",
      "24/09/06 14:53:07 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2523 bytes result sent to driver\n",
      "24/09/06 14:53:07 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 138 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:53:07 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:53:07 INFO DAGScheduler: ResultStage 1 (show at cell6.sc:2) finished in 0,168 s\n",
      "24/09/06 14:53:07 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:53:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/09/06 14:53:07 INFO DAGScheduler: Job 1 finished: show at cell6.sc:2, took 0,182296 s\n",
      "24/09/06 14:53:07 INFO CodeGenerator: Code generated in 23.21896 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "|Address                                               |Birthdate          |Country       |CustomerID|Email                   |Name           |Username        |\n",
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "|Unit 1047 Box 4089\\nDPO AA 57348                      |1994-02-20 00:46:27|United Kingdom|12346     |cooperalexis@hotmail.com|Lindsay Cowan  |valenciajennifer|\n",
      "|55711 Janet Plaza Apt. 865\\nChristinachester, CT 62716|1988-06-21 00:15:34|Iceland       |12347     |timothy78@hotmail.com   |Katherine David|hillrachel      |\n",
      "|Unit 2676 Box 9352\\nDPO AA 38560                      |1974-11-26 15:30:20|Finland       |12348     |tcrawford@gmail.com     |Leslie Martinez|serranobrian    |\n",
      "|2765 Powers Meadow\\nHeatherfurt, CT 53165             |1977-05-06 23:57:35|Italy         |12349     |dustin37@yahoo.com      |Brad Cardenas  |charleshudson   |\n",
      "|17677 Mark Crest\\nWalterberg, IA 39017                |1996-09-13 19:14:27|Norway        |12350     |amyholland@yahoo.com    |Natalie Ford   |gregoryharrison |\n",
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mcustomerDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Address: string, Birthdate: string ... 5 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val customerDF = spark.read.format(\"json\").load(\"data/customer_data.json\")\n",
    "customerDF.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad0ee3cc-f323-4c82-bd63-995e29f45550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:53:07 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ubuntu:34403 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:07 INFO BlockManagerInfo: Removed broadcast_1_piece0 on ubuntu:34403 in memory (size: 7.4 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:07 INFO BlockManagerInfo: Removed broadcast_3_piece0 on ubuntu:34403 in memory (size: 7.6 KiB, free: 4.5 GiB)\n"
     ]
    }
   ],
   "source": [
    "customerDF.createOrReplaceTempView(\"customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0972a67-b860-4b7d-9ba9-ee8a629bca48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres8\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Birthdate: string, Country: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select Birthdate, Country from customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "078f44ea-b558-4745-a5a4-55d1a72adb59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:53:08 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:53:08 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:53:08 INFO CodeGenerator: Code generated in 7.827997 ms\n",
      "24/09/06 14:53:08 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:08 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ubuntu:34403 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:08 INFO SparkContext: Created broadcast 4 from show at cell9.sc:1\n",
      "24/09/06 14:53:08 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:53:08 INFO SparkContext: Starting job: show at cell9.sc:1\n",
      "24/09/06 14:53:08 INFO DAGScheduler: Got job 2 (show at cell9.sc:1) with 1 output partitions\n",
      "24/09/06 14:53:08 INFO DAGScheduler: Final stage: ResultStage 2 (show at cell9.sc:1)\n",
      "24/09/06 14:53:08 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:53:08 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:53:08 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at show at cell9.sc:1), which has no missing parents\n",
      "24/09/06 14:53:08 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 14.6 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:08 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:08 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on ubuntu:34403 (size: 7.1 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:08 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:53:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at show at cell9.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:53:08 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:53:08 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8240 bytes) \n",
      "24/09/06 14:53:08 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "24/09/06 14:53:08 INFO CodeGenerator: Code generated in 12.617289 ms\n",
      "24/09/06 14:53:08 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/SQL/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:53:09 INFO CodeGenerator: Code generated in 8.713128 ms\n",
      "24/09/06 14:53:09 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1720 bytes result sent to driver\n",
      "24/09/06 14:53:09 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 45 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:53:09 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:53:09 INFO DAGScheduler: ResultStage 2 (show at cell9.sc:1) finished in 0,061 s\n",
      "24/09/06 14:53:09 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:53:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/09/06 14:53:09 INFO DAGScheduler: Job 2 finished: show at cell9.sc:1, took 0,071129 s\n",
      "24/09/06 14:53:09 INFO CodeGenerator: Code generated in 8.906341 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+\n",
      "|Birthdate          |Country       |\n",
      "+-------------------+--------------+\n",
      "|1994-02-20 00:46:27|United Kingdom|\n",
      "|1988-06-21 00:15:34|Iceland       |\n",
      "|1974-11-26 15:30:20|Finland       |\n",
      "|1977-05-06 23:57:35|Italy         |\n",
      "|1996-09-13 19:14:27|Norway        |\n",
      "+-------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Birthdate, Country from customer\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84f82b4d-3432-47ab-831b-6144aae62bd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:53:09 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:53:09 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:53:09 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:09 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:09 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on ubuntu:34403 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:09 INFO SparkContext: Created broadcast 6 from show at cell10.sc:1\n",
      "24/09/06 14:53:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:53:09 INFO SparkContext: Starting job: show at cell10.sc:1\n",
      "24/09/06 14:53:09 INFO DAGScheduler: Got job 3 (show at cell10.sc:1) with 1 output partitions\n",
      "24/09/06 14:53:09 INFO DAGScheduler: Final stage: ResultStage 3 (show at cell10.sc:1)\n",
      "24/09/06 14:53:09 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:53:09 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:53:09 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[15] at show at cell10.sc:1), which has no missing parents\n",
      "24/09/06 14:53:09 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 14.6 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:09 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:09 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on ubuntu:34403 (size: 7.1 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:09 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:53:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[15] at show at cell10.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:53:09 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:53:09 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8240 bytes) \n",
      "24/09/06 14:53:09 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)\n",
      "24/09/06 14:53:09 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/SQL/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:53:09 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1720 bytes result sent to driver\n",
      "24/09/06 14:53:09 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 29 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:53:09 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:53:09 INFO DAGScheduler: ResultStage 3 (show at cell10.sc:1) finished in 0,048 s\n",
      "24/09/06 14:53:09 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:53:09 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "24/09/06 14:53:09 INFO DAGScheduler: Job 3 finished: show at cell10.sc:1, took 0,056096 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+\n",
      "|Date               |Country       |\n",
      "+-------------------+--------------+\n",
      "|1994-02-20 00:46:27|United Kingdom|\n",
      "|1988-06-21 00:15:34|Iceland       |\n",
      "|1974-11-26 15:30:20|Finland       |\n",
      "|1977-05-06 23:57:35|Italy         |\n",
      "|1996-09-13 19:14:27|Norway        |\n",
      "+-------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Birthdate as Date, Country from customer\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab4fa6db-bd77-4f41-b7d9-47c5b7298a54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:53:09 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:53:09 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:53:09 INFO CodeGenerator: Code generated in 11.202559 ms\n",
      "24/09/06 14:53:09 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:09 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:09 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on ubuntu:34403 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:09 INFO SparkContext: Created broadcast 8 from show at cell11.sc:1\n",
      "24/09/06 14:53:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:53:09 INFO SparkContext: Starting job: show at cell11.sc:1\n",
      "24/09/06 14:53:09 INFO DAGScheduler: Got job 4 (show at cell11.sc:1) with 1 output partitions\n",
      "24/09/06 14:53:09 INFO DAGScheduler: Final stage: ResultStage 4 (show at cell11.sc:1)\n",
      "24/09/06 14:53:09 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:53:10 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:53:10 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at show at cell11.sc:1), which has no missing parents\n",
      "24/09/06 14:53:10 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 14.8 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:10 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:10 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on ubuntu:34403 (size: 7.2 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:10 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:53:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at show at cell11.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:53:10 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:53:10 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8240 bytes) \n",
      "24/09/06 14:53:10 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)\n",
      "24/09/06 14:53:10 INFO CodeGenerator: Code generated in 11.152089 ms\n",
      "24/09/06 14:53:10 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/SQL/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:53:10 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1733 bytes result sent to driver\n",
      "24/09/06 14:53:10 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 39 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:53:10 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:53:10 INFO DAGScheduler: ResultStage 4 (show at cell11.sc:1) finished in 0,056 s\n",
      "24/09/06 14:53:10 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:53:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "24/09/06 14:53:10 INFO DAGScheduler: Job 4 finished: show at cell11.sc:1, took 0,067753 s\n",
      "24/09/06 14:53:10 INFO CodeGenerator: Code generated in 9.028919 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+----+\n",
      "|Date               |Country       |Flag|\n",
      "+-------------------+--------------+----+\n",
      "|1994-02-20 00:46:27|United Kingdom|true|\n",
      "|1988-06-21 00:15:34|Iceland       |true|\n",
      "|1974-11-26 15:30:20|Finland       |true|\n",
      "|1977-05-06 23:57:35|Italy         |true|\n",
      "|1996-09-13 19:14:27|Norway        |true|\n",
      "+-------------------+--------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Birthdate as Date, Country, true as Flag from customer\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faa4178a-4f61-4403-acbb-5b33f7806c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Flag: boolean (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Birthdate as Date, Country, true as Flag from customer\").printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a665459b-edb1-433f-a231-92e7dadb44a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:53:10 INFO FileSourceStrategy: Pushed Filters: IsNotNull(Country),EqualTo(Country,Norway)\n",
      "24/09/06 14:53:10 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(Country#10),(Country#10 = Norway)\n",
      "24/09/06 14:53:10 INFO CodeGenerator: Code generated in 15.21472 ms\n",
      "24/09/06 14:53:10 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:10 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:10 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on ubuntu:34403 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:10 INFO SparkContext: Created broadcast 10 from show at cell13.sc:1\n",
      "24/09/06 14:53:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:53:10 INFO SparkContext: Starting job: show at cell13.sc:1\n",
      "24/09/06 14:53:10 INFO DAGScheduler: Got job 5 (show at cell13.sc:1) with 1 output partitions\n",
      "24/09/06 14:53:10 INFO DAGScheduler: Final stage: ResultStage 5 (show at cell13.sc:1)\n",
      "24/09/06 14:53:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:53:10 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:53:10 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[23] at show at cell13.sc:1), which has no missing parents\n",
      "24/09/06 14:53:10 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 17.7 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:10 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:10 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on ubuntu:34403 (size: 7.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:10 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:53:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[23] at show at cell13.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:53:10 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:53:10 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8240 bytes) \n",
      "24/09/06 14:53:10 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)\n",
      "24/09/06 14:53:10 INFO CodeGenerator: Code generated in 14.762938 ms\n",
      "24/09/06 14:53:10 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/SQL/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:53:10 INFO CodeGenerator: Code generated in 6.187183 ms\n",
      "24/09/06 14:53:10 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 2501 bytes result sent to driver\n",
      "24/09/06 14:53:10 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 66 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:53:10 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:53:10 INFO DAGScheduler: ResultStage 5 (show at cell13.sc:1) finished in 0,084 s\n",
      "24/09/06 14:53:10 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:53:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "24/09/06 14:53:10 INFO DAGScheduler: Job 5 finished: show at cell13.sc:1, took 0,093326 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+-------------------+-------+----------+-------------------------+--------------+---------------+\n",
      "|Address                                           |Birthdate          |Country|CustomerID|Email                    |Name          |Username       |\n",
      "+--------------------------------------------------+-------------------+-------+----------+-------------------------+--------------+---------------+\n",
      "|17677 Mark Crest\\nWalterberg, IA 39017            |1996-09-13 19:14:27|Norway |12350     |amyholland@yahoo.com     |Natalie Ford  |gregoryharrison|\n",
      "|50047 Smith Point Suite 162\\nWilkinsstad, PA 04106|1969-06-21 03:39:20|Norway |12352     |vcarter@hotmail.com      |Dana Clarke   |hmyers         |\n",
      "|0297 Jacob Ranch Apt. 019\\nNorth Judith, NV 27455 |1990-06-01 14:49:52|Norway |12381     |douglaschavez@hotmail.com|Matthew Jones |stephanie68    |\n",
      "|3102 Hopkins Walk\\nAndrechester, MD 54461         |1976-06-19 08:10:24|Norway |12430     |crystalromero@hotmail.com|Lisa Tate     |pgilbert       |\n",
      "|637 Philip Lock Suite 286\\nJohnsmouth, RI 96778   |1984-06-06 09:36:14|Norway |12432     |jessica87@hotmail.com    |Cheryl Herring|mathewsnicholas|\n",
      "+--------------------------------------------------+-------------------+-------+----------+-------------------------+--------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from customer where Country = 'Norway'\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da2d62a0-3279-4c3a-8ef9-84e2ba1edf4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:53:11 INFO BlockManagerInfo: Removed broadcast_5_piece0 on ubuntu:34403 in memory (size: 7.1 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:11 INFO BlockManagerInfo: Removed broadcast_2_piece0 on ubuntu:34403 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:11 INFO BlockManagerInfo: Removed broadcast_11_piece0 on ubuntu:34403 in memory (size: 7.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:11 INFO BlockManagerInfo: Removed broadcast_7_piece0 on ubuntu:34403 in memory (size: 7.1 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:11 INFO BlockManagerInfo: Removed broadcast_6_piece0 on ubuntu:34403 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:11 INFO BlockManagerInfo: Removed broadcast_9_piece0 on ubuntu:34403 in memory (size: 7.2 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:11 INFO BlockManagerInfo: Removed broadcast_4_piece0 on ubuntu:34403 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:11 INFO BlockManagerInfo: Removed broadcast_8_piece0 on ubuntu:34403 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:11 INFO FileSourceStrategy: Pushed Filters: IsNotNull(Country),Not(EqualTo(Country,Iceland))\n",
      "24/09/06 14:53:11 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(Country#10),NOT (Country#10 = Iceland)\n",
      "24/09/06 14:53:11 INFO CodeGenerator: Code generated in 9.481079 ms\n",
      "24/09/06 14:53:11 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:11 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:11 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on ubuntu:34403 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:11 INFO SparkContext: Created broadcast 12 from show at cell14.sc:1\n",
      "24/09/06 14:53:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:53:11 INFO SparkContext: Starting job: show at cell14.sc:1\n",
      "24/09/06 14:53:11 INFO DAGScheduler: Got job 6 (show at cell14.sc:1) with 1 output partitions\n",
      "24/09/06 14:53:11 INFO DAGScheduler: Final stage: ResultStage 6 (show at cell14.sc:1)\n",
      "24/09/06 14:53:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:53:11 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:53:11 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at show at cell14.sc:1), which has no missing parents\n",
      "24/09/06 14:53:11 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 17.8 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:11 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 8.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:11 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on ubuntu:34403 (size: 8.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:11 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:53:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at show at cell14.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:53:11 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:53:11 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8240 bytes) \n",
      "24/09/06 14:53:11 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)\n",
      "24/09/06 14:53:11 INFO CodeGenerator: Code generated in 15.674144 ms\n",
      "24/09/06 14:53:11 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/SQL/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:53:11 INFO CodeGenerator: Code generated in 6.013973 ms\n",
      "24/09/06 14:53:11 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 2517 bytes result sent to driver\n",
      "24/09/06 14:53:11 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 44 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:53:11 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:53:11 INFO DAGScheduler: ResultStage 6 (show at cell14.sc:1) finished in 0,062 s\n",
      "24/09/06 14:53:11 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:53:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "24/09/06 14:53:11 INFO DAGScheduler: Job 6 finished: show at cell14.sc:1, took 0,071768 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "|Address                                           |Birthdate          |Country       |CustomerID|Email                   |Name           |Username        |\n",
      "+--------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "|Unit 1047 Box 4089\\nDPO AA 57348                  |1994-02-20 00:46:27|United Kingdom|12346     |cooperalexis@hotmail.com|Lindsay Cowan  |valenciajennifer|\n",
      "|Unit 2676 Box 9352\\nDPO AA 38560                  |1974-11-26 15:30:20|Finland       |12348     |tcrawford@gmail.com     |Leslie Martinez|serranobrian    |\n",
      "|2765 Powers Meadow\\nHeatherfurt, CT 53165         |1977-05-06 23:57:35|Italy         |12349     |dustin37@yahoo.com      |Brad Cardenas  |charleshudson   |\n",
      "|17677 Mark Crest\\nWalterberg, IA 39017            |1996-09-13 19:14:27|Norway        |12350     |amyholland@yahoo.com    |Natalie Ford   |gregoryharrison |\n",
      "|50047 Smith Point Suite 162\\nWilkinsstad, PA 04106|1969-06-21 03:39:20|Norway        |12352     |vcarter@hotmail.com     |Dana Clarke    |hmyers          |\n",
      "+--------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from customer where Country <> 'Iceland'\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1009b655-c60c-4ab1-a45a-c4061edbcbfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:53:11 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:53:11 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:53:12 INFO CodeGenerator: Code generated in 12.383984 ms\n",
      "24/09/06 14:53:12 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:12 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:12 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on ubuntu:34403 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:12 INFO SparkContext: Created broadcast 14 from show at cell15.sc:1\n",
      "24/09/06 14:53:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:53:12 INFO SparkContext: Starting job: show at cell15.sc:1\n",
      "24/09/06 14:53:12 INFO DAGScheduler: Got job 7 (show at cell15.sc:1) with 1 output partitions\n",
      "24/09/06 14:53:12 INFO DAGScheduler: Final stage: ResultStage 7 (show at cell15.sc:1)\n",
      "24/09/06 14:53:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:53:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:53:12 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[31] at show at cell15.sc:1), which has no missing parents\n",
      "24/09/06 14:53:12 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 14.3 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:12 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:12 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on ubuntu:34403 (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:12 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:53:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[31] at show at cell15.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:53:12 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:53:12 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8240 bytes) \n",
      "24/09/06 14:53:12 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)\n",
      "24/09/06 14:53:12 INFO CodeGenerator: Code generated in 7.305581 ms\n",
      "24/09/06 14:53:12 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/SQL/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:53:12 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 2833 bytes result sent to driver\n",
      "24/09/06 14:53:12 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 79 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:53:12 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:53:12 INFO DAGScheduler: ResultStage 7 (show at cell15.sc:1) finished in 0,092 s\n",
      "24/09/06 14:53:12 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:53:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "24/09/06 14:53:12 INFO DAGScheduler: Job 7 finished: show at cell15.sc:1, took 0,097009 s\n",
      "24/09/06 14:53:12 INFO CodeGenerator: Code generated in 12.495318 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------+-------------------+--------------+----------+-----------------------+--------------------+-----------------+\n",
      "|Address                                             |Birthdate          |Country       |CustomerID|Email                  |Name                |Username         |\n",
      "+----------------------------------------------------+-------------------+--------------+----------+-----------------------+--------------------+-----------------+\n",
      "|6942 Connie Skyway\\nPatrickville, WA 16551          |1973-10-24 00:52:10|United Kingdom|12989     |amber97@hotmail.com    |Brandon Contreras   |ecasey           |\n",
      "|79375 David Neck\\nWest Matthewton, NJ 92863         |1971-05-04 22:20:10|United Kingdom|12988     |erica98@gmail.com      |Gabriel Romero      |qknight          |\n",
      "|00881 West Flat\\nNorth Emily, IL 32130              |1997-03-05 19:20:57|United Kingdom|12987     |vkeith@yahoo.com       |Christopher Lawrence|smcintyre        |\n",
      "|499 Jonathan Streets Apt. 890\\nEast Ashley, MD 76825|1987-10-24 20:05:15|United Kingdom|12985     |fredsmith@yahoo.com    |Xavier Myers        |stricklandjeffery|\n",
      "|9505 Melissa Streets\\nSouth Frankville, NJ 91189    |1975-09-22 15:21:58|United Kingdom|12984     |scottjonathan@yahoo.com|Brandy Huang        |amandawilliams   |\n",
      "+----------------------------------------------------+-------------------+--------------+----------+-----------------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from customer order by CustomerID desc\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "892882e0-4621-4c37-aeff-0ba555e03efc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:53:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:53:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:53:12 INFO CodeGenerator: Code generated in 11.513564 ms\n",
      "24/09/06 14:53:12 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:12 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:12 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on ubuntu:34403 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:12 INFO SparkContext: Created broadcast 16 from show at cell16.sc:1\n",
      "24/09/06 14:53:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:53:12 INFO SparkContext: Starting job: show at cell16.sc:1\n",
      "24/09/06 14:53:12 INFO DAGScheduler: Got job 8 (show at cell16.sc:1) with 1 output partitions\n",
      "24/09/06 14:53:12 INFO DAGScheduler: Final stage: ResultStage 8 (show at cell16.sc:1)\n",
      "24/09/06 14:53:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:53:12 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:53:12 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[35] at show at cell16.sc:1), which has no missing parents\n",
      "24/09/06 14:53:12 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 16.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:12 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:12 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on ubuntu:34403 (size: 7.7 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:12 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:53:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[35] at show at cell16.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:53:12 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:53:12 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8240 bytes) \n",
      "24/09/06 14:53:12 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)\n",
      "24/09/06 14:53:12 INFO CodeGenerator: Code generated in 16.475209 ms\n",
      "24/09/06 14:53:12 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/SQL/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:53:12 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 1705 bytes result sent to driver\n",
      "24/09/06 14:53:12 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 35 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:53:12 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:53:12 INFO DAGScheduler: ResultStage 8 (show at cell16.sc:1) finished in 0,049 s\n",
      "24/09/06 14:53:12 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:53:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "24/09/06 14:53:12 INFO DAGScheduler: Job 8 finished: show at cell16.sc:1, took 0,054767 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|Birthdate          |Date      |\n",
      "+-------------------+----------+\n",
      "|1994-02-20 00:46:27|1994-02-20|\n",
      "|1988-06-21 00:15:34|1988-06-21|\n",
      "|1974-11-26 15:30:20|1974-11-26|\n",
      "|1977-05-06 23:57:35|1977-05-06|\n",
      "|1996-09-13 19:14:27|1996-09-13|\n",
      "+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Birthdate, date_format(Birthdate, 'yyyy-MM-dd') as Date from customer\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76529542-c892-4968-b75c-0dc0da799bb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:53:13 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:53:13 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:53:13 INFO CodeGenerator: Code generated in 7.427655 ms\n",
      "24/09/06 14:53:13 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:13 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:13 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on ubuntu:34403 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:13 INFO SparkContext: Created broadcast 18 from show at cell17.sc:1\n",
      "24/09/06 14:53:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:53:13 INFO SparkContext: Starting job: show at cell17.sc:1\n",
      "24/09/06 14:53:13 INFO DAGScheduler: Got job 9 (show at cell17.sc:1) with 1 output partitions\n",
      "24/09/06 14:53:13 INFO DAGScheduler: Final stage: ResultStage 9 (show at cell17.sc:1)\n",
      "24/09/06 14:53:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:53:13 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:53:13 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[39] at show at cell17.sc:1), which has no missing parents\n",
      "24/09/06 14:53:13 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 16.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:13 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:13 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on ubuntu:34403 (size: 7.7 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:13 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:53:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[39] at show at cell17.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:53:13 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:53:13 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8240 bytes) \n",
      "24/09/06 14:53:13 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)\n",
      "24/09/06 14:53:13 INFO CodeGenerator: Code generated in 9.542299 ms\n",
      "24/09/06 14:53:13 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/SQL/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:53:13 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 1875 bytes result sent to driver\n",
      "24/09/06 14:53:13 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 27 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:53:13 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:53:13 INFO DAGScheduler: ResultStage 9 (show at cell17.sc:1) finished in 0,045 s\n",
      "24/09/06 14:53:13 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:53:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "24/09/06 14:53:13 INFO DAGScheduler: Job 9 finished: show at cell17.sc:1, took 0,051530 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+---------------------------------+\n",
      "|Name           |Username        |Identity                         |\n",
      "+---------------+----------------+---------------------------------+\n",
      "|Lindsay Cowan  |valenciajennifer|[Lindsay Cowan, valenciajennifer]|\n",
      "|Katherine David|hillrachel      |[Katherine David, hillrachel]    |\n",
      "|Leslie Martinez|serranobrian    |[Leslie Martinez, serranobrian]  |\n",
      "|Brad Cardenas  |charleshudson   |[Brad Cardenas, charleshudson]   |\n",
      "|Natalie Ford   |gregoryharrison |[Natalie Ford, gregoryharrison]  |\n",
      "+---------------+----------------+---------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Name, Username, array(Name, Username) as Identity from customer\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23a354ca-f39f-4aba-8138-081597ebe1d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:53:13 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:53:13 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:53:13 INFO CodeGenerator: Code generated in 33.192484 ms\n",
      "24/09/06 14:53:13 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:13 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:13 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on ubuntu:34403 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:13 INFO SparkContext: Created broadcast 20 from show at cell18.sc:1\n",
      "24/09/06 14:53:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:53:13 INFO DAGScheduler: Registering RDD 43 (show at cell18.sc:1) as input to shuffle 0\n",
      "24/09/06 14:53:13 INFO DAGScheduler: Got map stage job 10 (show at cell18.sc:1) with 1 output partitions\n",
      "24/09/06 14:53:13 INFO DAGScheduler: Final stage: ShuffleMapStage 10 (show at cell18.sc:1)\n",
      "24/09/06 14:53:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:53:13 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:53:13 INFO DAGScheduler: Submitting ShuffleMapStage 10 (MapPartitionsRDD[43] at show at cell18.sc:1), which has no missing parents\n",
      "24/09/06 14:53:13 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 37.9 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:13 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:13 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on ubuntu:34403 (size: 17.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:13 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:53:13 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 10 (MapPartitionsRDD[43] at show at cell18.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:53:13 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:53:13 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8229 bytes) \n",
      "24/09/06 14:53:13 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)\n",
      "24/09/06 14:53:13 INFO CodeGenerator: Code generated in 20.355958 ms\n",
      "24/09/06 14:53:13 INFO CodeGenerator: Code generated in 7.68414 ms\n",
      "24/09/06 14:53:13 INFO CodeGenerator: Code generated in 4.147283 ms\n",
      "24/09/06 14:53:13 INFO CodeGenerator: Code generated in 5.134389 ms\n",
      "24/09/06 14:53:13 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/SQL/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:53:13 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 2759 bytes result sent to driver\n",
      "24/09/06 14:53:13 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 143 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:53:13 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:53:13 INFO DAGScheduler: ShuffleMapStage 10 (show at cell18.sc:1) finished in 0,167 s\n",
      "24/09/06 14:53:13 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/06 14:53:13 INFO DAGScheduler: running: Set()\n",
      "24/09/06 14:53:13 INFO DAGScheduler: waiting: Set()\n",
      "24/09/06 14:53:13 INFO DAGScheduler: failed: Set()\n",
      "24/09/06 14:53:13 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/09/06 14:53:13 INFO CodeGenerator: Code generated in 4.87224 ms\n",
      "24/09/06 14:53:13 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/09/06 14:53:13 INFO CodeGenerator: Code generated in 11.829273 ms\n",
      "24/09/06 14:53:13 INFO SparkContext: Starting job: show at cell18.sc:1\n",
      "24/09/06 14:53:13 INFO DAGScheduler: Got job 11 (show at cell18.sc:1) with 1 output partitions\n",
      "24/09/06 14:53:13 INFO DAGScheduler: Final stage: ResultStage 12 (show at cell18.sc:1)\n",
      "24/09/06 14:53:13 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\n",
      "24/09/06 14:53:13 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:53:13 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[47] at show at cell18.sc:1), which has no missing parents\n",
      "24/09/06 14:53:14 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 40.6 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:14 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 19.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:14 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on ubuntu:34403 (size: 19.4 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:14 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:53:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[47] at show at cell18.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:53:14 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:53:14 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 11) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/06 14:53:14 INFO Executor: Running task 0.0 in stage 12.0 (TID 11)\n",
      "24/09/06 14:53:14 INFO CodeGenerator: Code generated in 3.842114 ms\n",
      "24/09/06 14:53:14 INFO ShuffleBlockFetcherIterator: Getting 1 (2.2 KiB) non-empty blocks including 1 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/06 14:53:14 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 9 ms\n",
      "24/09/06 14:53:14 INFO CodeGenerator: Code generated in 11.982867 ms\n",
      "24/09/06 14:53:14 INFO Executor: Finished task 0.0 in stage 12.0 (TID 11). 5371 bytes result sent to driver\n",
      "24/09/06 14:53:14 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 11) in 80 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:53:14 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:53:14 INFO DAGScheduler: ResultStage 12 (show at cell18.sc:1) finished in 0,093 s\n",
      "24/09/06 14:53:14 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:53:14 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
      "24/09/06 14:53:14 INFO DAGScheduler: Job 11 finished: show at cell18.sc:1, took 0,108439 s\n",
      "24/09/06 14:53:14 INFO CodeGenerator: Code generated in 7.607348 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|Country       |count|\n",
      "+--------------+-----+\n",
      "|United Kingdom|124  |\n",
      "|Germany       |86   |\n",
      "|France        |86   |\n",
      "|Spain         |30   |\n",
      "|Belgium       |25   |\n",
      "+--------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Country, count(*) as count from customer group by Country order by count desc\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b8d5c3b-05a9-4cb9-97ed-85fdf66d8f82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:53:40 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:53:40 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:53:40 INFO CodeGenerator: Code generated in 10.732957 ms\n",
      "24/09/06 14:53:40 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:40 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:40 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on ubuntu:34403 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:40 INFO SparkContext: Created broadcast 23 from show at cell19.sc:10\n",
      "24/09/06 14:53:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:53:40 INFO BlockManagerInfo: Removed broadcast_22_piece0 on ubuntu:34403 in memory (size: 19.4 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:40 INFO DAGScheduler: Registering RDD 52 (show at cell19.sc:10) as input to shuffle 1\n",
      "24/09/06 14:53:40 INFO DAGScheduler: Got map stage job 12 (show at cell19.sc:10) with 1 output partitions\n",
      "24/09/06 14:53:40 INFO DAGScheduler: Final stage: ShuffleMapStage 13 (show at cell19.sc:10)\n",
      "24/09/06 14:53:40 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:53:40 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:53:40 INFO BlockManagerInfo: Removed broadcast_13_piece0 on ubuntu:34403 in memory (size: 8.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:40 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[52] at show at cell19.sc:10), which has no missing parents\n",
      "24/09/06 14:53:40 INFO BlockManagerInfo: Removed broadcast_18_piece0 on ubuntu:34403 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:40 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 34.6 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:40 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 16.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:40 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on ubuntu:34403 (size: 16.4 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:40 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:53:40 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[52] at show at cell19.sc:10) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:53:40 INFO BlockManagerInfo: Removed broadcast_20_piece0 on ubuntu:34403 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:40 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:53:40 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 12) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8229 bytes) \n",
      "24/09/06 14:53:40 INFO Executor: Running task 0.0 in stage 13.0 (TID 12)\n",
      "24/09/06 14:53:40 INFO BlockManagerInfo: Removed broadcast_16_piece0 on ubuntu:34403 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:40 INFO BlockManagerInfo: Removed broadcast_19_piece0 on ubuntu:34403 in memory (size: 7.7 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:40 INFO BlockManagerInfo: Removed broadcast_12_piece0 on ubuntu:34403 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:40 INFO BlockManagerInfo: Removed broadcast_15_piece0 on ubuntu:34403 in memory (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:40 INFO BlockManagerInfo: Removed broadcast_14_piece0 on ubuntu:34403 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:40 INFO CodeGenerator: Code generated in 20.98966 ms\n",
      "24/09/06 14:53:40 INFO BlockManagerInfo: Removed broadcast_17_piece0 on ubuntu:34403 in memory (size: 7.7 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:40 INFO BlockManagerInfo: Removed broadcast_21_piece0 on ubuntu:34403 in memory (size: 17.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:40 INFO BlockManagerInfo: Removed broadcast_10_piece0 on ubuntu:34403 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:40 INFO CodeGenerator: Code generated in 5.95685 ms\n",
      "24/09/06 14:53:40 INFO CodeGenerator: Code generated in 8.27012 ms\n",
      "24/09/06 14:53:40 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/SQL/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:53:40 INFO CodeGenerator: Code generated in 5.307398 ms\n",
      "24/09/06 14:53:40 INFO CodeGenerator: Code generated in 3.321901 ms\n",
      "24/09/06 14:53:40 INFO CodeGenerator: Code generated in 4.980214 ms\n",
      "24/09/06 14:53:40 INFO CodeGenerator: Code generated in 4.118148 ms\n",
      "24/09/06 14:53:40 INFO CodeGenerator: Code generated in 3.734774 ms\n",
      "24/09/06 14:53:40 INFO CodeGenerator: Code generated in 6.392668 ms\n",
      "24/09/06 14:53:40 INFO Executor: Finished task 0.0 in stage 13.0 (TID 12). 2523 bytes result sent to driver\n",
      "24/09/06 14:53:40 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 12) in 219 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:53:40 INFO DAGScheduler: ShuffleMapStage 13 (show at cell19.sc:10) finished in 0,241 s\n",
      "24/09/06 14:53:40 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:53:40 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/06 14:53:40 INFO DAGScheduler: running: Set()\n",
      "24/09/06 14:53:40 INFO DAGScheduler: waiting: Set()\n",
      "24/09/06 14:53:40 INFO DAGScheduler: failed: Set()\n",
      "24/09/06 14:53:40 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/09/06 14:53:40 INFO CodeGenerator: Code generated in 4.21984 ms\n",
      "24/09/06 14:53:40 INFO CodeGenerator: Code generated in 7.143759 ms\n",
      "24/09/06 14:53:40 INFO SparkContext: Starting job: show at cell19.sc:10\n",
      "24/09/06 14:53:40 INFO DAGScheduler: Got job 13 (show at cell19.sc:10) with 1 output partitions\n",
      "24/09/06 14:53:40 INFO DAGScheduler: Final stage: ResultStage 15 (show at cell19.sc:10)\n",
      "24/09/06 14:53:40 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)\n",
      "24/09/06 14:53:40 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:53:40 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[57] at show at cell19.sc:10), which has no missing parents\n",
      "24/09/06 14:53:40 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 43.5 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:40 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 20.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:40 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on ubuntu:34403 (size: 20.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:40 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:53:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[57] at show at cell19.sc:10) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:53:40 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:53:40 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 13) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/06 14:53:40 INFO Executor: Running task 0.0 in stage 15.0 (TID 13)\n",
      "24/09/06 14:53:40 INFO CodeGenerator: Code generated in 5.369298 ms\n",
      "24/09/06 14:53:40 INFO ShuffleBlockFetcherIterator: Getting 1 (36.1 KiB) non-empty blocks including 1 (36.1 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/06 14:53:40 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "24/09/06 14:53:40 INFO CodeGenerator: Code generated in 6.66435 ms\n",
      "24/09/06 14:53:40 INFO CodeGenerator: Code generated in 4.25592 ms\n",
      "24/09/06 14:53:40 INFO CodeGenerator: Code generated in 6.099504 ms\n",
      "24/09/06 14:53:40 INFO CodeGenerator: Code generated in 4.088229 ms\n",
      "24/09/06 14:53:40 INFO Executor: Finished task 0.0 in stage 15.0 (TID 13). 6514 bytes result sent to driver\n",
      "24/09/06 14:53:40 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 13) in 76 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:53:40 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:53:40 INFO DAGScheduler: ResultStage 15 (show at cell19.sc:10) finished in 0,087 s\n",
      "24/09/06 14:53:40 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:53:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished\n",
      "24/09/06 14:53:40 INFO DAGScheduler: Job 13 finished: show at cell19.sc:10, took 0,097761 s\n",
      "24/09/06 14:53:40 INFO CodeGenerator: Code generated in 8.246603 ms\n",
      "24/09/06 14:53:40 INFO CodeGenerator: Code generated in 5.355507 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------------+---------------+\n",
      "|Country  |bd        |min(CustomerID)|max(CustomerID)|\n",
      "+---------+----------+---------------+---------------+\n",
      "|Australia|1966-09-17|12422          |12422          |\n",
      "|Australia|1967-11-29|12424          |12424          |\n",
      "|Australia|1985-12-30|12386          |12386          |\n",
      "|Australia|1986-08-28|12434          |12434          |\n",
      "|Australia|1987-02-26|12393          |12393          |\n",
      "+---------+----------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select\n",
    "  Country\n",
    "  , date_format(Birthdate, 'yyyy-MM-dd') as bd\n",
    "  , min(CustomerID)\n",
    "  , max(CustomerID)\n",
    "from customer\n",
    "group by Country, bd\n",
    "order by Country\"\"\")\n",
    ".show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24f30413-3569-4183-b47f-74878e88c7ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:53:45 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:53:45 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:53:45 INFO CodeGenerator: Code generated in 5.614474 ms\n",
      "24/09/06 14:53:45 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:45 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:45 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on ubuntu:34403 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:45 INFO SparkContext: Created broadcast 26 from show at cell20.sc:1\n",
      "24/09/06 14:53:45 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:53:45 INFO DAGScheduler: Registering RDD 61 (show at cell20.sc:1) as input to shuffle 2\n",
      "24/09/06 14:53:45 INFO DAGScheduler: Got map stage job 14 (show at cell20.sc:1) with 1 output partitions\n",
      "24/09/06 14:53:45 INFO DAGScheduler: Final stage: ShuffleMapStage 16 (show at cell20.sc:1)\n",
      "24/09/06 14:53:45 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:53:45 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:53:45 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[61] at show at cell20.sc:1), which has no missing parents\n",
      "24/09/06 14:53:45 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 17.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:45 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:45 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on ubuntu:34403 (size: 8.3 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:45 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:53:45 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[61] at show at cell20.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:53:45 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:53:45 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 14) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8229 bytes) \n",
      "24/09/06 14:53:45 INFO Executor: Running task 0.0 in stage 16.0 (TID 14)\n",
      "24/09/06 14:53:45 INFO CodeGenerator: Code generated in 6.075134 ms\n",
      "24/09/06 14:53:45 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/SQL/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:53:45 INFO CodeGenerator: Code generated in 3.934468 ms\n",
      "24/09/06 14:53:45 INFO Executor: Finished task 0.0 in stage 16.0 (TID 14). 1922 bytes result sent to driver\n",
      "24/09/06 14:53:45 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 14) in 29 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:53:45 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:53:45 INFO DAGScheduler: ShuffleMapStage 16 (show at cell20.sc:1) finished in 0,040 s\n",
      "24/09/06 14:53:45 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/06 14:53:45 INFO DAGScheduler: running: Set()\n",
      "24/09/06 14:53:45 INFO DAGScheduler: waiting: Set()\n",
      "24/09/06 14:53:45 INFO DAGScheduler: failed: Set()\n",
      "24/09/06 14:53:45 INFO CodeGenerator: Code generated in 7.4012 ms\n",
      "24/09/06 14:53:45 INFO SparkContext: Starting job: show at cell20.sc:1\n",
      "24/09/06 14:53:45 INFO DAGScheduler: Got job 15 (show at cell20.sc:1) with 1 output partitions\n",
      "24/09/06 14:53:45 INFO DAGScheduler: Final stage: ResultStage 18 (show at cell20.sc:1)\n",
      "24/09/06 14:53:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)\n",
      "24/09/06 14:53:45 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:53:45 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[64] at show at cell20.sc:1), which has no missing parents\n",
      "24/09/06 14:53:45 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 12.6 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:45 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:45 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on ubuntu:34403 (size: 6.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:45 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:53:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[64] at show at cell20.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:53:45 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:53:45 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 15) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/06 14:53:45 INFO Executor: Running task 0.0 in stage 18.0 (TID 15)\n",
      "24/09/06 14:53:45 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/06 14:53:45 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\n",
      "24/09/06 14:53:45 INFO CodeGenerator: Code generated in 9.200991 ms\n",
      "24/09/06 14:53:45 INFO Executor: Finished task 0.0 in stage 18.0 (TID 15). 4004 bytes result sent to driver\n",
      "24/09/06 14:53:45 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 15) in 24 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:53:45 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:53:45 INFO DAGScheduler: ResultStage 18 (show at cell20.sc:1) finished in 0,032 s\n",
      "24/09/06 14:53:45 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:53:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished\n",
      "24/09/06 14:53:45 INFO DAGScheduler: Job 15 finished: show at cell20.sc:1, took 0,040782 s\n",
      "24/09/06 14:53:45 INFO CodeGenerator: Code generated in 3.950841 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     507|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from customer\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab9600af-8959-4b6b-9943-034aa5e3d2f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:53:48 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:53:48 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:53:48 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:53:48 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:53:48 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:48 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:48 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on ubuntu:34403 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:48 INFO SparkContext: Created broadcast 29 from show at cell21.sc:1\n",
      "24/09/06 14:53:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:53:48 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:48 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:48 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on ubuntu:34403 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:48 INFO SparkContext: Created broadcast 30 from show at cell21.sc:1\n",
      "24/09/06 14:53:48 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:53:48 INFO DAGScheduler: Registering RDD 71 (show at cell21.sc:1) as input to shuffle 3\n",
      "24/09/06 14:53:48 INFO DAGScheduler: Got map stage job 16 (show at cell21.sc:1) with 2 output partitions\n",
      "24/09/06 14:53:48 INFO DAGScheduler: Final stage: ShuffleMapStage 19 (show at cell21.sc:1)\n",
      "24/09/06 14:53:48 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:53:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:53:48 INFO DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[71] at show at cell21.sc:1), which has no missing parents\n",
      "24/09/06 14:53:48 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 18.1 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:48 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:48 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on ubuntu:34403 (size: 8.9 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:48 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:53:48 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[71] at show at cell21.sc:1) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/09/06 14:53:48 INFO TaskSchedulerImpl: Adding task set 19.0 with 2 tasks resource profile 0\n",
      "24/09/06 14:53:48 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 16) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8338 bytes) \n",
      "24/09/06 14:53:48 INFO TaskSetManager: Starting task 1.0 in stage 19.0 (TID 17) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 8338 bytes) \n",
      "24/09/06 14:53:48 INFO Executor: Running task 0.0 in stage 19.0 (TID 16)\n",
      "24/09/06 14:53:48 INFO Executor: Running task 1.0 in stage 19.0 (TID 17)\n",
      "24/09/06 14:53:48 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/SQL/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:53:48 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/SQL/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:53:48 INFO Executor: Finished task 0.0 in stage 19.0 (TID 16). 1978 bytes result sent to driver\n",
      "24/09/06 14:53:48 INFO Executor: Finished task 1.0 in stage 19.0 (TID 17). 1978 bytes result sent to driver\n",
      "24/09/06 14:53:48 INFO TaskSetManager: Finished task 1.0 in stage 19.0 (TID 17) in 25 ms on ubuntu (executor driver) (1/2)\n",
      "24/09/06 14:53:48 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 16) in 27 ms on ubuntu (executor driver) (2/2)\n",
      "24/09/06 14:53:48 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:53:48 INFO DAGScheduler: ShuffleMapStage 19 (show at cell21.sc:1) finished in 0,038 s\n",
      "24/09/06 14:53:48 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/09/06 14:53:48 INFO DAGScheduler: running: Set()\n",
      "24/09/06 14:53:48 INFO DAGScheduler: waiting: Set()\n",
      "24/09/06 14:53:48 INFO DAGScheduler: failed: Set()\n",
      "24/09/06 14:53:48 INFO SparkContext: Starting job: show at cell21.sc:1\n",
      "24/09/06 14:53:48 INFO DAGScheduler: Got job 17 (show at cell21.sc:1) with 1 output partitions\n",
      "24/09/06 14:53:48 INFO DAGScheduler: Final stage: ResultStage 21 (show at cell21.sc:1)\n",
      "24/09/06 14:53:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)\n",
      "24/09/06 14:53:48 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:53:48 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[74] at show at cell21.sc:1), which has no missing parents\n",
      "24/09/06 14:53:48 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 12.6 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:48 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:53:48 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on ubuntu:34403 (size: 6.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:53:48 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:53:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[74] at show at cell21.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:53:48 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:53:48 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 18) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/09/06 14:53:48 INFO Executor: Running task 0.0 in stage 21.0 (TID 18)\n",
      "24/09/06 14:53:48 INFO ShuffleBlockFetcherIterator: Getting 2 (120.0 B) non-empty blocks including 2 (120.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/09/06 14:53:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "24/09/06 14:53:48 INFO Executor: Finished task 0.0 in stage 21.0 (TID 18). 4004 bytes result sent to driver\n",
      "24/09/06 14:53:48 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 18) in 9 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:53:48 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:53:48 INFO DAGScheduler: ResultStage 21 (show at cell21.sc:1) finished in 0,015 s\n",
      "24/09/06 14:53:48 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:53:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished\n",
      "24/09/06 14:53:48 INFO DAGScheduler: Job 17 finished: show at cell21.sc:1, took 0,019291 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    1014|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select count(*) from (\n",
    "  select * from customer\n",
    "  union all\n",
    "  select * from customer\n",
    ")\"\"\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a03ceb9c-63d7-4c02-932d-efbd1073edd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:54:02 INFO InMemoryFileIndex: It took 1 ms to list leaf files for 1 paths.\n",
      "24/09/06 14:54:02 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "24/09/06 14:54:02 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/09/06 14:54:02 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/09/06 14:54:02 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 198.5 KiB, free 4.5 GiB)\n",
      "24/09/06 14:54:02 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:54:02 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on ubuntu:34403 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:54:02 INFO SparkContext: Created broadcast 33 from load at cell22.sc:1\n",
      "24/09/06 14:54:02 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:54:02 INFO SparkContext: Starting job: load at cell22.sc:1\n",
      "24/09/06 14:54:02 INFO DAGScheduler: Got job 18 (load at cell22.sc:1) with 2 output partitions\n",
      "24/09/06 14:54:02 INFO DAGScheduler: Final stage: ResultStage 22 (load at cell22.sc:1)\n",
      "24/09/06 14:54:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:54:02 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:54:02 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[78] at load at cell22.sc:1), which has no missing parents\n",
      "24/09/06 14:54:02 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 15.8 KiB, free 4.5 GiB)\n",
      "24/09/06 14:54:02 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:54:02 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on ubuntu:34403 (size: 7.4 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:54:02 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:54:02 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 22 (MapPartitionsRDD[78] at load at cell22.sc:1) (first 15 tasks are for partitions Vector(0, 1))\n",
      "24/09/06 14:54:02 INFO TaskSchedulerImpl: Adding task set 22.0 with 2 tasks resource profile 0\n",
      "24/09/06 14:54:02 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 19) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8238 bytes) \n",
      "24/09/06 14:54:02 INFO TaskSetManager: Starting task 1.0 in stage 22.0 (TID 20) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 8238 bytes) \n",
      "24/09/06 14:54:02 INFO Executor: Running task 1.0 in stage 22.0 (TID 20)\n",
      "24/09/06 14:54:02 INFO Executor: Running task 0.0 in stage 22.0 (TID 19)\n",
      "24/09/06 14:54:02 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/SQL/data/retail_data.json, range: 4194304-7997578, partition values: [empty row]\n",
      "24/09/06 14:54:02 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/SQL/data/retail_data.json, range: 0-4194304, partition values: [empty row]\n",
      "24/09/06 14:54:02 INFO Executor: Finished task 1.0 in stage 22.0 (TID 20). 2093 bytes result sent to driver\n",
      "24/09/06 14:54:02 INFO TaskSetManager: Finished task 1.0 in stage 22.0 (TID 20) in 220 ms on ubuntu (executor driver) (1/2)\n",
      "24/09/06 14:54:02 INFO Executor: Finished task 0.0 in stage 22.0 (TID 19). 2093 bytes result sent to driver\n",
      "24/09/06 14:54:02 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 19) in 233 ms on ubuntu (executor driver) (2/2)\n",
      "24/09/06 14:54:02 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:54:02 INFO DAGScheduler: ResultStage 22 (load at cell22.sc:1) finished in 0,241 s\n",
      "24/09/06 14:54:02 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:54:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 22: Stage finished\n",
      "24/09/06 14:54:02 INFO DAGScheduler: Job 18 finished: load at cell22.sc:1, took 0,245377 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mretailDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CustomerID: string, Description: string ... 5 more fields]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val retailDF = spark.read.format(\"json\").load(\"data/retail_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "693813b9-7df9-4c0b-a4d8-a2a04080e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "retailDF.createOrReplaceTempView(\"retail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89952bc5-8dee-4caf-ac5f-3798dca1dd33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mjDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Address: string, Birthdate: string ... 12 more fields]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jDF = spark.sql(\"\"\"\n",
    "select c.*, r.*\n",
    "from customer c\n",
    "join retail r\n",
    "on c.CustomerID == r.CustomerID\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2eecf71-fdc1-41b4-b452-22f0b54bbe67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Birthdate: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Username: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- UnitPrice: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ae63d3f0-b6e2-4ae2-8381-7f03667aec0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:56:39 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/09/06 14:56:39 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#11)\n",
      "24/09/06 14:56:39 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/09/06 14:56:39 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#363)\n",
      "24/09/06 14:56:39 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:56:39 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:56:39 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on ubuntu:34403 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:56:39 INFO SparkContext: Created broadcast 40 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/06 14:56:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:56:39 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/06 14:56:39 INFO DAGScheduler: Got job 21 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "24/09/06 14:56:39 INFO DAGScheduler: Final stage: ResultStage 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "24/09/06 14:56:39 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:56:39 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:56:39 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[90] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "24/09/06 14:56:39 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 16.8 KiB, free 4.5 GiB)\n",
      "24/09/06 14:56:39 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 4.5 GiB)\n",
      "24/09/06 14:56:39 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on ubuntu:34403 (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:56:39 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:56:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[90] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:56:39 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:56:39 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 23) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8240 bytes) \n",
      "24/09/06 14:56:39 INFO Executor: Running task 0.0 in stage 25.0 (TID 23)\n",
      "24/09/06 14:56:39 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/SQL/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:56:39 INFO Executor: Finished task 0.0 in stage 25.0 (TID 23). 63135 bytes result sent to driver\n",
      "24/09/06 14:56:39 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 23) in 20 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:56:39 INFO DAGScheduler: ResultStage 25 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0,027 s\n",
      "24/09/06 14:56:39 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:56:39 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:56:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 25: Stage finished\n",
      "24/09/06 14:56:39 INFO DAGScheduler: Job 21 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0,031302 s\n",
      "24/09/06 14:56:39 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 32.0 MiB, free 4.5 GiB)\n",
      "24/09/06 14:56:39 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 65.5 KiB, free 4.5 GiB)\n",
      "24/09/06 14:56:39 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on ubuntu:34403 (size: 65.5 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:56:39 INFO SparkContext: Created broadcast 42 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/06 14:56:39 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/09/06 14:56:39 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#363)\n",
      "24/09/06 14:56:39 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:56:39 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:56:39 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on ubuntu:34403 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:56:39 INFO SparkContext: Created broadcast 43 from show at cell28.sc:1\n",
      "24/09/06 14:56:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:56:39 INFO SparkContext: Starting job: show at cell28.sc:1\n",
      "24/09/06 14:56:39 INFO DAGScheduler: Got job 22 (show at cell28.sc:1) with 1 output partitions\n",
      "24/09/06 14:56:39 INFO DAGScheduler: Final stage: ResultStage 26 (show at cell28.sc:1)\n",
      "24/09/06 14:56:39 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:56:39 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:56:39 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[94] at show at cell28.sc:1), which has no missing parents\n",
      "24/09/06 14:56:39 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 21.1 KiB, free 4.5 GiB)\n",
      "24/09/06 14:56:39 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:56:39 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on ubuntu:34403 (size: 9.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:56:39 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:56:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[94] at show at cell28.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:56:39 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:56:39 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 24) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8238 bytes) \n",
      "24/09/06 14:56:39 INFO Executor: Running task 0.0 in stage 26.0 (TID 24)\n",
      "24/09/06 14:56:39 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/SQL/data/retail_data.json, range: 0-4194304, partition values: [empty row]\n",
      "24/09/06 14:56:39 INFO Executor: Finished task 0.0 in stage 26.0 (TID 24). 2554 bytes result sent to driver\n",
      "24/09/06 14:56:39 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 24) in 11 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:56:39 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:56:39 INFO DAGScheduler: ResultStage 26 (show at cell28.sc:1) finished in 0,017 s\n",
      "24/09/06 14:56:39 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:56:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 26: Stage finished\n",
      "24/09/06 14:56:39 INFO DAGScheduler: Job 22 finished: show at cell28.sc:1, took 0,024805 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+----------+---------------------------------+---------------+---------+--------+---------+---------+\n",
      "|Address                                               |Birthdate          |Country       |CustomerID|Email                   |Name           |Username        |CustomerID|Description                      |InvoiceDate    |InvoiceNo|Quantity|StockCode|UnitPrice|\n",
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+----------+---------------------------------+---------------+---------+--------+---------+---------+\n",
      "|Unit 1047 Box 4089\\nDPO AA 57348                      |1994-02-20 00:46:27|United Kingdom|12346     |cooperalexis@hotmail.com|Lindsay Cowan  |valenciajennifer|12346     |MEDIUM CERAMIC TOP STORAGE JAR   |1/18/2011 10:01|541431   |74215   |23166    |1.04     |\n",
      "|Unit 1047 Box 4089\\nDPO AA 57348                      |1994-02-20 00:46:27|United Kingdom|12346     |cooperalexis@hotmail.com|Lindsay Cowan  |valenciajennifer|12346     |MEDIUM CERAMIC TOP STORAGE JAR   |1/18/2011 10:17|C541433  |-74215  |23166    |1.04     |\n",
      "|55711 Janet Plaza Apt. 865\\nChristinachester, CT 62716|1988-06-21 00:15:34|Iceland       |12347     |timothy78@hotmail.com   |Katherine David|hillrachel      |12347     |BLACK CANDELABRA T-LIGHT HOLDER  |12/7/2010 14:57|537626   |12      |85116    |2.1      |\n",
      "|55711 Janet Plaza Apt. 865\\nChristinachester, CT 62716|1988-06-21 00:15:34|Iceland       |12347     |timothy78@hotmail.com   |Katherine David|hillrachel      |12347     |AIRLINE BAG VINTAGE JET SET BROWN|12/7/2010 14:57|537626   |4       |22375    |4.25     |\n",
      "|55711 Janet Plaza Apt. 865\\nChristinachester, CT 62716|1988-06-21 00:15:34|Iceland       |12347     |timothy78@hotmail.com   |Katherine David|hillrachel      |12347     |COLOUR GLASS. STAR T-LIGHT HOLDER|12/7/2010 14:57|537626   |12      |71477    |3.25     |\n",
      "+------------------------------------------------------+-------------------+--------------+----------+------------------------+---------------+----------------+----------+---------------------------------+---------------+---------+--------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jDF.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bf6c161-de26-4439-92d7-c7274e6846f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": "[AMBIGUOUS_REFERENCE] Reference `CustomerID` is ambiguous, could be: [`c`.`CustomerID`, `r`.`CustomerID`].",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.AnalysisException: [AMBIGUOUS_REFERENCE] Reference `CustomerID` is ambiguous, could be: [`c`.`CustomerID`, `r`.`CustomerID`].\u001b[39m",
      "  org.apache.spark.sql.errors.QueryCompilationErrors$.ambiguousReferenceError(\u001b[32mQueryCompilationErrors.scala\u001b[39m:\u001b[32m1938\u001b[39m)",
      "  org.apache.spark.sql.catalyst.expressions.package$AttributeSeq.resolve(\u001b[32mpackage.scala\u001b[39m:\u001b[32m377\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m144\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpressionByPlanChildren$2(\u001b[32mColumnResolutionHelper.scala\u001b[39m:\u001b[32m377\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$3(\u001b[32mColumnResolutionHelper.scala\u001b[39m:\u001b[32m157\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.package$.withPosition(\u001b[32mpackage.scala\u001b[39m:\u001b[32m100\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.$anonfun$resolveExpression$1(\u001b[32mColumnResolutionHelper.scala\u001b[39m:\u001b[32m164\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32morigin.scala\u001b[39m:\u001b[32m76\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.innerResolve$1(\u001b[32mColumnResolutionHelper.scala\u001b[39m:\u001b[32m135\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpression(\u001b[32mColumnResolutionHelper.scala\u001b[39m:\u001b[32m194\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren(\u001b[32mColumnResolutionHelper.scala\u001b[39m:\u001b[32m384\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.ColumnResolutionHelper.resolveExpressionByPlanChildren$(\u001b[32mColumnResolutionHelper.scala\u001b[39m:\u001b[32m357\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.resolveExpressionByPlanChildren(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m1466\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$16.$anonfun$applyOrElse$110(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m1604\u001b[39m)",
      "  scala.collection.TraversableLike.$anonfun$map$1(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m286\u001b[39m)",
      "  scala.collection.mutable.ResizableArray.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m62\u001b[39m)",
      "  scala.collection.mutable.ResizableArray.foreach$(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m55\u001b[39m)",
      "  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m49\u001b[39m)",
      "  scala.collection.TraversableLike.map(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m286\u001b[39m)",
      "  scala.collection.TraversableLike.map$(\u001b[32mTraversableLike.scala\u001b[39m:\u001b[32m279\u001b[39m)",
      "  scala.collection.AbstractTraversable.map(\u001b[32mTraversable.scala\u001b[39m:\u001b[32m108\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$16.applyOrElse(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m1604\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$16.applyOrElse(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m1491\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m138\u001b[39m)",
      "  org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(\u001b[32morigin.scala\u001b[39m:\u001b[32m76\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m138\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m323\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m134\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m130\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m32\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m111\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m110\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(\u001b[32mLogicalPlan.scala\u001b[39m:\u001b[32m32\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m1491\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m1466\u001b[39m)",
      "  org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(\u001b[32mRuleExecutor.scala\u001b[39m:\u001b[32m222\u001b[39m)",
      "  scala.collection.LinearSeqOptimized.foldLeft(\u001b[32mLinearSeqOptimized.scala\u001b[39m:\u001b[32m126\u001b[39m)",
      "  scala.collection.LinearSeqOptimized.foldLeft$(\u001b[32mLinearSeqOptimized.scala\u001b[39m:\u001b[32m122\u001b[39m)",
      "  scala.collection.immutable.List.foldLeft(\u001b[32mList.scala\u001b[39m:\u001b[32m91\u001b[39m)",
      "  org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(\u001b[32mRuleExecutor.scala\u001b[39m:\u001b[32m219\u001b[39m)",
      "  org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(\u001b[32mRuleExecutor.scala\u001b[39m:\u001b[32m211\u001b[39m)",
      "  scala.collection.immutable.List.foreach(\u001b[32mList.scala\u001b[39m:\u001b[32m431\u001b[39m)",
      "  org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(\u001b[32mRuleExecutor.scala\u001b[39m:\u001b[32m211\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m226\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m222\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m173\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer.execute(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m222\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer.execute(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m188\u001b[39m)",
      "  org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(\u001b[32mRuleExecutor.scala\u001b[39m:\u001b[32m182\u001b[39m)",
      "  org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(\u001b[32mQueryPlanningTracker.scala\u001b[39m:\u001b[32m89\u001b[39m)",
      "  org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(\u001b[32mRuleExecutor.scala\u001b[39m:\u001b[32m182\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m209\u001b[39m)",
      "  org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(\u001b[32mAnalysisHelper.scala\u001b[39m:\u001b[32m330\u001b[39m)",
      "  org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(\u001b[32mAnalyzer.scala\u001b[39m:\u001b[32m208\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m77\u001b[39m)",
      "  org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(\u001b[32mQueryPlanningTracker.scala\u001b[39m:\u001b[32m138\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m219\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution$.withInternalError(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m546\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m219\u001b[39m)",
      "  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m900\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.executePhase(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m218\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m77\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.analyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m74\u001b[39m)",
      "  org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m66\u001b[39m)",
      "  org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(\u001b[32mDataset.scala\u001b[39m:\u001b[32m91\u001b[39m)",
      "  org.apache.spark.sql.SparkSession.withActive(\u001b[32mSparkSession.scala\u001b[39m:\u001b[32m900\u001b[39m)",
      "  org.apache.spark.sql.Dataset$.ofRows(\u001b[32mDataset.scala\u001b[39m:\u001b[32m89\u001b[39m)",
      "  org.apache.spark.sql.Dataset.withPlan(\u001b[32mDataset.scala\u001b[39m:\u001b[32m4363\u001b[39m)",
      "  org.apache.spark.sql.Dataset.select(\u001b[32mDataset.scala\u001b[39m:\u001b[32m1541\u001b[39m)",
      "  org.apache.spark.sql.Dataset.select(\u001b[32mDataset.scala\u001b[39m:\u001b[32m1558\u001b[39m)",
      "  ammonite.$sess.cell29$Helper.<init>(\u001b[32mcell29.sc\u001b[39m:\u001b[32m1\u001b[39m)",
      "  ammonite.$sess.cell29$.<init>(\u001b[32mcell29.sc\u001b[39m:\u001b[32m7\u001b[39m)",
      "  ammonite.$sess.cell29$.<clinit>(\u001b[32mcell29.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "jDF.select(\"CustomerID\", \"Name\", \"Birthdate\", \"InvoiceDate\").show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d2aa62b-4e34-4450-8163-f1ceb7add173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mjDF2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CustomerID: string, Name: string ... 2 more fields]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jDF2 = spark.sql(\"\"\"\n",
    "select c.CustomerID, c.Name, c.Birthdate, r.InvoiceDate\n",
    "from customer c\n",
    "join retail r\n",
    "on c.CustomerID == r.CustomerID\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d5b6a43-739f-4d7d-a9d1-a16c247164c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:59:34 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/09/06 14:59:34 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#11)\n",
      "24/09/06 14:59:34 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/09/06 14:59:34 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#363)\n",
      "24/09/06 14:59:34 INFO CodeGenerator: Code generated in 5.679604 ms\n",
      "24/09/06 14:59:34 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/09/06 14:59:34 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/09/06 14:59:34 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on ubuntu:34403 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:59:34 INFO SparkContext: Created broadcast 45 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/06 14:59:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:59:34 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/06 14:59:34 INFO DAGScheduler: Got job 23 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) with 1 output partitions\n",
      "24/09/06 14:59:34 INFO DAGScheduler: Final stage: ResultStage 27 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264)\n",
      "24/09/06 14:59:34 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:59:34 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:59:34 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[98] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264), which has no missing parents\n",
      "24/09/06 14:59:34 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 15.3 KiB, free 4.5 GiB)\n",
      "24/09/06 14:59:34 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 4.5 GiB)\n",
      "24/09/06 14:59:34 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on ubuntu:34403 (size: 7.3 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:59:34 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:59:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[98] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:59:34 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:59:34 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 25) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8240 bytes) \n",
      "24/09/06 14:59:34 INFO Executor: Running task 0.0 in stage 27.0 (TID 25)\n",
      "24/09/06 14:59:34 INFO CodeGenerator: Code generated in 5.419507 ms\n",
      "24/09/06 14:59:34 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/SQL/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/09/06 14:59:34 INFO CodeGenerator: Code generated in 3.085687 ms\n",
      "24/09/06 14:59:34 INFO Executor: Finished task 0.0 in stage 27.0 (TID 25). 20233 bytes result sent to driver\n",
      "24/09/06 14:59:34 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 25) in 31 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:59:34 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:59:34 INFO DAGScheduler: ResultStage 27 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:264) finished in 0,042 s\n",
      "24/09/06 14:59:34 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:59:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished\n",
      "24/09/06 14:59:34 INFO DAGScheduler: Job 23 finished: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264, took 0,047728 s\n",
      "24/09/06 14:59:34 INFO CodeGenerator: Code generated in 4.185482 ms\n",
      "24/09/06 14:59:34 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 32.0 MiB, free 4.4 GiB)\n",
      "24/09/06 14:59:34 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 22.5 KiB, free 4.4 GiB)\n",
      "24/09/06 14:59:34 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on ubuntu:34403 (size: 22.5 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:59:34 INFO SparkContext: Created broadcast 47 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:264\n",
      "24/09/06 14:59:34 INFO FileSourceStrategy: Pushed Filters: IsNotNull(CustomerID)\n",
      "24/09/06 14:59:34 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(CustomerID#363)\n",
      "24/09/06 14:59:34 INFO CodeGenerator: Code generated in 7.951374 ms\n",
      "24/09/06 14:59:34 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 198.4 KiB, free 4.4 GiB)\n",
      "24/09/06 14:59:34 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.4 GiB)\n",
      "24/09/06 14:59:34 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on ubuntu:34403 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:59:34 INFO SparkContext: Created broadcast 48 from show at cell31.sc:1\n",
      "24/09/06 14:59:34 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/09/06 14:59:34 INFO SparkContext: Starting job: show at cell31.sc:1\n",
      "24/09/06 14:59:34 INFO DAGScheduler: Got job 24 (show at cell31.sc:1) with 1 output partitions\n",
      "24/09/06 14:59:34 INFO DAGScheduler: Final stage: ResultStage 28 (show at cell31.sc:1)\n",
      "24/09/06 14:59:34 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/09/06 14:59:34 INFO DAGScheduler: Missing parents: List()\n",
      "24/09/06 14:59:34 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[102] at show at cell31.sc:1), which has no missing parents\n",
      "24/09/06 14:59:34 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 17.3 KiB, free 4.4 GiB)\n",
      "24/09/06 14:59:34 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 8.1 KiB, free 4.4 GiB)\n",
      "24/09/06 14:59:34 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on ubuntu:34403 (size: 8.1 KiB, free: 4.5 GiB)\n",
      "24/09/06 14:59:34 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1580\n",
      "24/09/06 14:59:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[102] at show at cell31.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/09/06 14:59:34 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0\n",
      "24/09/06 14:59:34 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 26) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8238 bytes) \n",
      "24/09/06 14:59:34 INFO Executor: Running task 0.0 in stage 28.0 (TID 26)\n",
      "24/09/06 14:59:34 INFO CodeGenerator: Code generated in 8.670045 ms\n",
      "24/09/06 14:59:34 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/SQL/data/retail_data.json, range: 0-4194304, partition values: [empty row]\n",
      "24/09/06 14:59:34 INFO Executor: Finished task 0.0 in stage 28.0 (TID 26). 1842 bytes result sent to driver\n",
      "24/09/06 14:59:34 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 26) in 22 ms on ubuntu (executor driver) (1/1)\n",
      "24/09/06 14:59:34 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "24/09/06 14:59:34 INFO DAGScheduler: ResultStage 28 (show at cell31.sc:1) finished in 0,029 s\n",
      "24/09/06 14:59:34 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/09/06 14:59:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished\n",
      "24/09/06 14:59:34 INFO DAGScheduler: Job 24 finished: show at cell31.sc:1, took 0,034507 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-------------------+---------------+\n",
      "|CustomerID|Name           |Birthdate          |InvoiceDate    |\n",
      "+----------+---------------+-------------------+---------------+\n",
      "|12346     |Lindsay Cowan  |1994-02-20 00:46:27|1/18/2011 10:01|\n",
      "|12346     |Lindsay Cowan  |1994-02-20 00:46:27|1/18/2011 10:17|\n",
      "|12347     |Katherine David|1988-06-21 00:15:34|12/7/2010 14:57|\n",
      "|12347     |Katherine David|1988-06-21 00:15:34|12/7/2010 14:57|\n",
      "|12347     |Katherine David|1988-06-21 00:15:34|12/7/2010 14:57|\n",
      "+----------+---------------+-------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jDF2.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88bd309b-390d-412c-978a-8c361167421f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 14:59:49 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
      "24/09/06 14:59:49 INFO SparkUI: Stopped Spark web UI at http://ubuntu:4040\n",
      "24/09/06 14:59:49 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "24/09/06 14:59:49 INFO MemoryStore: MemoryStore cleared\n",
      "24/09/06 14:59:49 INFO BlockManager: BlockManager stopped\n",
      "24/09/06 14:59:49 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "24/09/06 14:59:49 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "24/09/06 14:59:49 INFO SparkContext: Successfully stopped SparkContext\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b09568-457c-4a66-a8d5-866e1a627c03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
