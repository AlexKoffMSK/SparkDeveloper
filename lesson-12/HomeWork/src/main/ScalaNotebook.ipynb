{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90139c87-d51b-4add-82bc-15424218c905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.sql.Timestamp\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.time._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.5.0`\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import java.sql.Timestamp\n",
    "import java.time._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25a080c5-d50c-4f05-952a-a7682b845cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "24/06/15 19:12:28 WARN Utils: Your hostname, MacBook-Air-Aroslav.local resolves to a loopback address: 127.0.0.1; using 192.168.1.222 instead (on interface en0)\n",
      "24/06/15 19:12:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/06/15 19:12:29 INFO SparkContext: Running Spark version 3.5.0\n",
      "24/06/15 19:12:29 INFO SparkContext: OS info Mac OS X, 14.4.1, aarch64\n",
      "24/06/15 19:12:29 INFO SparkContext: Java version 11.0.23\n",
      "24/06/15 19:12:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/15 19:12:29 INFO ResourceUtils: ==============================================================\n",
      "24/06/15 19:12:29 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/06/15 19:12:29 INFO ResourceUtils: ==============================================================\n",
      "24/06/15 19:12:29 INFO SparkContext: Submitted application: Hello Spark\n",
      "24/06/15 19:12:29 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/06/15 19:12:29 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/06/15 19:12:29 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/06/15 19:12:29 INFO SecurityManager: Changing view acls to: iaroslavrussu\n",
      "24/06/15 19:12:29 INFO SecurityManager: Changing modify acls to: iaroslavrussu\n",
      "24/06/15 19:12:29 INFO SecurityManager: Changing view acls groups to: \n",
      "24/06/15 19:12:29 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/06/15 19:12:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: iaroslavrussu; groups with view permissions: EMPTY; users with modify permissions: iaroslavrussu; groups with modify permissions: EMPTY\n",
      "24/06/15 19:12:29 INFO Utils: Successfully started service 'sparkDriver' on port 60341.\n",
      "24/06/15 19:12:29 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/06/15 19:12:29 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/06/15 19:12:29 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/06/15 19:12:29 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/06/15 19:12:29 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/06/15 19:12:29 INFO DiskBlockManager: Created local directory at /private/var/folders/8c/hr706rln1376_6tc0szf72yr0000gn/T/blockmgr-7f4d80e9-cf43-487c-b0c3-cec65efb7319\n",
      "24/06/15 19:12:29 INFO MemoryStore: MemoryStore started with capacity 1048.8 MiB\n",
      "24/06/15 19:12:29 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/06/15 19:12:29 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/06/15 19:12:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/06/15 19:12:29 INFO Executor: Starting executor ID driver on host 192.168.1.222\n",
      "24/06/15 19:12:29 INFO Executor: OS info Mac OS X, 14.4.1, aarch64\n",
      "24/06/15 19:12:29 INFO Executor: Java version 11.0.23\n",
      "24/06/15 19:12:29 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "24/06/15 19:12:29 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@214df0a for default.\n",
      "24/06/15 19:12:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 60342.\n",
      "24/06/15 19:12:29 INFO NettyBlockTransferService: Server created on 192.168.1.222:60342\n",
      "24/06/15 19:12:29 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/06/15 19:12:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.222, 60342, None)\n",
      "24/06/15 19:12:29 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.222:60342 with 1048.8 MiB RAM, BlockManagerId(driver, 192.168.1.222, 60342, None)\n",
      "24/06/15 19:12:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.222, 60342, None)\n",
      "24/06/15 19:12:29 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.222, 60342, None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.version == 3.5.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@6a4245e6\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "                .builder()\n",
    "                .master(\"local[*]\")\n",
    "                .appName(\"Hello Spark\")\n",
    "                .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "import spark.implicits._\n",
    "println(s\"spark.version == ${spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f88e2fa-deec-4e80-b919-29d4cb041a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n",
      "Data ready\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtaxi_data_path\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"resources/data/yellow_taxi_jan_25_2018\"\u001b[39m\n",
       "\u001b[36mzone_data_path\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"resources/data/taxi_zones.csv\"\u001b[39m\n",
       "\u001b[36mschemaTaxi\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mSeq\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"VendorID\"\u001b[39m,\n",
       "    dataType = IntegerType,\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"tpep_pickup_datetime\"\u001b[39m,\n",
       "    dataType = TimestampType,\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"tpep_dropoff_datetime\"\u001b[39m,\n",
       "    dataType = TimestampType,\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"passenger_count\"\u001b[39m,\n",
       "    dataType = IntegerType,\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"trip_distance\"\u001b[39m,\n",
       "    dataType = DoubleType,\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"RatecodeID\"\u001b[39m,\n",
       "    dataType = IntegerType,\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"store_and_fwd_flag\"\u001b[39m,\n",
       "...\n",
       "\u001b[36mbaseDf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [VendorID: int, tpep_pickup_datetime: timestamp ... 15 more fields]\n",
       "\u001b[36mschemaZone\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mSeq\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"LocationID\"\u001b[39m,\n",
       "    dataType = IntegerType,\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"Borough\"\u001b[39m,\n",
       "    dataType = StringType,\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"Zone\"\u001b[39m,\n",
       "    dataType = StringType,\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\n",
       "    metadata = {}\n",
       "  ),\n",
       "  \u001b[33mStructField\u001b[39m(\n",
       "    name = \u001b[32m\"service_zone\"\u001b[39m,\n",
       "    dataType = StringType,\n",
       "    nullable = \u001b[32mtrue\u001b[39m,\n",
       "    metadata = {}\n",
       "  )\n",
       ")\n",
       "\u001b[36mzoneDf\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [LocationID: int, Borough: string ... 2 more fields]\n",
       "\u001b[36mres3_6\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [VendorID: int, tpep_pickup_datetime: timestamp ... 15 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val taxi_data_path= \"resources/data/yellow_taxi_jan_25_2018\"\n",
    "val zone_data_path = \"resources/data/taxi_zones.csv\"\n",
    "\n",
    "val schemaTaxi = StructType(\n",
    "  Array(\n",
    "    StructField(\"VendorID\", IntegerType),\n",
    "    StructField(\"tpep_pickup_datetime\", TimestampType),\n",
    "    StructField(\"tpep_dropoff_datetime\", TimestampType),\n",
    "    StructField(\"passenger_count\", IntegerType),\n",
    "    StructField(\"trip_distance\", DoubleType),\n",
    "    StructField(\"RatecodeID\", IntegerType),\n",
    "    StructField(\"store_and_fwd_flag\", StringType),\n",
    "    StructField(\"PULocationID\", IntegerType),\n",
    "    StructField(\"DOLocationID\", IntegerType),\n",
    "    StructField(\"payment_type\", IntegerType),\n",
    "    StructField(\"fare_amount\", DoubleType),\n",
    "    StructField(\"extra\", DoubleType),\n",
    "    StructField(\"mta_tax\", DoubleType),\n",
    "    StructField(\"tip_amount\", DoubleType),\n",
    "    StructField(\"tolls_amount\", DoubleType),\n",
    "    StructField(\"improvement_surcharge\", DoubleType),\n",
    "    StructField(\"total_amount\", DoubleType)\n",
    "  )\n",
    ")\n",
    "val baseDf = spark.read.schema(schemaTaxi).parquet(taxi_data_path)\n",
    "\n",
    "val schemaZone = StructType(\n",
    "    Array(\n",
    "        StructField(\"LocationID\", IntegerType),\n",
    "        StructField(\"Borough\", StringType),\n",
    "        StructField(\"Zone\", StringType),\n",
    "        StructField(\"service_zone\", StringType)\n",
    "    )\n",
    ")\n",
    "val zoneDf = spark.read.schema(schemaZone).option(\"header\", \"true\").csv(zone_data_path)\n",
    "baseDf.cache()\n",
    "baseDf.printSchema()\n",
    "zoneDf.printSchema()\n",
    "println(\"Data ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ca28103-6c59-42fd-ae49-e44362c073bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------------+\n",
      "|Zone                     |trips_number|\n",
      "+-------------------------+------------+\n",
      "|Midtown Center           |15099       |\n",
      "|Upper East Side North    |14261       |\n",
      "|Upper East Side South    |13754       |\n",
      "|Murray Hill              |11239       |\n",
      "|Midtown East             |11090       |\n",
      "|Times Sq/Theatre District|10054       |\n",
      "|Union Sq                 |9929        |\n",
      "|Lincoln Square East      |8666        |\n",
      "|Midtown North            |8594        |\n",
      "|Clinton East             |8258        |\n",
      "+-------------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtask1\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[\u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mRow\u001b[39m] = [Zone: string, trips_number: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val task1 = baseDf.join(zoneDf, baseDf(\"DOLocationID\") === zoneDf(\"LocationID\"))\n",
    "  .groupBy(\"Zone\").count().withColumnRenamed(\"count\", \"trips_number\")\n",
    "  .orderBy(col(\"trips_number\").desc)\n",
    "task1.show(10, false)\n",
    "/* \n",
    "+-------------------------+------------+\n",
    "|Zone                     |trips_number|\n",
    "+-------------------------+------------+\n",
    "|Midtown Center           |15099       |\n",
    "|Upper East Side North    |14261       |\n",
    "|Upper East Side South    |13754       |\n",
    "|Murray Hill              |11239       |\n",
    "|Midtown East             |11090       |\n",
    "|Times Sq/Theatre District|10054       |\n",
    "|Union Sq                 |9929        |\n",
    "|Lincoln Square East      |8666        |\n",
    "|Midtown North            |8594        |\n",
    "|Clinton East             |8258        |\n",
    "+-------------------------+------------+\n",
    "only showing top 10 rows\n",
    "*/\n",
    "task1.write.mode(\"overwrite\").parquet(\"result/task1/task1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15946da0-3e56-4718-8703-06742a88fca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|tpep_pickup_datetime|trips_number|\n",
      "+--------------------+------------+\n",
      "|2018-01-25 09:51:56 |18          |\n",
      "|2018-01-25 19:48:51 |17          |\n",
      "|2018-01-25 19:54:47 |17          |\n",
      "|2018-01-25 14:32:01 |17          |\n",
      "|2018-01-25 22:38:40 |17          |\n",
      "|2018-01-25 18:24:54 |16          |\n",
      "|2018-01-25 19:11:41 |16          |\n",
      "|2018-01-25 09:25:26 |16          |\n",
      "|2018-01-25 20:25:25 |15          |\n",
      "|2018-01-25 19:01:49 |15          |\n",
      "+--------------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtask2\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mDataFrame\u001b[39m = [tpep_pickup_datetime: timestamp, trips_number: bigint]\n",
       "\u001b[36mtaskText\u001b[39m: \u001b[32mArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mArray\u001b[39m(\n",
       "  \u001b[32m\"\"\"2018-01-25 09:51:56.0,18\n",
       "\"\"\"\u001b[39m,\n",
       "  \u001b[32m\"\"\"2018-01-25 19:48:51.0,17\n",
       "\"\"\"\u001b[39m,\n",
       "  \u001b[32m\"\"\"2018-01-25 19:54:47.0,17\n",
       "\"\"\"\u001b[39m,\n",
       "  \u001b[32m\"\"\"2018-01-25 14:32:01.0,17\n",
       "\"\"\"\u001b[39m,\n",
       "  \u001b[32m\"\"\"2018-01-25 22:38:40.0,17\n",
       "\"\"\"\u001b[39m,\n",
       "  \u001b[32m\"\"\"2018-01-25 18:24:54.0,16\n",
       "\"\"\"\u001b[39m,\n",
       "  \u001b[32m\"\"\"2018-01-25 19:11:41.0,16\n",
       "\"\"\"\u001b[39m,\n",
       "  \u001b[32m\"\"\"2018-01-25 09:25:26.0,16\n",
       "\"\"\"\u001b[39m,\n",
       "  \u001b[32m\"\"\"2018-01-25 09:46:21.0,15\n",
       "\"\"\"\u001b[39m,\n",
       "  \u001b[32m\"\"\"2018-01-25 20:57:23.0,15\n",
       "\"\"\"\u001b[39m,\n",
       "  \u001b[32m\"\"\"2018-01-25 15:39:44.0,15\n",
       "\"\"\"\u001b[39m,\n",
       "  \u001b[32m\"\"\"2018-01-25 19:01:49.0,15\n",
       "\"\"\"\u001b[39m,\n",
       "  \u001b[32m\"\"\"2018-01-25 22:26:49.0,15\n",
       "\"\"\"\u001b[39m,\n",
       "  \u001b[32m\"\"\"2018-01-25 18:10:51.0,15\n",
       "\"\"\"\u001b[39m,\n",
       "  \u001b[32m\"\"\"2018-01-25 21:17:03.0,15\n",
       "\"\"\"\u001b[39m,\n",
       "  \u001b[32m\"\"\"2018-01-25 20:25:25.0,15\n",
       "\"\"\"\u001b[39m,\n",
       "  \u001b[32m\"\"\"2018-01-25 20:00:19.0,15\n",
       "\"\"\"\u001b[39m,\n",
       "  \u001b[32m\"\"\"2018-01-25 20:00:14.0,15\n",
       "\"\"\"\u001b[39m,\n",
       "  \u001b[32m\"\"\"2018-01-25 18:57:08.0,15\n",
       "\"\"\"\u001b[39m,\n",
       "...\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io.File\u001b[39m\n",
       "\u001b[36mdir\u001b[39m: \u001b[32mFile\u001b[39m = result/task2\n",
       "\u001b[36mres12_5\u001b[39m: \u001b[32mAnyVal\u001b[39m = \u001b[32mtrue\u001b[39m\n",
       "\u001b[36mfile\u001b[39m: \u001b[32mjava\u001b[39m.\u001b[32mio\u001b[39m.\u001b[32mFileWriter\u001b[39m = java.io.FileWriter@3a7ebefb"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val task2 = baseDf.groupBy(\"tpep_pickup_datetime\").count().withColumnRenamed(\"count\", \"trips_number\")\n",
    "                  .orderBy(col(\"trips_number\").desc)\n",
    "                  .select(col(\"tpep_pickup_datetime\"),\n",
    "                           col(\"trips_number\"))\n",
    "task2.show(10, false)\n",
    "val taskText = task2.rdd.map(row => s\"${row(0).toString},${row(1).toString}\\n\").collect()\n",
    "\n",
    "import java.io.File\n",
    "val dir = new File(\"result/task2\")\n",
    "if (!dir.exists()) {\n",
    "  dir.mkdirs()\n",
    "}\n",
    "\n",
    "val file = new java.io.FileWriter(\"result/task2/task2.txt\")\n",
    "file.write(taskText.mkString)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afd544db-97e7-41f6-881c-5c3428ae50bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|trip_distance|trips_number|\n",
      "+-------------+------------+\n",
      "|0.8          |8223        |\n",
      "|0.7          |8156        |\n",
      "|0.9          |8102        |\n",
      "|1.0          |7821        |\n",
      "|1.1          |7453        |\n",
      "|0.6          |7422        |\n",
      "|1.2          |6895        |\n",
      "|1.3          |6514        |\n",
      "|1.4          |5979        |\n",
      "|0.5          |5958        |\n",
      "+-------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtask3\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32msql\u001b[39m.\u001b[32mDataset\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mLong\u001b[39m)] = [trip_distance: double, trips_number: bigint]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val task3 = baseDf.groupBy(\"trip_distance\").count()\n",
    "                  .withColumnRenamed(\"count\", \"trips_number\")\n",
    "                  .orderBy(col(\"trips_number\").desc)\n",
    "                  .as[(String, Long)]\n",
    "\n",
    "task3.show(10, false)\n",
    "task3.write.mode(\"overwrite\").parquet(\"result/task3/task3.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b0cd5d-5942-48b3-ac67-2d12940e52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.unpersist()\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
