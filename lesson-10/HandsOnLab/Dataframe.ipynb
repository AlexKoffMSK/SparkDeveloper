{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2515a1c-c4f9-4890-aa6d-641bf419cea6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.5.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc99a5fd-2141-4ffe-afb1-4b0a595a75af",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.WARN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ab7dc3-9173-4d9e-9a91-97256abb3a30",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49835da8-c4d0-461a-a2c8-3c0959118bb4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7df9de1d-1f63-41b1-999b-69614c723fae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "24/06/05 17:28:21 INFO SparkContext: Running Spark version 3.5.0\n",
      "24/06/05 17:28:21 INFO SparkContext: OS info Linux, 6.5.0-35-generic, amd64\n",
      "24/06/05 17:28:21 INFO SparkContext: Java version 11.0.22\n",
      "24/06/05 17:28:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/06/05 17:28:22 INFO ResourceUtils: ==============================================================\n",
      "24/06/05 17:28:22 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/06/05 17:28:22 INFO ResourceUtils: ==============================================================\n",
      "24/06/05 17:28:22 INFO SparkContext: Submitted application: Dataframe API\n",
      "24/06/05 17:28:22 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/06/05 17:28:22 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/06/05 17:28:22 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/06/05 17:28:22 INFO SecurityManager: Changing view acls to: vadim\n",
      "24/06/05 17:28:22 INFO SecurityManager: Changing modify acls to: vadim\n",
      "24/06/05 17:28:22 INFO SecurityManager: Changing view acls groups to: \n",
      "24/06/05 17:28:22 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/06/05 17:28:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: vadim; groups with view permissions: EMPTY; users with modify permissions: vadim; groups with modify permissions: EMPTY\n",
      "24/06/05 17:28:22 INFO Utils: Successfully started service 'sparkDriver' on port 46179.\n",
      "24/06/05 17:28:22 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/06/05 17:28:22 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/06/05 17:28:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/06/05 17:28:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/06/05 17:28:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/06/05 17:28:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4f9515dc-9830-40bc-b75b-d0fbec8ae400\n",
      "24/06/05 17:28:22 INFO MemoryStore: MemoryStore started with capacity 4.5 GiB\n",
      "24/06/05 17:28:22 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/06/05 17:28:22 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/06/05 17:28:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/06/05 17:28:22 INFO Executor: Starting executor ID driver on host ubuntu\n",
      "24/06/05 17:28:22 INFO Executor: OS info Linux, 6.5.0-35-generic, amd64\n",
      "24/06/05 17:28:22 INFO Executor: Java version 11.0.22\n",
      "24/06/05 17:28:22 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "24/06/05 17:28:22 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2efa09d1 for default.\n",
      "24/06/05 17:28:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37615.\n",
      "24/06/05 17:28:22 INFO NettyBlockTransferService: Server created on ubuntu:37615\n",
      "24/06/05 17:28:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/06/05 17:28:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ubuntu, 37615, None)\n",
      "24/06/05 17:28:22 INFO BlockManagerMasterEndpoint: Registering block manager ubuntu:37615 with 4.5 GiB RAM, BlockManagerId(driver, ubuntu, 37615, None)\n",
      "24/06/05 17:28:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ubuntu, 37615, None)\n",
      "24/06/05 17:28:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ubuntu, 37615, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@5fe333d7\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    "                .builder()\n",
    "                .master(\"local[*]\")\n",
    "                .appName(\"Dataframe API\")\n",
    "                .getOrCreate()\n",
    "\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59c101c3-900c-4a51-8f5f-bfc0ce535ef9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.version == 3.5.0\n"
     ]
    }
   ],
   "source": [
    "println(s\"spark.version == ${spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0928fb8-44ac-4f97-ab64-94faf88d9e6d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1 - Создание DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbd336b",
   "metadata": {},
   "source": [
    "### fromRDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68e5128a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 17:33:21 INFO CodeGenerator: Code generated in 217.886853 ms\n",
      "24/06/05 17:33:21 INFO SparkContext: Starting job: show at cell7.sc:5\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Got job 0 (show at cell7.sc:5) with 1 output partitions\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Final stage: ResultStage 0 (show at cell7.sc:5)\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[4] at show at cell7.sc:5), which has no missing parents\n",
      "24/06/05 17:33:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/06/05 17:33:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.7 KiB, free 4.5 GiB)\n",
      "24/06/05 17:33:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ubuntu:37615 (size: 5.7 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:33:21 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[4] at show at cell7.sc:5) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/05 17:33:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "24/06/05 17:33:21 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7607 bytes) \n",
      "24/06/05 17:33:21 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "24/06/05 17:33:21 INFO CodeGenerator: Code generated in 20.052945 ms\n",
      "24/06/05 17:33:21 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1452 bytes result sent to driver\n",
      "24/06/05 17:33:21 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 227 ms on ubuntu (executor driver) (1/1)\n",
      "24/06/05 17:33:21 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "24/06/05 17:33:21 INFO DAGScheduler: ResultStage 0 (show at cell7.sc:5) finished in 0,432 s\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/05 17:33:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Job 0 finished: show at cell7.sc:5, took 0,490242 s\n",
      "24/06/05 17:33:21 INFO SparkContext: Starting job: show at cell7.sc:5\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Got job 1 (show at cell7.sc:5) with 4 output partitions\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Final stage: ResultStage 1 (show at cell7.sc:5)\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at show at cell7.sc:5), which has no missing parents\n",
      "24/06/05 17:33:21 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/06/05 17:33:21 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 4.5 GiB)\n",
      "24/06/05 17:33:21 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ubuntu:37615 (size: 5.8 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:33:21 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at show at cell7.sc:5) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\n",
      "24/06/05 17:33:21 INFO TaskSchedulerImpl: Adding task set 1.0 with 4 tasks resource profile 0\n",
      "24/06/05 17:33:21 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7607 bytes) \n",
      "24/06/05 17:33:21 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7668 bytes) \n",
      "24/06/05 17:33:21 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7607 bytes) \n",
      "24/06/05 17:33:21 INFO TaskSetManager: Starting task 3.0 in stage 1.0 (TID 4) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7607 bytes) \n",
      "24/06/05 17:33:21 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "24/06/05 17:33:21 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n",
      "24/06/05 17:33:21 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)\n",
      "24/06/05 17:33:21 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1452 bytes result sent to driver\n",
      "24/06/05 17:33:21 INFO Executor: Running task 3.0 in stage 1.0 (TID 4)\n",
      "24/06/05 17:33:21 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 45 ms on ubuntu (executor driver) (1/4)\n",
      "24/06/05 17:33:21 INFO Executor: Finished task 1.0 in stage 1.0 (TID 2). 1447 bytes result sent to driver\n",
      "24/06/05 17:33:21 INFO Executor: Finished task 2.0 in stage 1.0 (TID 3). 1409 bytes result sent to driver\n",
      "24/06/05 17:33:21 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 2) in 56 ms on ubuntu (executor driver) (2/4)\n",
      "24/06/05 17:33:21 INFO TaskSetManager: Finished task 2.0 in stage 1.0 (TID 3) in 55 ms on ubuntu (executor driver) (3/4)\n",
      "24/06/05 17:33:21 INFO Executor: Finished task 3.0 in stage 1.0 (TID 4). 1409 bytes result sent to driver\n",
      "24/06/05 17:33:21 INFO TaskSetManager: Finished task 3.0 in stage 1.0 (TID 4) in 60 ms on ubuntu (executor driver) (4/4)\n",
      "24/06/05 17:33:21 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "24/06/05 17:33:21 INFO DAGScheduler: ResultStage 1 (show at cell7.sc:5) finished in 0,094 s\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/05 17:33:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Job 1 finished: show at cell7.sc:5, took 0,105933 s\n",
      "24/06/05 17:33:21 INFO SparkContext: Starting job: show at cell7.sc:5\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Got job 2 (show at cell7.sc:5) with 3 output partitions\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Final stage: ResultStage 2 (show at cell7.sc:5)\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[4] at show at cell7.sc:5), which has no missing parents\n",
      "24/06/05 17:33:21 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 13.0 KiB, free 4.5 GiB)\n",
      "24/06/05 17:33:21 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 4.5 GiB)\n",
      "24/06/05 17:33:21 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ubuntu:37615 (size: 5.8 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:33:21 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 2 (MapPartitionsRDD[4] at show at cell7.sc:5) (first 15 tasks are for partitions Vector(5, 6, 7))\n",
      "24/06/05 17:33:21 INFO TaskSchedulerImpl: Adding task set 2.0 with 3 tasks resource profile 0\n",
      "24/06/05 17:33:21 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 5) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7668 bytes) \n",
      "24/06/05 17:33:21 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 6) (ubuntu, executor driver, partition 6, PROCESS_LOCAL, 7607 bytes) \n",
      "24/06/05 17:33:21 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 7) (ubuntu, executor driver, partition 7, PROCESS_LOCAL, 7667 bytes) \n",
      "24/06/05 17:33:21 INFO Executor: Running task 1.0 in stage 2.0 (TID 6)\n",
      "24/06/05 17:33:21 INFO Executor: Running task 2.0 in stage 2.0 (TID 7)\n",
      "24/06/05 17:33:21 INFO Executor: Running task 0.0 in stage 2.0 (TID 5)\n",
      "24/06/05 17:33:21 INFO Executor: Finished task 1.0 in stage 2.0 (TID 6). 1409 bytes result sent to driver\n",
      "24/06/05 17:33:21 INFO Executor: Finished task 2.0 in stage 2.0 (TID 7). 1447 bytes result sent to driver\n",
      "24/06/05 17:33:21 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 6) in 13 ms on ubuntu (executor driver) (1/3)\n",
      "24/06/05 17:33:21 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 7) in 13 ms on ubuntu (executor driver) (2/3)\n",
      "24/06/05 17:33:21 INFO Executor: Finished task 0.0 in stage 2.0 (TID 5). 1490 bytes result sent to driver\n",
      "24/06/05 17:33:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 5) in 21 ms on ubuntu (executor driver) (3/3)\n",
      "24/06/05 17:33:21 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "24/06/05 17:33:21 INFO DAGScheduler: ResultStage 2 (show at cell7.sc:5) finished in 0,035 s\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/05 17:33:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "24/06/05 17:33:21 INFO DAGScheduler: Job 2 finished: show at cell7.sc:5, took 0,053537 s\n",
      "24/06/05 17:33:21 INFO CodeGenerator: Code generated in 28.633094 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|StudentID|Course|\n",
      "+---------+------+\n",
      "|        1| Spark|\n",
      "|        2| Scala|\n",
      "|        3|  Java|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mcolumns\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m\"StudentID\"\u001b[39m, \u001b[32m\"Course\"\u001b[39m)\n",
       "\u001b[36mdata\u001b[39m: \u001b[32mSeq\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m)] = \u001b[33mList\u001b[39m(\n",
       "  (\u001b[32m\"1\"\u001b[39m, \u001b[32m\"Spark\"\u001b[39m),\n",
       "  (\u001b[32m\"2\"\u001b[39m, \u001b[32m\"Scala\"\u001b[39m),\n",
       "  (\u001b[32m\"3\"\u001b[39m, \u001b[32m\"Java\"\u001b[39m)\n",
       ")\n",
       "\u001b[36mfromRDD\u001b[39m: \u001b[32mDataFrame\u001b[39m = [StudentID: string, Course: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val columns = Seq(\"StudentID\", \"Course\")\n",
    "val data = Seq((\"1\", \"Spark\"), (\"2\", \"Scala\"), (\"3\", \"Java\"))\n",
    "\n",
    "val fromRDD: DataFrame = spark.sparkContext.parallelize(data).toDF(columns: _*)\n",
    "\n",
    "fromRDD.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af3a0dd",
   "metadata": {},
   "source": [
    "### fromList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d4fdcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 17:34:39 INFO CodeGenerator: Code generated in 14.366845 ms\n",
      "24/06/05 17:34:39 INFO CodeGenerator: Code generated in 6.480361 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| _1|   _2|\n",
      "+---+-----+\n",
      "|  1|Spark|\n",
      "|  2|Scala|\n",
      "|  3| Java|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfromList\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_1: string, _2: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fromList: DataFrame = data.toDF()\n",
    "\n",
    "fromList.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64564ccb",
   "metadata": {},
   "source": [
    "### createDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "495197d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| _1|   _2|\n",
      "+---+-----+\n",
      "|  1|Spark|\n",
      "|  2|Scala|\n",
      "|  3| Java|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mcreateDataFrame\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_1: string, _2: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val createDataFrame: DataFrame = spark.createDataFrame(data)\n",
    "\n",
    "createDataFrame.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ec3d2e",
   "metadata": {},
   "source": [
    "### withSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7121de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 17:40:13 INFO CodeGenerator: Code generated in 8.583686 ms\n",
      "24/06/05 17:40:13 INFO SparkContext: Starting job: show at cell12.sc:15\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Got job 3 (show at cell12.sc:15) with 1 output partitions\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Final stage: ResultStage 3 (show at cell12.sc:15)\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[8] at show at cell12.sc:15), which has no missing parents\n",
      "24/06/05 17:40:13 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.3 KiB, free 4.5 GiB)\n",
      "24/06/05 17:40:13 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 4.5 GiB)\n",
      "24/06/05 17:40:13 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ubuntu:37615 (size: 9.8 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:40:13 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[8] at show at cell12.sc:15) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/05 17:40:13 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "24/06/05 17:40:13 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 8) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 7619 bytes) \n",
      "24/06/05 17:40:13 INFO Executor: Running task 0.0 in stage 3.0 (TID 8)\n",
      "24/06/05 17:40:13 INFO CodeGenerator: Code generated in 17.751329 ms\n",
      "24/06/05 17:40:13 INFO Executor: Finished task 0.0 in stage 3.0 (TID 8). 1409 bytes result sent to driver\n",
      "24/06/05 17:40:13 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 8) in 48 ms on ubuntu (executor driver) (1/1)\n",
      "24/06/05 17:40:13 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "24/06/05 17:40:13 INFO DAGScheduler: ResultStage 3 (show at cell12.sc:15) finished in 0,098 s\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/05 17:40:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Job 3 finished: show at cell12.sc:15, took 0,104019 s\n",
      "24/06/05 17:40:13 INFO SparkContext: Starting job: show at cell12.sc:15\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Got job 4 (show at cell12.sc:15) with 4 output partitions\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Final stage: ResultStage 4 (show at cell12.sc:15)\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[8] at show at cell12.sc:15), which has no missing parents\n",
      "24/06/05 17:40:13 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 21.3 KiB, free 4.5 GiB)\n",
      "24/06/05 17:40:13 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 4.5 GiB)\n",
      "24/06/05 17:40:13 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ubuntu:37615 (size: 9.8 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:40:13 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 4 (MapPartitionsRDD[8] at show at cell12.sc:15) (first 15 tasks are for partitions Vector(1, 2, 3, 4))\n",
      "24/06/05 17:40:13 INFO TaskSchedulerImpl: Adding task set 4.0 with 4 tasks resource profile 0\n",
      "24/06/05 17:40:13 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 9) (ubuntu, executor driver, partition 1, PROCESS_LOCAL, 7619 bytes) \n",
      "24/06/05 17:40:13 INFO TaskSetManager: Starting task 1.0 in stage 4.0 (TID 10) (ubuntu, executor driver, partition 2, PROCESS_LOCAL, 7619 bytes) \n",
      "24/06/05 17:40:13 INFO TaskSetManager: Starting task 2.0 in stage 4.0 (TID 11) (ubuntu, executor driver, partition 3, PROCESS_LOCAL, 7760 bytes) \n",
      "24/06/05 17:40:13 INFO TaskSetManager: Starting task 3.0 in stage 4.0 (TID 12) (ubuntu, executor driver, partition 4, PROCESS_LOCAL, 7619 bytes) \n",
      "24/06/05 17:40:13 INFO BlockManagerInfo: Removed broadcast_3_piece0 on ubuntu:37615 in memory (size: 9.8 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:40:13 INFO Executor: Running task 0.0 in stage 4.0 (TID 9)\n",
      "24/06/05 17:40:13 INFO Executor: Running task 1.0 in stage 4.0 (TID 10)\n",
      "24/06/05 17:40:13 INFO Executor: Running task 3.0 in stage 4.0 (TID 12)\n",
      "24/06/05 17:40:13 INFO Executor: Running task 2.0 in stage 4.0 (TID 11)\n",
      "24/06/05 17:40:13 INFO Executor: Finished task 0.0 in stage 4.0 (TID 9). 1452 bytes result sent to driver\n",
      "24/06/05 17:40:13 INFO Executor: Finished task 1.0 in stage 4.0 (TID 10). 1452 bytes result sent to driver\n",
      "24/06/05 17:40:13 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 9) in 79 ms on ubuntu (executor driver) (1/4)\n",
      "24/06/05 17:40:13 INFO Executor: Finished task 3.0 in stage 4.0 (TID 12). 1452 bytes result sent to driver\n",
      "24/06/05 17:40:13 INFO TaskSetManager: Finished task 1.0 in stage 4.0 (TID 10) in 81 ms on ubuntu (executor driver) (2/4)\n",
      "24/06/05 17:40:13 INFO TaskSetManager: Finished task 3.0 in stage 4.0 (TID 12) in 42 ms on ubuntu (executor driver) (3/4)\n",
      "24/06/05 17:40:13 INFO CodeGenerator: Code generated in 41.347371 ms\n",
      "24/06/05 17:40:13 INFO Executor: Finished task 2.0 in stage 4.0 (TID 11). 1490 bytes result sent to driver\n",
      "24/06/05 17:40:13 INFO TaskSetManager: Finished task 2.0 in stage 4.0 (TID 11) in 129 ms on ubuntu (executor driver) (4/4)\n",
      "24/06/05 17:40:13 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "24/06/05 17:40:13 INFO DAGScheduler: ResultStage 4 (show at cell12.sc:15) finished in 0,141 s\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/05 17:40:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Job 4 finished: show at cell12.sc:15, took 0,150193 s\n",
      "24/06/05 17:40:13 INFO SparkContext: Starting job: show at cell12.sc:15\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Got job 5 (show at cell12.sc:15) with 3 output partitions\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Final stage: ResultStage 5 (show at cell12.sc:15)\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[8] at show at cell12.sc:15), which has no missing parents\n",
      "24/06/05 17:40:13 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 21.3 KiB, free 4.5 GiB)\n",
      "24/06/05 17:40:13 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 9.8 KiB, free 4.5 GiB)\n",
      "24/06/05 17:40:13 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on ubuntu:37615 (size: 9.8 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:40:13 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 5 (MapPartitionsRDD[8] at show at cell12.sc:15) (first 15 tasks are for partitions Vector(5, 6, 7))\n",
      "24/06/05 17:40:13 INFO TaskSchedulerImpl: Adding task set 5.0 with 3 tasks resource profile 0\n",
      "24/06/05 17:40:13 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 13) (ubuntu, executor driver, partition 5, PROCESS_LOCAL, 7619 bytes) \n",
      "24/06/05 17:40:13 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 14) (ubuntu, executor driver, partition 6, PROCESS_LOCAL, 7619 bytes) \n",
      "24/06/05 17:40:13 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 15) (ubuntu, executor driver, partition 7, PROCESS_LOCAL, 7760 bytes) \n",
      "24/06/05 17:40:13 INFO Executor: Running task 1.0 in stage 5.0 (TID 14)\n",
      "24/06/05 17:40:13 INFO Executor: Running task 2.0 in stage 5.0 (TID 15)\n",
      "24/06/05 17:40:13 INFO Executor: Running task 0.0 in stage 5.0 (TID 13)\n",
      "24/06/05 17:40:13 INFO Executor: Finished task 0.0 in stage 5.0 (TID 13). 1409 bytes result sent to driver\n",
      "24/06/05 17:40:13 INFO Executor: Finished task 1.0 in stage 5.0 (TID 14). 1409 bytes result sent to driver\n",
      "24/06/05 17:40:13 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 13) in 14 ms on ubuntu (executor driver) (1/3)\n",
      "24/06/05 17:40:13 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 14) in 14 ms on ubuntu (executor driver) (2/3)\n",
      "24/06/05 17:40:13 INFO Executor: Finished task 2.0 in stage 5.0 (TID 15). 1447 bytes result sent to driver\n",
      "24/06/05 17:40:13 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 15) in 22 ms on ubuntu (executor driver) (3/3)\n",
      "24/06/05 17:40:13 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "24/06/05 17:40:13 INFO DAGScheduler: ResultStage 5 (show at cell12.sc:15) finished in 0,041 s\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/05 17:40:13 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "24/06/05 17:40:13 INFO DAGScheduler: Job 5 finished: show at cell12.sc:15, took 0,051345 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|StudentID|Course|\n",
      "+---------+------+\n",
      "|        1| Spark|\n",
      "|        2| Scala|\n",
      "+---------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mwithSchema\u001b[39m: \u001b[32mDataFrame\u001b[39m = [StudentID: int, Course: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val withSchema: DataFrame = {\n",
    "    val schema = StructType( Array(\n",
    "      StructField(\"StudentID\", IntegerType, true),\n",
    "      StructField(\"Course\", StringType, true)\n",
    "    ))\n",
    "\n",
    "    val rdd = (spark.sparkContext.parallelize(Seq(\n",
    "      Row(1, \"Spark\"),\n",
    "      Row(2, \"Scala\")\n",
    "    )))\n",
    "\n",
    "    spark.createDataFrame(rdd, schema)\n",
    "}\n",
    "\n",
    "withSchema.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8949d910",
   "metadata": {},
   "source": [
    "### fromFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24814ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 17:53:30 INFO BlockManagerInfo: Removed broadcast_8_piece0 on ubuntu:37615 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:53:30 INFO BlockManagerInfo: Removed broadcast_6_piece0 on ubuntu:37615 in memory (size: 33.9 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:53:30 INFO BlockManagerInfo: Removed broadcast_7_piece0 on ubuntu:37615 in memory (size: 7.4 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:53:30 INFO BlockManagerInfo: Removed broadcast_9_piece0 on ubuntu:37615 in memory (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:53:30 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "24/06/05 17:53:30 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "24/06/05 17:53:30 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/06/05 17:53:30 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/06/05 17:53:30 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 198.5 KiB, free 4.5 GiB)\n",
      "24/06/05 17:53:30 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 33.9 KiB, free 4.5 GiB)\n",
      "24/06/05 17:53:30 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on ubuntu:37615 (size: 33.9 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:53:30 INFO SparkContext: Created broadcast 10 from load at cell15.sc:1\n",
      "24/06/05 17:53:30 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/06/05 17:53:30 INFO SparkContext: Starting job: load at cell15.sc:1\n",
      "24/06/05 17:53:30 INFO DAGScheduler: Got job 8 (load at cell15.sc:1) with 1 output partitions\n",
      "24/06/05 17:53:30 INFO DAGScheduler: Final stage: ResultStage 8 (load at cell15.sc:1)\n",
      "24/06/05 17:53:30 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 17:53:30 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 17:53:30 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[20] at load at cell15.sc:1), which has no missing parents\n",
      "24/06/05 17:53:30 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 15.8 KiB, free 4.5 GiB)\n",
      "24/06/05 17:53:30 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 4.5 GiB)\n",
      "24/06/05 17:53:30 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on ubuntu:37615 (size: 7.4 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:53:30 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 17:53:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[20] at load at cell15.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/05 17:53:30 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0\n",
      "24/06/05 17:53:30 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 18) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/06/05 17:53:30 INFO Executor: Running task 0.0 in stage 8.0 (TID 18)\n",
      "24/06/05 17:53:30 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/Dataframe/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/06/05 17:53:30 INFO Executor: Finished task 0.0 in stage 8.0 (TID 18). 2076 bytes result sent to driver\n",
      "24/06/05 17:53:30 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 18) in 34 ms on ubuntu (executor driver) (1/1)\n",
      "24/06/05 17:53:30 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool \n",
      "24/06/05 17:53:30 INFO DAGScheduler: ResultStage 8 (load at cell15.sc:1) finished in 0,047 s\n",
      "24/06/05 17:53:30 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/05 17:53:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished\n",
      "24/06/05 17:53:30 INFO DAGScheduler: Job 8 finished: load at cell15.sc:1, took 0,053639 s\n",
      "24/06/05 17:53:31 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/06/05 17:53:31 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/06/05 17:53:31 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/06/05 17:53:31 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/06/05 17:53:31 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on ubuntu:37615 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:53:31 INFO SparkContext: Created broadcast 12 from show at cell15.sc:3\n",
      "24/06/05 17:53:31 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/06/05 17:53:31 INFO SparkContext: Starting job: show at cell15.sc:3\n",
      "24/06/05 17:53:31 INFO DAGScheduler: Got job 9 (show at cell15.sc:3) with 1 output partitions\n",
      "24/06/05 17:53:31 INFO DAGScheduler: Final stage: ResultStage 9 (show at cell15.sc:3)\n",
      "24/06/05 17:53:31 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 17:53:31 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 17:53:31 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[24] at show at cell15.sc:3), which has no missing parents\n",
      "24/06/05 17:53:31 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 17.0 KiB, free 4.5 GiB)\n",
      "24/06/05 17:53:31 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 4.5 GiB)\n",
      "24/06/05 17:53:31 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on ubuntu:37615 (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:53:31 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 17:53:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[24] at show at cell15.sc:3) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/05 17:53:31 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "24/06/05 17:53:31 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 19) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/06/05 17:53:31 INFO Executor: Running task 0.0 in stage 9.0 (TID 19)\n",
      "24/06/05 17:53:31 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/Dataframe/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/06/05 17:53:31 INFO Executor: Finished task 0.0 in stage 9.0 (TID 19). 4581 bytes result sent to driver\n",
      "24/06/05 17:53:31 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 19) in 18 ms on ubuntu (executor driver) (1/1)\n",
      "24/06/05 17:53:31 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "24/06/05 17:53:31 INFO DAGScheduler: ResultStage 9 (show at cell15.sc:3) finished in 0,033 s\n",
      "24/06/05 17:53:31 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/05 17:53:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "24/06/05 17:53:31 INFO DAGScheduler: Job 9 finished: show at cell15.sc:3, took 0,039838 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "|             Address|          Birthdate|       Country|CustomerID|               Email|             Name|        Username|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "|Unit 1047 Box 408...|1994-02-20 00:46:27|United Kingdom|     12346|cooperalexis@hotm...|    Lindsay Cowan|valenciajennifer|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|  Katherine David|      hillrachel|\n",
      "|Unit 2676 Box 935...|1974-11-26 15:30:20|       Finland|     12348| tcrawford@gmail.com|  Leslie Martinez|    serranobrian|\n",
      "|2765 Powers Meado...|1977-05-06 23:57:35|         Italy|     12349|  dustin37@yahoo.com|    Brad Cardenas|   charleshudson|\n",
      "|17677 Mark Crest\\...|1996-09-13 19:14:27|        Norway|     12350|amyholland@yahoo.com|     Natalie Ford| gregoryharrison|\n",
      "|50047 Smith Point...|1969-06-21 03:39:20|        Norway|     12352| vcarter@hotmail.com|      Dana Clarke|          hmyers|\n",
      "|633 Miller Turnpi...|1993-02-25 18:37:29|       Bahrain|     12353|   laura34@yahoo.com|     Gary Nichols|  andrewhamilton|\n",
      "|38456 Rachael Cau...|1993-03-13 12:37:34|         Spain|     12354|   zmelton@gmail.com|       John Parks|      matthewray|\n",
      "|4140 Pamela Hollo...|1972-11-10 12:01:08|       Bahrain|     12355|   scott50@yahoo.com|Jennifer Lawrence|          glopez|\n",
      "|8681 Karen Roads ...|1973-01-13 17:17:26|      Portugal|     12356|josephmacias@hotm...|    James Sanchez|        wesley20|\n",
      "|18637 Jessica Rid...|1989-11-24 17:12:54|   Switzerland|     12357|michael16@hotmail...|     Ashley Lopez|     thomasdavid|\n",
      "|2129 Joel Rapids\\...|1977-06-19 22:35:52|       Austria|     12358|michaelespinoza@g...| Dr. Angela Brown|      patricia44|\n",
      "|86636 Maria Viadu...|1983-09-21 05:22:18|        Cyprus|     12359|  ryanpena@yahoo.com|        John Vega|     nelsonmaria|\n",
      "|1579 Young Trail\\...|1980-10-28 17:25:59|       Austria|     12360|briannafrost@yaho...|     Lauren Clark|   portermichael|\n",
      "|USNS Howard\\nFPO ...|1982-09-01 09:12:57|       Belgium|     12361|virginia36@hotmai...|Jacqueline Haynes|   johnsonshelly|\n",
      "|70092 Adams Prair...|1979-02-03 03:42:47|       Belgium|     12362|   april04@gmail.com|     Brian Flores|    hunterdaniel|\n",
      "|7322 Owens Inlet ...|1974-12-21 13:27:20|   Unspecified|     12363|   omolina@gmail.com|Christopher Gomez|         james75|\n",
      "|86176 Katherine C...|1990-07-17 15:47:12|       Belgium|     12364|barbaraduncan@gma...|     Robert Burns|          eric10|\n",
      "|932 Jeremy Spring...|1981-07-10 00:35:00|        Cyprus|     12365|nicoleanderson@ho...|    Joshua Parker|     millerrenee|\n",
      "|USNV Chavez\\nFPO ...|1989-12-26 00:58:01|       Denmark|     12367|   aaron99@yahoo.com|Christine Douglas|       michael58|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfromFile\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Address: string, Birthdate: string ... 5 more fields]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fromFile: DataFrame = spark.read.format(\"json\").load(\"data/customer_data.json\")\n",
    "\n",
    "fromFile.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab7d210",
   "metadata": {},
   "source": [
    "## 2 - Операции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "067a8858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 17:54:07 INFO BlockManagerInfo: Removed broadcast_12_piece0 on ubuntu:37615 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:54:07 INFO BlockManagerInfo: Removed broadcast_11_piece0 on ubuntu:37615 in memory (size: 7.4 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:54:07 INFO BlockManagerInfo: Removed broadcast_10_piece0 on ubuntu:37615 in memory (size: 33.9 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:54:07 INFO BlockManagerInfo: Removed broadcast_13_piece0 on ubuntu:37615 in memory (size: 7.6 KiB, free: 4.5 GiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mcustomerDf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Address: string, Birthdate: string ... 5 more fields]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val customerDf = fromFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ddc3ab",
   "metadata": {},
   "source": [
    "### basicOperations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c371153c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Birthdate: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cc65050",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 17:54:41 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/06/05 17:54:41 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/06/05 17:54:41 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/06/05 17:54:42 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/06/05 17:54:42 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on ubuntu:37615 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:54:42 INFO SparkContext: Created broadcast 14 from head at cell18.sc:1\n",
      "24/06/05 17:54:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/06/05 17:54:42 INFO SparkContext: Starting job: head at cell18.sc:1\n",
      "24/06/05 17:54:42 INFO DAGScheduler: Got job 10 (head at cell18.sc:1) with 1 output partitions\n",
      "24/06/05 17:54:42 INFO DAGScheduler: Final stage: ResultStage 10 (head at cell18.sc:1)\n",
      "24/06/05 17:54:42 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 17:54:42 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 17:54:42 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[27] at head at cell18.sc:1), which has no missing parents\n",
      "24/06/05 17:54:42 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 11.6 KiB, free 4.5 GiB)\n",
      "24/06/05 17:54:42 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 4.5 GiB)\n",
      "24/06/05 17:54:42 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on ubuntu:37615 (size: 6.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:54:42 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 17:54:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[27] at head at cell18.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/05 17:54:42 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
      "24/06/05 17:54:42 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 20) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/06/05 17:54:42 INFO Executor: Running task 0.0 in stage 10.0 (TID 20)\n",
      "24/06/05 17:54:42 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/Dataframe/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/06/05 17:54:42 INFO Executor: Finished task 0.0 in stage 10.0 (TID 20). 1623 bytes result sent to driver\n",
      "24/06/05 17:54:42 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 20) in 21 ms on ubuntu (executor driver) (1/1)\n",
      "24/06/05 17:54:42 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "24/06/05 17:54:42 INFO DAGScheduler: ResultStage 10 (head at cell18.sc:1) finished in 0,037 s\n",
      "24/06/05 17:54:42 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/05 17:54:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
      "24/06/05 17:54:42 INFO DAGScheduler: Job 10 finished: head at cell18.sc:1, took 0,044237 s\n",
      "24/06/05 17:54:42 INFO CodeGenerator: Code generated in 15.219152 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres18\u001b[39m: \u001b[32mRow\u001b[39m = [Unit 1047 Box 4089\n",
       "DPO AA 57348,1994-02-20 00:46:27,United Kingdom,12346,cooperalexis@hotmail.com,Lindsay Cowan,valenciajennifer]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerDf.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a9c61d",
   "metadata": {},
   "source": [
    "#### Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b78c5d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 17:56:10 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/06/05 17:56:10 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/06/05 17:56:10 INFO CodeGenerator: Code generated in 6.437509 ms\n",
      "24/06/05 17:56:10 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:10 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:10 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on ubuntu:37615 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:56:10 INFO SparkContext: Created broadcast 16 from show at cell19.sc:1\n",
      "24/06/05 17:56:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/06/05 17:56:10 INFO SparkContext: Starting job: show at cell19.sc:1\n",
      "24/06/05 17:56:10 INFO DAGScheduler: Got job 11 (show at cell19.sc:1) with 1 output partitions\n",
      "24/06/05 17:56:10 INFO DAGScheduler: Final stage: ResultStage 11 (show at cell19.sc:1)\n",
      "24/06/05 17:56:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 17:56:10 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 17:56:10 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[31] at show at cell19.sc:1), which has no missing parents\n",
      "24/06/05 17:56:10 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 14.6 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:10 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 7.1 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:10 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on ubuntu:37615 (size: 7.1 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:56:10 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 17:56:10 INFO BlockManagerInfo: Removed broadcast_14_piece0 on ubuntu:37615 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:56:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[31] at show at cell19.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/05 17:56:10 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
      "24/06/05 17:56:10 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 21) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/06/05 17:56:10 INFO BlockManagerInfo: Removed broadcast_15_piece0 on ubuntu:37615 in memory (size: 6.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:56:10 INFO Executor: Running task 0.0 in stage 11.0 (TID 21)\n",
      "24/06/05 17:56:10 INFO CodeGenerator: Code generated in 7.121555 ms\n",
      "24/06/05 17:56:10 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/Dataframe/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/06/05 17:56:10 INFO CodeGenerator: Code generated in 6.845936 ms\n",
      "24/06/05 17:56:10 INFO Executor: Finished task 0.0 in stage 11.0 (TID 21). 2153 bytes result sent to driver\n",
      "24/06/05 17:56:10 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 21) in 34 ms on ubuntu (executor driver) (1/1)\n",
      "24/06/05 17:56:10 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "24/06/05 17:56:10 INFO DAGScheduler: ResultStage 11 (show at cell19.sc:1) finished in 0,054 s\n",
      "24/06/05 17:56:10 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/05 17:56:10 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
      "24/06/05 17:56:10 INFO DAGScheduler: Job 11 finished: show at cell19.sc:1, took 0,057410 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+\n",
      "|          Birthdate|       Country|\n",
      "+-------------------+--------------+\n",
      "|1994-02-20 00:46:27|United Kingdom|\n",
      "|1988-06-21 00:15:34|       Iceland|\n",
      "|1974-11-26 15:30:20|       Finland|\n",
      "|1977-05-06 23:57:35|         Italy|\n",
      "|1996-09-13 19:14:27|        Norway|\n",
      "|1969-06-21 03:39:20|        Norway|\n",
      "|1993-02-25 18:37:29|       Bahrain|\n",
      "|1993-03-13 12:37:34|         Spain|\n",
      "|1972-11-10 12:01:08|       Bahrain|\n",
      "|1973-01-13 17:17:26|      Portugal|\n",
      "|1989-11-24 17:12:54|   Switzerland|\n",
      "|1977-06-19 22:35:52|       Austria|\n",
      "|1983-09-21 05:22:18|        Cyprus|\n",
      "|1980-10-28 17:25:59|       Austria|\n",
      "|1982-09-01 09:12:57|       Belgium|\n",
      "|1979-02-03 03:42:47|       Belgium|\n",
      "|1974-12-21 13:27:20|   Unspecified|\n",
      "|1990-07-17 15:47:12|       Belgium|\n",
      "|1981-07-10 00:35:00|        Cyprus|\n",
      "|1989-12-26 00:58:01|       Denmark|\n",
      "+-------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.select(\"Birthdate\", \"Country\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1822538d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 17:56:15 INFO BlockManagerInfo: Removed broadcast_17_piece0 on ubuntu:37615 in memory (size: 7.1 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:56:15 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/06/05 17:56:15 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/06/05 17:56:15 INFO CodeGenerator: Code generated in 6.290274 ms\n",
      "24/06/05 17:56:15 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:15 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:15 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on ubuntu:37615 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:56:15 INFO SparkContext: Created broadcast 18 from show at cell20.sc:1\n",
      "24/06/05 17:56:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/06/05 17:56:15 INFO SparkContext: Starting job: show at cell20.sc:1\n",
      "24/06/05 17:56:15 INFO DAGScheduler: Got job 12 (show at cell20.sc:1) with 1 output partitions\n",
      "24/06/05 17:56:15 INFO DAGScheduler: Final stage: ResultStage 12 (show at cell20.sc:1)\n",
      "24/06/05 17:56:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 17:56:15 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 17:56:15 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[35] at show at cell20.sc:1), which has no missing parents\n",
      "24/06/05 17:56:15 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 14.2 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:15 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:15 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on ubuntu:37615 (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:56:15 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 17:56:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[35] at show at cell20.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/05 17:56:15 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0\n",
      "24/06/05 17:56:15 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 22) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/06/05 17:56:15 INFO Executor: Running task 0.0 in stage 12.0 (TID 22)\n",
      "24/06/05 17:56:15 INFO CodeGenerator: Code generated in 10.977594 ms\n",
      "24/06/05 17:56:15 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/Dataframe/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/06/05 17:56:15 INFO Executor: Finished task 0.0 in stage 12.0 (TID 22). 1753 bytes result sent to driver\n",
      "24/06/05 17:56:15 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 22) in 29 ms on ubuntu (executor driver) (1/1)\n",
      "24/06/05 17:56:15 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool \n",
      "24/06/05 17:56:15 INFO DAGScheduler: ResultStage 12 (show at cell20.sc:1) finished in 0,043 s\n",
      "24/06/05 17:56:15 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/05 17:56:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished\n",
      "24/06/05 17:56:15 INFO DAGScheduler: Job 12 finished: show at cell20.sc:1, took 0,049305 s\n",
      "24/06/05 17:56:15 INFO CodeGenerator: Code generated in 5.55741 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|       Country|\n",
      "+--------------+\n",
      "|United Kingdom|\n",
      "|       Iceland|\n",
      "|       Finland|\n",
      "|         Italy|\n",
      "|        Norway|\n",
      "|        Norway|\n",
      "|       Bahrain|\n",
      "|         Spain|\n",
      "|       Bahrain|\n",
      "|      Portugal|\n",
      "|   Switzerland|\n",
      "|       Austria|\n",
      "|        Cyprus|\n",
      "|       Austria|\n",
      "|       Belgium|\n",
      "|       Belgium|\n",
      "|   Unspecified|\n",
      "|       Belgium|\n",
      "|        Cyprus|\n",
      "|       Denmark|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.select(col(\"Country\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f91b725e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 17:56:20 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/06/05 17:56:20 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/06/05 17:56:20 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:20 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:20 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on ubuntu:37615 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:56:20 INFO SparkContext: Created broadcast 20 from show at cell21.sc:1\n",
      "24/06/05 17:56:20 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/06/05 17:56:20 INFO SparkContext: Starting job: show at cell21.sc:1\n",
      "24/06/05 17:56:20 INFO DAGScheduler: Got job 13 (show at cell21.sc:1) with 1 output partitions\n",
      "24/06/05 17:56:20 INFO BlockManagerInfo: Removed broadcast_19_piece0 on ubuntu:37615 in memory (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:56:20 INFO DAGScheduler: Final stage: ResultStage 13 (show at cell21.sc:1)\n",
      "24/06/05 17:56:20 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 17:56:20 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 17:56:20 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[39] at show at cell21.sc:1), which has no missing parents\n",
      "24/06/05 17:56:20 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 14.2 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:20 INFO BlockManagerInfo: Removed broadcast_18_piece0 on ubuntu:37615 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:56:20 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:20 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on ubuntu:37615 (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:56:20 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 17:56:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[39] at show at cell21.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/05 17:56:20 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0\n",
      "24/06/05 17:56:20 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 23) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/06/05 17:56:20 INFO Executor: Running task 0.0 in stage 13.0 (TID 23)\n",
      "24/06/05 17:56:20 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/Dataframe/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/06/05 17:56:20 INFO Executor: Finished task 0.0 in stage 13.0 (TID 23). 1753 bytes result sent to driver\n",
      "24/06/05 17:56:20 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 23) in 14 ms on ubuntu (executor driver) (1/1)\n",
      "24/06/05 17:56:20 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool \n",
      "24/06/05 17:56:20 INFO DAGScheduler: ResultStage 13 (show at cell21.sc:1) finished in 0,059 s\n",
      "24/06/05 17:56:20 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/05 17:56:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished\n",
      "24/06/05 17:56:20 INFO DAGScheduler: Job 13 finished: show at cell21.sc:1, took 0,103966 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|       Country|\n",
      "+--------------+\n",
      "|United Kingdom|\n",
      "|       Iceland|\n",
      "|       Finland|\n",
      "|         Italy|\n",
      "|        Norway|\n",
      "|        Norway|\n",
      "|       Bahrain|\n",
      "|         Spain|\n",
      "|       Bahrain|\n",
      "|      Portugal|\n",
      "|   Switzerland|\n",
      "|       Austria|\n",
      "|        Cyprus|\n",
      "|       Austria|\n",
      "|       Belgium|\n",
      "|       Belgium|\n",
      "|   Unspecified|\n",
      "|       Belgium|\n",
      "|        Cyprus|\n",
      "|       Denmark|\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.select('Country).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee0f2ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 17:56:25 INFO BlockManagerInfo: Removed broadcast_21_piece0 on ubuntu:37615 in memory (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:56:25 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/06/05 17:56:25 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/06/05 17:56:25 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:25 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:25 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on ubuntu:37615 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:56:25 INFO SparkContext: Created broadcast 22 from show at cell22.sc:1\n",
      "24/06/05 17:56:25 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/06/05 17:56:25 INFO SparkContext: Starting job: show at cell22.sc:1\n",
      "24/06/05 17:56:25 INFO DAGScheduler: Got job 14 (show at cell22.sc:1) with 1 output partitions\n",
      "24/06/05 17:56:25 INFO DAGScheduler: Final stage: ResultStage 14 (show at cell22.sc:1)\n",
      "24/06/05 17:56:25 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 17:56:25 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 17:56:25 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[43] at show at cell22.sc:1), which has no missing parents\n",
      "24/06/05 17:56:25 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 14.2 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:25 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:25 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on ubuntu:37615 (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:56:25 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 17:56:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[43] at show at cell22.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/05 17:56:25 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks resource profile 0\n",
      "24/06/05 17:56:25 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 24) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/06/05 17:56:25 INFO Executor: Running task 0.0 in stage 14.0 (TID 24)\n",
      "24/06/05 17:56:25 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/Dataframe/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/06/05 17:56:25 INFO Executor: Finished task 0.0 in stage 14.0 (TID 24). 1899 bytes result sent to driver\n",
      "24/06/05 17:56:25 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 24) in 15 ms on ubuntu (executor driver) (1/1)\n",
      "24/06/05 17:56:25 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool \n",
      "24/06/05 17:56:25 INFO DAGScheduler: ResultStage 14 (show at cell22.sc:1) finished in 0,023 s\n",
      "24/06/05 17:56:25 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/05 17:56:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 14: Stage finished\n",
      "24/06/05 17:56:25 INFO DAGScheduler: Job 14 finished: show at cell22.sc:1, took 0,031045 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               Date|\n",
      "+-------------------+\n",
      "|1994-02-20 00:46:27|\n",
      "|1988-06-21 00:15:34|\n",
      "|1974-11-26 15:30:20|\n",
      "|1977-05-06 23:57:35|\n",
      "|1996-09-13 19:14:27|\n",
      "|1969-06-21 03:39:20|\n",
      "|1993-02-25 18:37:29|\n",
      "|1993-03-13 12:37:34|\n",
      "|1972-11-10 12:01:08|\n",
      "|1973-01-13 17:17:26|\n",
      "|1989-11-24 17:12:54|\n",
      "|1977-06-19 22:35:52|\n",
      "|1983-09-21 05:22:18|\n",
      "|1980-10-28 17:25:59|\n",
      "|1982-09-01 09:12:57|\n",
      "|1979-02-03 03:42:47|\n",
      "|1974-12-21 13:27:20|\n",
      "|1990-07-17 15:47:12|\n",
      "|1981-07-10 00:35:00|\n",
      "|1989-12-26 00:58:01|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.selectExpr(\"Birthdate as Date\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "250879d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 17:56:29 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/06/05 17:56:29 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/06/05 17:56:29 INFO CodeGenerator: Code generated in 10.428265 ms\n",
      "24/06/05 17:56:29 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:29 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:29 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on ubuntu:37615 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:56:29 INFO SparkContext: Created broadcast 24 from show at cell23.sc:1\n",
      "24/06/05 17:56:29 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/06/05 17:56:29 INFO SparkContext: Starting job: show at cell23.sc:1\n",
      "24/06/05 17:56:29 INFO DAGScheduler: Got job 15 (show at cell23.sc:1) with 1 output partitions\n",
      "24/06/05 17:56:29 INFO DAGScheduler: Final stage: ResultStage 15 (show at cell23.sc:1)\n",
      "24/06/05 17:56:29 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 17:56:29 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 17:56:29 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[47] at show at cell23.sc:1), which has no missing parents\n",
      "24/06/05 17:56:29 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 17.1 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:29 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:29 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on ubuntu:37615 (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:56:29 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 17:56:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[47] at show at cell23.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/05 17:56:29 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks resource profile 0\n",
      "24/06/05 17:56:29 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 25) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/06/05 17:56:29 INFO Executor: Running task 0.0 in stage 15.0 (TID 25)\n",
      "24/06/05 17:56:29 INFO CodeGenerator: Code generated in 21.820945 ms\n",
      "24/06/05 17:56:29 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/Dataframe/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/06/05 17:56:29 INFO Executor: Finished task 0.0 in stage 15.0 (TID 25). 4738 bytes result sent to driver\n",
      "24/06/05 17:56:29 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 25) in 37 ms on ubuntu (executor driver) (1/1)\n",
      "24/06/05 17:56:29 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool \n",
      "24/06/05 17:56:29 INFO DAGScheduler: ResultStage 15 (show at cell23.sc:1) finished in 0,045 s\n",
      "24/06/05 17:56:29 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/05 17:56:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 15: Stage finished\n",
      "24/06/05 17:56:29 INFO DAGScheduler: Job 15 finished: show at cell23.sc:1, took 0,049126 s\n",
      "24/06/05 17:56:29 INFO CodeGenerator: Code generated in 9.779848 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+----+\n",
      "|             Address|          Birthdate|       Country|CustomerID|               Email|             Name|        Username|Flag|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+----+\n",
      "|Unit 1047 Box 408...|1994-02-20 00:46:27|United Kingdom|     12346|cooperalexis@hotm...|    Lindsay Cowan|valenciajennifer|true|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|  Katherine David|      hillrachel|true|\n",
      "|Unit 2676 Box 935...|1974-11-26 15:30:20|       Finland|     12348| tcrawford@gmail.com|  Leslie Martinez|    serranobrian|true|\n",
      "|2765 Powers Meado...|1977-05-06 23:57:35|         Italy|     12349|  dustin37@yahoo.com|    Brad Cardenas|   charleshudson|true|\n",
      "|17677 Mark Crest\\...|1996-09-13 19:14:27|        Norway|     12350|amyholland@yahoo.com|     Natalie Ford| gregoryharrison|true|\n",
      "|50047 Smith Point...|1969-06-21 03:39:20|        Norway|     12352| vcarter@hotmail.com|      Dana Clarke|          hmyers|true|\n",
      "|633 Miller Turnpi...|1993-02-25 18:37:29|       Bahrain|     12353|   laura34@yahoo.com|     Gary Nichols|  andrewhamilton|true|\n",
      "|38456 Rachael Cau...|1993-03-13 12:37:34|         Spain|     12354|   zmelton@gmail.com|       John Parks|      matthewray|true|\n",
      "|4140 Pamela Hollo...|1972-11-10 12:01:08|       Bahrain|     12355|   scott50@yahoo.com|Jennifer Lawrence|          glopez|true|\n",
      "|8681 Karen Roads ...|1973-01-13 17:17:26|      Portugal|     12356|josephmacias@hotm...|    James Sanchez|        wesley20|true|\n",
      "|18637 Jessica Rid...|1989-11-24 17:12:54|   Switzerland|     12357|michael16@hotmail...|     Ashley Lopez|     thomasdavid|true|\n",
      "|2129 Joel Rapids\\...|1977-06-19 22:35:52|       Austria|     12358|michaelespinoza@g...| Dr. Angela Brown|      patricia44|true|\n",
      "|86636 Maria Viadu...|1983-09-21 05:22:18|        Cyprus|     12359|  ryanpena@yahoo.com|        John Vega|     nelsonmaria|true|\n",
      "|1579 Young Trail\\...|1980-10-28 17:25:59|       Austria|     12360|briannafrost@yaho...|     Lauren Clark|   portermichael|true|\n",
      "|USNS Howard\\nFPO ...|1982-09-01 09:12:57|       Belgium|     12361|virginia36@hotmai...|Jacqueline Haynes|   johnsonshelly|true|\n",
      "|70092 Adams Prair...|1979-02-03 03:42:47|       Belgium|     12362|   april04@gmail.com|     Brian Flores|    hunterdaniel|true|\n",
      "|7322 Owens Inlet ...|1974-12-21 13:27:20|   Unspecified|     12363|   omolina@gmail.com|Christopher Gomez|         james75|true|\n",
      "|86176 Katherine C...|1990-07-17 15:47:12|       Belgium|     12364|barbaraduncan@gma...|     Robert Burns|          eric10|true|\n",
      "|932 Jeremy Spring...|1981-07-10 00:35:00|        Cyprus|     12365|nicoleanderson@ho...|    Joshua Parker|     millerrenee|true|\n",
      "|USNV Chavez\\nFPO ...|1989-12-26 00:58:01|       Denmark|     12367|   aaron99@yahoo.com|Christine Douglas|       michael58|true|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.withColumn(\"Flag\", lit(true)).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7df0d8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 17:56:35 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/06/05 17:56:35 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/06/05 17:56:35 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:35 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:35 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on ubuntu:37615 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:56:35 INFO SparkContext: Created broadcast 26 from show at cell24.sc:1\n",
      "24/06/05 17:56:35 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/06/05 17:56:35 INFO SparkContext: Starting job: show at cell24.sc:1\n",
      "24/06/05 17:56:35 INFO DAGScheduler: Got job 16 (show at cell24.sc:1) with 1 output partitions\n",
      "24/06/05 17:56:35 INFO DAGScheduler: Final stage: ResultStage 16 (show at cell24.sc:1)\n",
      "24/06/05 17:56:35 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 17:56:35 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 17:56:35 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[51] at show at cell24.sc:1), which has no missing parents\n",
      "24/06/05 17:56:35 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 17.0 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:35 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 4.5 GiB)\n",
      "24/06/05 17:56:35 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on ubuntu:37615 (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:56:35 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 17:56:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[51] at show at cell24.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/05 17:56:35 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks resource profile 0\n",
      "24/06/05 17:56:35 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 26) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/06/05 17:56:35 INFO Executor: Running task 0.0 in stage 16.0 (TID 26)\n",
      "24/06/05 17:56:35 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/Dataframe/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/06/05 17:56:35 INFO Executor: Finished task 0.0 in stage 16.0 (TID 26). 4581 bytes result sent to driver\n",
      "24/06/05 17:56:35 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 26) in 16 ms on ubuntu (executor driver) (1/1)\n",
      "24/06/05 17:56:35 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool \n",
      "24/06/05 17:56:35 INFO DAGScheduler: ResultStage 16 (show at cell24.sc:1) finished in 0,026 s\n",
      "24/06/05 17:56:35 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/05 17:56:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished\n",
      "24/06/05 17:56:35 INFO DAGScheduler: Job 16 finished: show at cell24.sc:1, took 0,036288 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "|             Address|               Date|       Country|CustomerID|               Email|             Name|        Username|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "|Unit 1047 Box 408...|1994-02-20 00:46:27|United Kingdom|     12346|cooperalexis@hotm...|    Lindsay Cowan|valenciajennifer|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|  Katherine David|      hillrachel|\n",
      "|Unit 2676 Box 935...|1974-11-26 15:30:20|       Finland|     12348| tcrawford@gmail.com|  Leslie Martinez|    serranobrian|\n",
      "|2765 Powers Meado...|1977-05-06 23:57:35|         Italy|     12349|  dustin37@yahoo.com|    Brad Cardenas|   charleshudson|\n",
      "|17677 Mark Crest\\...|1996-09-13 19:14:27|        Norway|     12350|amyholland@yahoo.com|     Natalie Ford| gregoryharrison|\n",
      "|50047 Smith Point...|1969-06-21 03:39:20|        Norway|     12352| vcarter@hotmail.com|      Dana Clarke|          hmyers|\n",
      "|633 Miller Turnpi...|1993-02-25 18:37:29|       Bahrain|     12353|   laura34@yahoo.com|     Gary Nichols|  andrewhamilton|\n",
      "|38456 Rachael Cau...|1993-03-13 12:37:34|         Spain|     12354|   zmelton@gmail.com|       John Parks|      matthewray|\n",
      "|4140 Pamela Hollo...|1972-11-10 12:01:08|       Bahrain|     12355|   scott50@yahoo.com|Jennifer Lawrence|          glopez|\n",
      "|8681 Karen Roads ...|1973-01-13 17:17:26|      Portugal|     12356|josephmacias@hotm...|    James Sanchez|        wesley20|\n",
      "|18637 Jessica Rid...|1989-11-24 17:12:54|   Switzerland|     12357|michael16@hotmail...|     Ashley Lopez|     thomasdavid|\n",
      "|2129 Joel Rapids\\...|1977-06-19 22:35:52|       Austria|     12358|michaelespinoza@g...| Dr. Angela Brown|      patricia44|\n",
      "|86636 Maria Viadu...|1983-09-21 05:22:18|        Cyprus|     12359|  ryanpena@yahoo.com|        John Vega|     nelsonmaria|\n",
      "|1579 Young Trail\\...|1980-10-28 17:25:59|       Austria|     12360|briannafrost@yaho...|     Lauren Clark|   portermichael|\n",
      "|USNS Howard\\nFPO ...|1982-09-01 09:12:57|       Belgium|     12361|virginia36@hotmai...|Jacqueline Haynes|   johnsonshelly|\n",
      "|70092 Adams Prair...|1979-02-03 03:42:47|       Belgium|     12362|   april04@gmail.com|     Brian Flores|    hunterdaniel|\n",
      "|7322 Owens Inlet ...|1974-12-21 13:27:20|   Unspecified|     12363|   omolina@gmail.com|Christopher Gomez|         james75|\n",
      "|86176 Katherine C...|1990-07-17 15:47:12|       Belgium|     12364|barbaraduncan@gma...|     Robert Burns|          eric10|\n",
      "|932 Jeremy Spring...|1981-07-10 00:35:00|        Cyprus|     12365|nicoleanderson@ho...|    Joshua Parker|     millerrenee|\n",
      "|USNV Chavez\\nFPO ...|1989-12-26 00:58:01|       Denmark|     12367|   aaron99@yahoo.com|Christine Douglas|       michael58|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.withColumnRenamed(\"Birthdate\", \"Date\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39bab1b",
   "metadata": {},
   "source": [
    "#### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30bc0966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 17:58:11 INFO FileSourceStrategy: Pushed Filters: IsNotNull(Country),EqualTo(Country,Norway)\n",
      "24/06/05 17:58:11 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(Country#148),(Country#148 = Norway)\n",
      "24/06/05 17:58:11 INFO BlockManagerInfo: Removed broadcast_27_piece0 on ubuntu:37615 in memory (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:58:11 INFO BlockManagerInfo: Removed broadcast_26_piece0 on ubuntu:37615 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:58:11 INFO CodeGenerator: Code generated in 8.353026 ms\n",
      "24/06/05 17:58:11 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/06/05 17:58:11 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/06/05 17:58:11 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on ubuntu:37615 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:58:11 INFO SparkContext: Created broadcast 28 from show at cell25.sc:1\n",
      "24/06/05 17:58:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/06/05 17:58:11 INFO SparkContext: Starting job: show at cell25.sc:1\n",
      "24/06/05 17:58:11 INFO DAGScheduler: Got job 17 (show at cell25.sc:1) with 1 output partitions\n",
      "24/06/05 17:58:11 INFO DAGScheduler: Final stage: ResultStage 17 (show at cell25.sc:1)\n",
      "24/06/05 17:58:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 17:58:11 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 17:58:11 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[55] at show at cell25.sc:1), which has no missing parents\n",
      "24/06/05 17:58:11 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 17.7 KiB, free 4.5 GiB)\n",
      "24/06/05 17:58:11 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 4.5 GiB)\n",
      "24/06/05 17:58:11 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on ubuntu:37615 (size: 7.9 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:58:11 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 17:58:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[55] at show at cell25.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/05 17:58:11 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks resource profile 0\n",
      "24/06/05 17:58:11 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 27) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/06/05 17:58:11 INFO Executor: Running task 0.0 in stage 17.0 (TID 27)\n",
      "24/06/05 17:58:11 INFO CodeGenerator: Code generated in 11.767477 ms\n",
      "24/06/05 17:58:11 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/Dataframe/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/06/05 17:58:11 INFO CodeGenerator: Code generated in 5.373788 ms\n",
      "24/06/05 17:58:11 INFO Executor: Finished task 0.0 in stage 17.0 (TID 27). 3059 bytes result sent to driver\n",
      "24/06/05 17:58:11 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 27) in 63 ms on ubuntu (executor driver) (1/1)\n",
      "24/06/05 17:58:11 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool \n",
      "24/06/05 17:58:11 INFO DAGScheduler: ResultStage 17 (show at cell25.sc:1) finished in 0,073 s\n",
      "24/06/05 17:58:11 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/05 17:58:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished\n",
      "24/06/05 17:58:11 INFO DAGScheduler: Job 17 finished: show at cell25.sc:1, took 0,076990 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------+----------+--------------------+----------------+---------------+\n",
      "|             Address|          Birthdate|Country|CustomerID|               Email|            Name|       Username|\n",
      "+--------------------+-------------------+-------+----------+--------------------+----------------+---------------+\n",
      "|17677 Mark Crest\\...|1996-09-13 19:14:27| Norway|     12350|amyholland@yahoo.com|    Natalie Ford|gregoryharrison|\n",
      "|50047 Smith Point...|1969-06-21 03:39:20| Norway|     12352| vcarter@hotmail.com|     Dana Clarke|         hmyers|\n",
      "|0297 Jacob Ranch ...|1990-06-01 14:49:52| Norway|     12381|douglaschavez@hot...|   Matthew Jones|    stephanie68|\n",
      "|3102 Hopkins Walk...|1976-06-19 08:10:24| Norway|     12430|crystalromero@hot...|       Lisa Tate|       pgilbert|\n",
      "|637 Philip Lock S...|1984-06-06 09:36:14| Norway|     12432|jessica87@hotmail...|  Cheryl Herring|mathewsnicholas|\n",
      "|546 Tyler Prairie...|1985-05-27 10:39:47| Norway|     12433|mariahmcpherson@g...|  Kaitlin Miller|         lyoung|\n",
      "|6270 Jennifer Pra...|1977-06-01 20:40:04| Norway|     12436|lynncynthia@hotma...|    Rodney Giles|       swiggins|\n",
      "|415 Megan Parkway...|1971-08-29 06:21:22| Norway|     12438|  jeff42@hotmail.com| Thomas Figueroa|  matthewharris|\n",
      "|PSC 4266, Box 099...|1971-09-03 05:42:49| Norway|     12444| richard20@gmail.com|     Meghan Wood|   salazarbilly|\n",
      "|1333 Michael Vill...|1995-03-09 03:25:02| Norway|     12752|seanrobles@gmail.com|Lauren Hernandez|    morrowhenry|\n",
      "+--------------------+-------------------+-------+----------+--------------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.filter(\"Country = 'Norway'\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb032124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 17:58:15 INFO BlockManagerInfo: Removed broadcast_29_piece0 on ubuntu:37615 in memory (size: 7.9 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:58:15 INFO FileSourceStrategy: Pushed Filters: IsNotNull(Country),EqualTo(Country,Iceland)\n",
      "24/06/05 17:58:15 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(Country#148),(Country#148 = Iceland)\n",
      "24/06/05 17:58:15 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/06/05 17:58:15 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/06/05 17:58:15 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on ubuntu:37615 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:58:15 INFO SparkContext: Created broadcast 30 from show at cell26.sc:1\n",
      "24/06/05 17:58:15 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/06/05 17:58:15 INFO SparkContext: Starting job: show at cell26.sc:1\n",
      "24/06/05 17:58:15 INFO DAGScheduler: Got job 18 (show at cell26.sc:1) with 1 output partitions\n",
      "24/06/05 17:58:15 INFO DAGScheduler: Final stage: ResultStage 18 (show at cell26.sc:1)\n",
      "24/06/05 17:58:15 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 17:58:15 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 17:58:15 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[59] at show at cell26.sc:1), which has no missing parents\n",
      "24/06/05 17:58:15 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 17.7 KiB, free 4.5 GiB)\n",
      "24/06/05 17:58:15 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 7.9 KiB, free 4.5 GiB)\n",
      "24/06/05 17:58:15 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on ubuntu:37615 (size: 7.9 KiB, free: 4.5 GiB)\n",
      "24/06/05 17:58:15 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 17:58:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[59] at show at cell26.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/05 17:58:15 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0\n",
      "24/06/05 17:58:15 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 28) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/06/05 17:58:15 INFO Executor: Running task 0.0 in stage 18.0 (TID 28)\n",
      "24/06/05 17:58:15 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/Dataframe/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/06/05 17:58:15 INFO Executor: Finished task 0.0 in stage 18.0 (TID 28). 1771 bytes result sent to driver\n",
      "24/06/05 17:58:15 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 28) in 26 ms on ubuntu (executor driver) (1/1)\n",
      "24/06/05 17:58:15 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool \n",
      "24/06/05 17:58:15 INFO DAGScheduler: ResultStage 18 (show at cell26.sc:1) finished in 0,042 s\n",
      "24/06/05 17:58:15 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/05 17:58:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished\n",
      "24/06/05 17:58:15 INFO DAGScheduler: Job 18 finished: show at cell26.sc:1, took 0,060882 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------+----------+--------------------+---------------+----------+\n",
      "|             Address|          Birthdate|Country|CustomerID|               Email|           Name|  Username|\n",
      "+--------------------+-------------------+-------+----------+--------------------+---------------+----------+\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|Iceland|     12347|timothy78@hotmail...|Katherine David|hillrachel|\n",
      "+--------------------+-------------------+-------+----------+--------------------+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.where('Country === \"Iceland\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5044e9ab",
   "metadata": {},
   "source": [
    "#### Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5c70ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 19:01:59 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/06/05 19:01:59 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/06/05 19:01:59 INFO CodeGenerator: Code generated in 11.732776 ms\n",
      "24/06/05 19:01:59 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/06/05 19:01:59 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/06/05 19:01:59 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on ubuntu:37615 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 19:01:59 INFO SparkContext: Created broadcast 32 from show at cell27.sc:1\n",
      "24/06/05 19:01:59 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/06/05 19:01:59 INFO SparkContext: Starting job: show at cell27.sc:1\n",
      "24/06/05 19:01:59 INFO DAGScheduler: Got job 19 (show at cell27.sc:1) with 1 output partitions\n",
      "24/06/05 19:01:59 INFO DAGScheduler: Final stage: ResultStage 19 (show at cell27.sc:1)\n",
      "24/06/05 19:01:59 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 19:01:59 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 19:01:59 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[63] at show at cell27.sc:1), which has no missing parents\n",
      "24/06/05 19:01:59 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 14.3 KiB, free 4.5 GiB)\n",
      "24/06/05 19:01:59 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 4.5 GiB)\n",
      "24/06/05 19:01:59 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on ubuntu:37615 (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 19:01:59 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 19:01:59 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[63] at show at cell27.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/05 19:01:59 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks resource profile 0\n",
      "24/06/05 19:01:59 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 29) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/06/05 19:01:59 INFO Executor: Running task 0.0 in stage 19.0 (TID 29)\n",
      "24/06/05 19:01:59 INFO CodeGenerator: Code generated in 4.854271 ms\n",
      "24/06/05 19:01:59 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/Dataframe/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/06/05 19:01:59 INFO Executor: Finished task 0.0 in stage 19.0 (TID 29). 6279 bytes result sent to driver\n",
      "24/06/05 19:01:59 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 29) in 49 ms on ubuntu (executor driver) (1/1)\n",
      "24/06/05 19:01:59 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool \n",
      "24/06/05 19:01:59 INFO DAGScheduler: ResultStage 19 (show at cell27.sc:1) finished in 0,060 s\n",
      "24/06/05 19:01:59 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/05 19:01:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 19: Stage finished\n",
      "24/06/05 19:01:59 INFO DAGScheduler: Job 19 finished: show at cell27.sc:1, took 0,065455 s\n",
      "24/06/05 19:01:59 INFO CodeGenerator: Code generated in 11.903209 ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------+----------+--------------------+--------------------+-----------------+\n",
      "|             Address|          Birthdate|       Country|CustomerID|               Email|                Name|         Username|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+--------------------+-----------------+\n",
      "|6942 Connie Skywa...|1973-10-24 00:52:10|United Kingdom|     12989| amber97@hotmail.com|   Brandon Contreras|           ecasey|\n",
      "|79375 David Neck\\...|1971-05-04 22:20:10|United Kingdom|     12988|   erica98@gmail.com|      Gabriel Romero|          qknight|\n",
      "|00881 West Flat\\n...|1997-03-05 19:20:57|United Kingdom|     12987|    vkeith@yahoo.com|Christopher Lawrence|        smcintyre|\n",
      "|499 Jonathan Stre...|1987-10-24 20:05:15|United Kingdom|     12985| fredsmith@yahoo.com|        Xavier Myers|stricklandjeffery|\n",
      "|9505 Melissa Stre...|1975-09-22 15:21:58|United Kingdom|     12984|scottjonathan@yah...|        Brandy Huang|   amandawilliams|\n",
      "|399 Fuentes Roads...|1986-09-30 18:12:45|United Kingdom|     12982|cynthia31@hotmail...|      Linda Stephens|     davidsonomar|\n",
      "|1573 Jessica Glen...|1984-07-23 03:09:18|United Kingdom|     12981|  esharp@hotmail.com|          Dawn Woods|         steven67|\n",
      "|153 Tara Ridges S...|1974-03-03 07:52:15|United Kingdom|     12980| jessica87@gmail.com|         Sean Brooks|        kristen26|\n",
      "|62134 Chen Valley...|1990-10-09 01:29:02|United Kingdom|     12977| fmatthews@gmail.com|          Kyle Simon|          emily28|\n",
      "|7521 Christopher ...|1973-10-10 23:57:51|United Kingdom|     12976|williamsheidi@yah...|         Hannah Rose|         eugene04|\n",
      "|00679 Lucero Moun...|1987-10-13 12:41:52|United Kingdom|     12974|thomasreyes@yahoo...|     Daniel Fletcher|  velazquezangela|\n",
      "|885 Zamora Hills\\...|1986-11-14 14:18:47|United Kingdom|     12971|   cwilcox@yahoo.com|       Caitlin Walls|         ashley11|\n",
      "|4539 Powers Orcha...|1990-09-11 06:01:18|United Kingdom|     12970|edwardspeter@yaho...|      Jonathan Hines|          mmiller|\n",
      "|335 Lewis Land\\nL...|1994-04-25 16:59:48|United Kingdom|     12968| mmurray@hotmail.com|         Susan Davis|   jacksoncolleen|\n",
      "|Unit 3978 Box 615...|1969-05-28 22:09:26|United Kingdom|     12967|thomasjames@gmail...|    Amber Williamson|    justinjohnson|\n",
      "|PSC 6600, Box 447...|1975-10-14 17:46:59|United Kingdom|     12966|colinward@hotmail...|        Amy Robinson|     sarathompson|\n",
      "|0240 Ernest Under...|1968-06-14 23:10:47|United Kingdom|     12965|jeremy10@hotmail.com|      Raymond Patton|      meganbrewer|\n",
      "|2433 Amy Shoals\\n...|1975-11-05 04:34:04|United Kingdom|     12963|  tjohnson@yahoo.com|        Sheila Parks|      dominique55|\n",
      "|833 Wilson Street...|1978-12-25 10:12:45|United Kingdom|     12962|kathyphillips@yah...|        Cheryl Burns|       diazsharon|\n",
      "|809 Robert Plain ...|1978-07-26 19:48:26|United Kingdom|     12957|  steven78@gmail.com|     Reginald Wright|         nathan71|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+--------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.sort('CustomerID.desc).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99396b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 19:02:05 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/06/05 19:02:05 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/06/05 19:02:05 INFO CodeGenerator: Code generated in 3.765704 ms\n",
      "24/06/05 19:02:05 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/06/05 19:02:05 INFO BlockManagerInfo: Removed broadcast_32_piece0 on ubuntu:37615 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 19:02:05 INFO BlockManagerInfo: Removed broadcast_33_piece0 on ubuntu:37615 in memory (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 19:02:05 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/06/05 19:02:05 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on ubuntu:37615 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 19:02:05 INFO SparkContext: Created broadcast 34 from show at cell28.sc:1\n",
      "24/06/05 19:02:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/06/05 19:02:05 INFO SparkContext: Starting job: show at cell28.sc:1\n",
      "24/06/05 19:02:05 INFO DAGScheduler: Got job 20 (show at cell28.sc:1) with 1 output partitions\n",
      "24/06/05 19:02:05 INFO DAGScheduler: Final stage: ResultStage 20 (show at cell28.sc:1)\n",
      "24/06/05 19:02:05 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 19:02:05 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 19:02:05 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[67] at show at cell28.sc:1), which has no missing parents\n",
      "24/06/05 19:02:05 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 14.3 KiB, free 4.5 GiB)\n",
      "24/06/05 19:02:05 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 4.5 GiB)\n",
      "24/06/05 19:02:05 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on ubuntu:37615 (size: 7.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 19:02:05 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 19:02:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[67] at show at cell28.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/05 19:02:05 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks resource profile 0\n",
      "24/06/05 19:02:05 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 30) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/06/05 19:02:05 INFO Executor: Running task 0.0 in stage 20.0 (TID 30)\n",
      "24/06/05 19:02:05 INFO CodeGenerator: Code generated in 5.774085 ms\n",
      "24/06/05 19:02:05 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/Dataframe/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/06/05 19:02:05 INFO Executor: Finished task 0.0 in stage 20.0 (TID 30). 6119 bytes result sent to driver\n",
      "24/06/05 19:02:05 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 30) in 29 ms on ubuntu (executor driver) (1/1)\n",
      "24/06/05 19:02:05 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool \n",
      "24/06/05 19:02:05 INFO DAGScheduler: ResultStage 20 (show at cell28.sc:1) finished in 0,035 s\n",
      "24/06/05 19:02:05 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/05 19:02:05 INFO TaskSchedulerImpl: Killing all running tasks in stage 20: Stage finished\n",
      "24/06/05 19:02:05 INFO DAGScheduler: Job 20 finished: show at cell28.sc:1, took 0,041425 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "|             Address|          Birthdate|       Country|CustomerID|               Email|             Name|        Username|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "|Unit 1047 Box 408...|1994-02-20 00:46:27|United Kingdom|     12346|cooperalexis@hotm...|    Lindsay Cowan|valenciajennifer|\n",
      "|55711 Janet Plaza...|1988-06-21 00:15:34|       Iceland|     12347|timothy78@hotmail...|  Katherine David|      hillrachel|\n",
      "|Unit 2676 Box 935...|1974-11-26 15:30:20|       Finland|     12348| tcrawford@gmail.com|  Leslie Martinez|    serranobrian|\n",
      "|2765 Powers Meado...|1977-05-06 23:57:35|         Italy|     12349|  dustin37@yahoo.com|    Brad Cardenas|   charleshudson|\n",
      "|17677 Mark Crest\\...|1996-09-13 19:14:27|        Norway|     12350|amyholland@yahoo.com|     Natalie Ford| gregoryharrison|\n",
      "|50047 Smith Point...|1969-06-21 03:39:20|        Norway|     12352| vcarter@hotmail.com|      Dana Clarke|          hmyers|\n",
      "|633 Miller Turnpi...|1993-02-25 18:37:29|       Bahrain|     12353|   laura34@yahoo.com|     Gary Nichols|  andrewhamilton|\n",
      "|38456 Rachael Cau...|1993-03-13 12:37:34|         Spain|     12354|   zmelton@gmail.com|       John Parks|      matthewray|\n",
      "|4140 Pamela Hollo...|1972-11-10 12:01:08|       Bahrain|     12355|   scott50@yahoo.com|Jennifer Lawrence|          glopez|\n",
      "|8681 Karen Roads ...|1973-01-13 17:17:26|      Portugal|     12356|josephmacias@hotm...|    James Sanchez|        wesley20|\n",
      "|18637 Jessica Rid...|1989-11-24 17:12:54|   Switzerland|     12357|michael16@hotmail...|     Ashley Lopez|     thomasdavid|\n",
      "|2129 Joel Rapids\\...|1977-06-19 22:35:52|       Austria|     12358|michaelespinoza@g...| Dr. Angela Brown|      patricia44|\n",
      "|86636 Maria Viadu...|1983-09-21 05:22:18|        Cyprus|     12359|  ryanpena@yahoo.com|        John Vega|     nelsonmaria|\n",
      "|1579 Young Trail\\...|1980-10-28 17:25:59|       Austria|     12360|briannafrost@yaho...|     Lauren Clark|   portermichael|\n",
      "|USNS Howard\\nFPO ...|1982-09-01 09:12:57|       Belgium|     12361|virginia36@hotmai...|Jacqueline Haynes|   johnsonshelly|\n",
      "|70092 Adams Prair...|1979-02-03 03:42:47|       Belgium|     12362|   april04@gmail.com|     Brian Flores|    hunterdaniel|\n",
      "|7322 Owens Inlet ...|1974-12-21 13:27:20|   Unspecified|     12363|   omolina@gmail.com|Christopher Gomez|         james75|\n",
      "|86176 Katherine C...|1990-07-17 15:47:12|       Belgium|     12364|barbaraduncan@gma...|     Robert Burns|          eric10|\n",
      "|932 Jeremy Spring...|1981-07-10 00:35:00|        Cyprus|     12365|nicoleanderson@ho...|    Joshua Parker|     millerrenee|\n",
      "|USNV Chavez\\nFPO ...|1989-12-26 00:58:01|       Denmark|     12367|   aaron99@yahoo.com|Christine Douglas|       michael58|\n",
      "+--------------------+-------------------+--------------+----------+--------------------+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.orderBy(\"CustomerID\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdc9ec6",
   "metadata": {},
   "source": [
    "#### Repartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93bd54b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num partitions: 1\n"
     ]
    }
   ],
   "source": [
    "println(s\"Num partitions: ${customerDf.rdd.getNumPartitions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1910596b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 19:04:14 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/06/05 19:04:14 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/06/05 19:04:14 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/06/05 19:04:14 INFO BlockManagerInfo: Removed broadcast_49_piece0 on ubuntu:37615 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 19:04:14 INFO BlockManagerInfo: Removed broadcast_50_piece0 on ubuntu:37615 in memory (size: 7.3 KiB, free: 4.5 GiB)\n",
      "24/06/05 19:04:14 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/06/05 19:04:14 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on ubuntu:37615 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 19:04:14 INFO SparkContext: Created broadcast 51 from rdd at cell36.sc:2\n",
      "24/06/05 19:04:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/06/05 19:04:14 INFO DAGScheduler: Registering RDD 128 (rdd at cell36.sc:2) as input to shuffle 7\n",
      "24/06/05 19:04:14 INFO DAGScheduler: Got map stage job 28 (rdd at cell36.sc:2) with 1 output partitions\n",
      "24/06/05 19:04:14 INFO DAGScheduler: Final stage: ShuffleMapStage 28 (rdd at cell36.sc:2)\n",
      "24/06/05 19:04:14 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 19:04:14 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 19:04:14 INFO DAGScheduler: Submitting ShuffleMapStage 28 (MapPartitionsRDD[128] at rdd at cell36.sc:2), which has no missing parents\n",
      "24/06/05 19:04:14 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 14.5 KiB, free 4.5 GiB)\n",
      "24/06/05 19:04:14 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 4.5 GiB)\n",
      "24/06/05 19:04:14 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on ubuntu:37615 (size: 7.3 KiB, free: 4.5 GiB)\n",
      "24/06/05 19:04:14 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 19:04:14 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[128] at rdd at cell36.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/05 19:04:14 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks resource profile 0\n",
      "24/06/05 19:04:14 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 38) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8235 bytes) \n",
      "24/06/05 19:04:14 INFO Executor: Running task 0.0 in stage 28.0 (TID 38)\n",
      "24/06/05 19:04:14 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/Dataframe/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/06/05 19:04:14 INFO Executor: Finished task 0.0 in stage 28.0 (TID 38). 1713 bytes result sent to driver\n",
      "24/06/05 19:04:14 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 38) in 25 ms on ubuntu (executor driver) (1/1)\n",
      "24/06/05 19:04:14 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool \n",
      "24/06/05 19:04:14 INFO DAGScheduler: ShuffleMapStage 28 (rdd at cell36.sc:2) finished in 0,035 s\n",
      "24/06/05 19:04:14 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/06/05 19:04:14 INFO DAGScheduler: running: Set()\n",
      "24/06/05 19:04:14 INFO DAGScheduler: waiting: Set()\n",
      "24/06/05 19:04:14 INFO DAGScheduler: failed: Set()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New num partitions: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mrepartitionedDf\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [Address: string, Birthdate: string ... 5 more fields]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val repartitionedDf = customerDf.repartition(5, col(\"Country\"))\n",
    "println(s\"New num partitions: ${repartitionedDf.rdd.getNumPartitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44b583d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 19:03:58 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/06/05 19:03:58 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/06/05 19:03:58 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/06/05 19:03:58 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/06/05 19:03:58 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on ubuntu:37615 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 19:03:58 INFO SparkContext: Created broadcast 49 from rdd at cell35.sc:1\n",
      "24/06/05 19:03:58 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/06/05 19:03:58 INFO DAGScheduler: Registering RDD 120 (rdd at cell35.sc:1) as input to shuffle 6\n",
      "24/06/05 19:03:58 INFO DAGScheduler: Got map stage job 27 (rdd at cell35.sc:1) with 1 output partitions\n",
      "24/06/05 19:03:58 INFO DAGScheduler: Final stage: ShuffleMapStage 27 (rdd at cell35.sc:1)\n",
      "24/06/05 19:03:58 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 19:03:58 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 19:03:58 INFO DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[120] at rdd at cell35.sc:1), which has no missing parents\n",
      "24/06/05 19:03:58 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 14.5 KiB, free 4.5 GiB)\n",
      "24/06/05 19:03:58 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 7.3 KiB, free 4.5 GiB)\n",
      "24/06/05 19:03:58 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on ubuntu:37615 (size: 7.3 KiB, free: 4.5 GiB)\n",
      "24/06/05 19:03:58 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 19:03:58 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[120] at rdd at cell35.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/05 19:03:58 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks resource profile 0\n",
      "24/06/05 19:03:58 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 37) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8235 bytes) \n",
      "24/06/05 19:03:58 INFO Executor: Running task 0.0 in stage 27.0 (TID 37)\n",
      "24/06/05 19:03:58 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/Dataframe/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/06/05 19:03:58 INFO Executor: Finished task 0.0 in stage 27.0 (TID 37). 1713 bytes result sent to driver\n",
      "24/06/05 19:03:58 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 37) in 33 ms on ubuntu (executor driver) (1/1)\n",
      "24/06/05 19:03:58 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool \n",
      "24/06/05 19:03:58 INFO DAGScheduler: ShuffleMapStage 27 (rdd at cell35.sc:1) finished in 0,049 s\n",
      "24/06/05 19:03:58 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/06/05 19:03:58 INFO DAGScheduler: running: Set()\n",
      "24/06/05 19:03:58 INFO DAGScheduler: waiting: Set()\n",
      "24/06/05 19:03:58 INFO DAGScheduler: failed: Set()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num partitions after coalesce: 1\n"
     ]
    }
   ],
   "source": [
    "println(s\"Num partitions after coalesce: ${repartitionedDf.coalesce(1).rdd.getNumPartitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97975bbd",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e35c3a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 19:06:55 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/06/05 19:06:55 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/06/05 19:06:55 INFO CodeGenerator: Code generated in 12.969149 ms\n",
      "24/06/05 19:06:55 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/06/05 19:06:55 INFO BlockManagerInfo: Removed broadcast_52_piece0 on ubuntu:37615 in memory (size: 7.3 KiB, free: 4.5 GiB)\n",
      "24/06/05 19:06:55 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/06/05 19:06:55 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on ubuntu:37615 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 19:06:55 INFO SparkContext: Created broadcast 53 from show at cell37.sc:1\n",
      "24/06/05 19:06:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/06/05 19:06:55 INFO SparkContext: Starting job: show at cell37.sc:1\n",
      "24/06/05 19:06:55 INFO DAGScheduler: Got job 29 (show at cell37.sc:1) with 1 output partitions\n",
      "24/06/05 19:06:55 INFO DAGScheduler: Final stage: ResultStage 29 (show at cell37.sc:1)\n",
      "24/06/05 19:06:55 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 19:06:55 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 19:06:55 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[136] at show at cell37.sc:1), which has no missing parents\n",
      "24/06/05 19:06:55 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 15.5 KiB, free 4.5 GiB)\n",
      "24/06/05 19:06:55 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 4.5 GiB)\n",
      "24/06/05 19:06:55 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on ubuntu:37615 (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/06/05 19:06:55 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 19:06:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[136] at show at cell37.sc:1) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/05 19:06:55 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks resource profile 0\n",
      "24/06/05 19:06:55 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 39) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8246 bytes) \n",
      "24/06/05 19:06:56 INFO Executor: Running task 0.0 in stage 29.0 (TID 39)\n",
      "24/06/05 19:06:56 INFO CodeGenerator: Code generated in 10.693047 ms\n",
      "24/06/05 19:06:56 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/Dataframe/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/06/05 19:06:56 INFO Executor: Finished task 0.0 in stage 29.0 (TID 39). 1778 bytes result sent to driver\n",
      "24/06/05 19:06:56 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 39) in 34 ms on ubuntu (executor driver) (1/1)\n",
      "24/06/05 19:06:56 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool \n",
      "24/06/05 19:06:56 INFO DAGScheduler: ResultStage 29 (show at cell37.sc:1) finished in 0,042 s\n",
      "24/06/05 19:06:56 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/05 19:06:56 INFO TaskSchedulerImpl: Killing all running tasks in stage 29: Stage finished\n",
      "24/06/05 19:06:56 INFO DAGScheduler: Job 29 finished: show at cell37.sc:1, took 0,046904 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|date_format(Birthdate, yyy-MM-dd)|\n",
      "+---------------------------------+\n",
      "|                       1994-02-20|\n",
      "|                       1988-06-21|\n",
      "|                       1974-11-26|\n",
      "|                       1977-05-06|\n",
      "|                       1996-09-13|\n",
      "|                       1969-06-21|\n",
      "|                       1993-02-25|\n",
      "|                       1993-03-13|\n",
      "|                       1972-11-10|\n",
      "|                       1973-01-13|\n",
      "|                       1989-11-24|\n",
      "|                       1977-06-19|\n",
      "|                       1983-09-21|\n",
      "|                       1980-10-28|\n",
      "|                       1982-09-01|\n",
      "|                       1979-02-03|\n",
      "|                       1974-12-21|\n",
      "|                       1990-07-17|\n",
      "|                       1981-07-10|\n",
      "|                       1989-12-26|\n",
      "+---------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.select(date_format(col(\"Birthdate\"), \"yyy-MM-dd\")).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1d84b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Address: string (nullable = true)\n",
      " |-- Birthdate: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Username: string (nullable = true)\n",
      " |-- Identity: array (nullable = false)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.withColumn(\"Identity\", array('Name, 'Username)).printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b17c1f-3197-43bd-b043-a96f256bff3c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97bfc928-6e23-4c15-b3d3-e9b314fc23e2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/05 19:08:24 INFO FileSourceStrategy: Pushed Filters: \n",
      "24/06/05 19:08:24 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "24/06/05 19:08:24 INFO BlockManagerInfo: Removed broadcast_53_piece0 on ubuntu:37615 in memory (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 19:08:24 INFO BlockManagerInfo: Removed broadcast_54_piece0 on ubuntu:37615 in memory (size: 7.6 KiB, free: 4.5 GiB)\n",
      "24/06/05 19:08:24 INFO CodeGenerator: Code generated in 44.43497 ms\n",
      "24/06/05 19:08:24 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 198.4 KiB, free 4.5 GiB)\n",
      "24/06/05 19:08:24 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 34.0 KiB, free 4.5 GiB)\n",
      "24/06/05 19:08:24 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on ubuntu:37615 (size: 34.0 KiB, free: 4.5 GiB)\n",
      "24/06/05 19:08:24 INFO SparkContext: Created broadcast 55 from show at cell39.sc:2\n",
      "24/06/05 19:08:24 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "24/06/05 19:08:24 INFO DAGScheduler: Registering RDD 140 (show at cell39.sc:2) as input to shuffle 8\n",
      "24/06/05 19:08:24 INFO DAGScheduler: Got map stage job 30 (show at cell39.sc:2) with 1 output partitions\n",
      "24/06/05 19:08:24 INFO DAGScheduler: Final stage: ShuffleMapStage 30 (show at cell39.sc:2)\n",
      "24/06/05 19:08:24 INFO DAGScheduler: Parents of final stage: List()\n",
      "24/06/05 19:08:24 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 19:08:24 INFO DAGScheduler: Submitting ShuffleMapStage 30 (MapPartitionsRDD[140] at show at cell39.sc:2), which has no missing parents\n",
      "24/06/05 19:08:24 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 38.0 KiB, free 4.5 GiB)\n",
      "24/06/05 19:08:24 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 17.9 KiB, free 4.5 GiB)\n",
      "24/06/05 19:08:24 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on ubuntu:37615 (size: 17.9 KiB, free: 4.5 GiB)\n",
      "24/06/05 19:08:24 INFO SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 19:08:24 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[140] at show at cell39.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/05 19:08:24 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks resource profile 0\n",
      "24/06/05 19:08:24 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 40) (ubuntu, executor driver, partition 0, PROCESS_LOCAL, 8235 bytes) \n",
      "24/06/05 19:08:24 INFO Executor: Running task 0.0 in stage 30.0 (TID 40)\n",
      "24/06/05 19:08:24 INFO CodeGenerator: Code generated in 32.613274 ms\n",
      "24/06/05 19:08:24 INFO CodeGenerator: Code generated in 6.536298 ms\n",
      "24/06/05 19:08:24 INFO CodeGenerator: Code generated in 4.418306 ms\n",
      "24/06/05 19:08:24 INFO CodeGenerator: Code generated in 4.391044 ms\n",
      "24/06/05 19:08:24 INFO FileScanRDD: Reading File path: file:///home/vadim/workspace/Spark/Dataframe/data/customer_data.json, range: 0-108790, partition values: [empty row]\n",
      "24/06/05 19:08:24 INFO Executor: Finished task 0.0 in stage 30.0 (TID 40). 2716 bytes result sent to driver\n",
      "24/06/05 19:08:24 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 40) in 115 ms on ubuntu (executor driver) (1/1)\n",
      "24/06/05 19:08:24 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool \n",
      "24/06/05 19:08:24 INFO DAGScheduler: ShuffleMapStage 30 (show at cell39.sc:2) finished in 0,126 s\n",
      "24/06/05 19:08:24 INFO DAGScheduler: looking for newly runnable stages\n",
      "24/06/05 19:08:24 INFO DAGScheduler: running: Set()\n",
      "24/06/05 19:08:24 INFO DAGScheduler: waiting: Set()\n",
      "24/06/05 19:08:24 INFO DAGScheduler: failed: Set()\n",
      "24/06/05 19:08:24 INFO ShufflePartitionsUtil: For shuffle(8), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "24/06/05 19:08:24 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "24/06/05 19:08:24 INFO CodeGenerator: Code generated in 13.492616 ms\n",
      "24/06/05 19:08:24 INFO SparkContext: Starting job: show at cell39.sc:2\n",
      "24/06/05 19:08:24 INFO DAGScheduler: Got job 31 (show at cell39.sc:2) with 1 output partitions\n",
      "24/06/05 19:08:24 INFO DAGScheduler: Final stage: ResultStage 32 (show at cell39.sc:2)\n",
      "24/06/05 19:08:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 31)\n",
      "24/06/05 19:08:24 INFO DAGScheduler: Missing parents: List()\n",
      "24/06/05 19:08:24 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[143] at show at cell39.sc:2), which has no missing parents\n",
      "24/06/05 19:08:24 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 40.0 KiB, free 4.5 GiB)\n",
      "24/06/05 19:08:24 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 19.1 KiB, free 4.5 GiB)\n",
      "24/06/05 19:08:24 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on ubuntu:37615 (size: 19.1 KiB, free: 4.5 GiB)\n",
      "24/06/05 19:08:24 INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1580\n",
      "24/06/05 19:08:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[143] at show at cell39.sc:2) (first 15 tasks are for partitions Vector(0))\n",
      "24/06/05 19:08:24 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks resource profile 0\n",
      "24/06/05 19:08:24 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 41) (ubuntu, executor driver, partition 0, NODE_LOCAL, 7615 bytes) \n",
      "24/06/05 19:08:24 INFO Executor: Running task 0.0 in stage 32.0 (TID 41)\n",
      "24/06/05 19:08:24 INFO ShuffleBlockFetcherIterator: Getting 1 (2.2 KiB) non-empty blocks including 1 (2.2 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "24/06/05 19:08:24 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 8 ms\n",
      "24/06/05 19:08:24 INFO CodeGenerator: Code generated in 17.938408 ms\n",
      "24/06/05 19:08:24 INFO Executor: Finished task 0.0 in stage 32.0 (TID 41). 5569 bytes result sent to driver\n",
      "24/06/05 19:08:24 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 41) in 103 ms on ubuntu (executor driver) (1/1)\n",
      "24/06/05 19:08:24 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool \n",
      "24/06/05 19:08:24 INFO DAGScheduler: ResultStage 32 (show at cell39.sc:2) finished in 0,117 s\n",
      "24/06/05 19:08:24 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "24/06/05 19:08:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 32: Stage finished\n",
      "24/06/05 19:08:24 INFO DAGScheduler: Job 31 finished: show at cell39.sc:2, took 0,126953 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             Country|count(1)|\n",
      "+--------------------+--------+\n",
      "|              Sweden|       7|\n",
      "|           Singapore|       1|\n",
      "|             Germany|      86|\n",
      "|                 RSA|       1|\n",
      "|              France|      86|\n",
      "|              Greece|       2|\n",
      "|             Belgium|      25|\n",
      "|             Finland|      12|\n",
      "|         Unspecified|       2|\n",
      "|               Italy|      14|\n",
      "|              Norway|      10|\n",
      "|               Spain|      30|\n",
      "|             Denmark|       8|\n",
      "|             Iceland|       1|\n",
      "|              Israel|       4|\n",
      "|                 USA|       4|\n",
      "|              Cyprus|       7|\n",
      "|        Saudi Arabia|       1|\n",
      "|         Switzerland|      16|\n",
      "|United Arab Emirates|       1|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "customerDf.groupBy(\"Country\").agg(count(lit(1))).show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a00e4b2",
   "metadata": {},
   "source": [
    "### join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cf39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val retailDf = spark.read.format(\"json\").load(\"data/retail_data.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45091db0-fbd1-4567-b5eb-faec32fd5325",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e2b3ef-f005-4fa1-b6e5-533bf3fd55e6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.12)",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
